import base64
import contextlib
import io
import json
import os.path as osp

import PIL.Image

from labelme import PY2
from labelme import QT4
from labelme import __version__
from labelme import utils
from labelme.logger import logger

PIL.Image.MAX_IMAGE_PIXELS = None


@contextlib.contextmanager
def open(name, mode):
    assert mode in ["r", "w"]
    if PY2:
        mode += "b"
        encoding = None
    else:
        encoding = "utf-8"
    yield io.open(name, mode, encoding=encoding)
    return


class LabelFileError(Exception):
    pass


class LabelFile(object):
    suffix = ".json"

    def __init__(self, filename=None):
        self.shapes = []
        self.imagePath = None
        self.imageData = None


    long_description=LONG_DESCRIPTION,
    url='https://youtube-wrapper.readthedocs.io/en/latest/index.html',
    author='Lyle Okoth',
    author_email='lyceokoth@gmail.com',
    license='MIT',
    install_requires=install_requires,
    keywords=key_words,
    classifiers=[
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.10',
        'Programming Language :: Python :: 3.11',
        'Operating System :: OS Independent'
    ],
)


                self.prevhVertex = self.hVertex
                self.hVertex = None
                self.prevhShape = self.hShape = shape
                self.prevhEdge = self.hEdge
                self.hEdge = None
                self.setToolTip(
                    self.tr("Click & drag to move shape '%s'") % shape.label
                )
                self.setStatusTip(self.toolTip())
                self.overrideCursor(CURSOR_GRAB)
                self.update()
                break
        else:  # Nothing found, clear highlights, reset state.
            self.unHighlight()
        self.vertexSelected.emit(self.hVertex is not None)

    def addPointToEdge(self):
        shape = self.prevhShape
        index = self.prevhEdge
        point = self.prevMovePoint
        if shape is None or index is None or point is None:
            return
        shape.insertPoint(index, point)
        shape.highlightVertex(index, shape.MOVE_VERTEX)
        self.hShape = shape
        self.hVertex = index
        self.hEdge = None
        self.movingShape = True

    def removeSelectedPoint(self):
        shape = self.prevhShape
        index = self.prevhVertex
        if shape is None or index is None:
            return
        shape.removePoint(index)
        shape.highlightClear()
        self.hShape = shape
        self.prevhVertex = None
        self.movingShape = True  # Save changes



    if out_file:
        out_file = osp.abspath(out_file)
        if osp.exists(out_file):
            raise RuntimeError("File exists: %s" % out_file)
        else:
            open(osp.abspath(out_file), "w")

    cmd = (
        "docker run -it --rm"
        " -e DISPLAY={0}:0"
        " -e QT_X11_NO_MITSHM=1"
        " -v /tmp/.X11-unix:/tmp/.X11-unix"
        " -v {1}:{2}"
        " -w /home/developer"
    )
    in_file_a = osp.abspath(in_file)
    in_file_b = osp.join("/home/developer", osp.basename(in_file))
    cmd = cmd.format(
        ip,
        in_file_a,
        in_file_b,
    )
    if out_file:
        out_file_a = osp.abspath(out_file)
        out_file_b = osp.join("/home/developer", osp.basename(out_file))
        cmd += " -v {0}:{1}".format(out_file_a, out_file_b)
    cmd += " wkentaro/labelme labelme {0}".format(in_file_b)
    if out_file:
        cmd += " -O {0}".format(out_file_b)
    subprocess.call(shlex.split(cmd))

    if out_file:
        try:
            json.load(open(out_file))
            return out_file
        except Exception:
            if open(out_file).read() == "":
                os.remove(out_file)
            raise RuntimeError("Annotation is cancelled.")



import torch
from tqdm import tqdm
from torch.nn import CrossEntropyLoss
from torch import optim
from torchvision.transforms import Compose, Resize, RandomCrop, ToTensor, Normalize
from torch.utils.tensorboard import SummaryWriter
from utils import save_checkpoint, load_checkpoint, print_examples
from create_dataset import get_loader
from model import CNNToRNN


def train():
    transforms = Compose(
        [
            Resize((356, 356)),
            RandomCrop((299, 299)),
            ToTensor(),
            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ]
    )

    train_loader, dataset = get_loader(
        images_dir="raw-data/Images",
        captions_file="raw-data/captions.txt",
        transforms=transforms,
        num_workers=2,
    )

    torch.backends.cudnn.benchmark = True
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    load_model = False
    save_model = False
    train_CNN = False

    # Hyperparameters
    embed_size = 256
    hidden_size = 256
    vocab_size = len(dataset.vocabulary)
    num_layers = 1
    learning_rate = 3e-4


            createLineMode=createLineMode,
            createPointMode=createPointMode,
            createLineStripMode=createLineStripMode,
            createAiPolygonMode=createAiPolygonMode,
            createAiMaskMode=createAiMaskMode,
            zoom=zoom,
            zoomIn=zoomIn,
            zoomOut=zoomOut,
            zoomOrg=zoomOrg,
            keepPrevScale=keepPrevScale,
            fitWindow=fitWindow,
            fitWidth=fitWidth,
            brightnessContrast=brightnessContrast,
            zoomActions=zoomActions,
            openNextImg=openNextImg,
            openPrevImg=openPrevImg,
            fileMenuActions=(open_, opendir, save, saveAs, close, quit),
            tool=(),
            # XXX: need to add some actions here to activate the shortcut
            editMenu=(
                edit,
                duplicate,
                copy,
                paste,
                delete,
                None,
                undo,
                undoLastPoint,
                None,
                removePoint,
                None,
                toggle_keep_prev_mode,
            ),
            # menu shown at right click
            menu=(
                createMode,
                createRectangleMode,
                createCircleMode,
                createLineMode,
                createPointMode,


            llm=self.chat_model,
            prompt=self.mapping_prompt.chat_prompt,
            output_parser=self.mapping_prompt.parser,
            verbose=debug,
            output_key="mapping_list",
        )

        overall_chain = SequentialChain(
            chains=[travel_agent, parser],
            input_variables=["query", "format_instructions"],
            output_variables=["agent_suggestion", "mapping_list"],
            verbose=debug,
        )

        return overall_chain

    def suggest_travel(self, query):
        """

        Parameters
        ----------
        query

        Returns
        -------

        """
        self.logger.info("Validating query")
        t1 = time.time()
        self.logger.info(
            "Calling validation (model is {}) on user input".format(
                self.chat_model.model_name
            )
        )
        validation_result = self.validation_chain(
            {
                "query": query,
                "format_instructions": self.validation_prompt.parser.get_format_instructions(),
            }
        )


# Scrapy settings for slidesmodel project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = "slidesmodel"

SPIDER_MODULES = ["slidesmodel.spiders"]
NEWSPIDER_MODULE = "slidesmodel.spiders"


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = "slidesmodel (+http://www.yourdomain.com)"

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {


from langchain.tools import tool
from .helpers import advanced_video_search
from youtube.models import Search


class FindProductVideoTools():
    @tool
    def find_product_video_id(product: str) -> str:
        """Useful when you need to find a product review video from youtube."""
        query: str = f'reviews of the latest {product}'
        search_results: list[Search] = advanced_video_search(query)
        return search_results[0].resource_id
        
        

# flake8: noqa

from . import draw_json
from . import draw_label_png
from . import export_json
from . import on_docker


ef generate_bookmarks(users: list[User], posts: list[Post], bookmarks_count: int = 100) -> list[Bookmark]:
    """Generate bookmarks."""
    bookmarks: list[Bookmark] = []
    ids = set()
    for _ in range(bookmarks_count):
        author_id: str = random.choice(users).id
        post_id: str = random.choice(posts).id
        bookmark: bookmark = Bookmark(author_id=author_id, post_id=post_id)
        if (author_id, post_id) not in ids:
            bookmarks.append(bookmark)
        ids.add((author_id, post_id))
    return bookmarks

def generate_comments(users: list[User], posts: list[Post], comments_count: int = 500) -> list[Like]:
    """Generate likes."""
    comments: list[Comment] = []
    ids = set()
    for _ in range(comments_count):
        author_id: str = random.choice(users).id
        post_id: str = random.choice(posts).id
        comment: comment = Comment(
            id='Comment_' + str(uuid4()),
            author_id=author_id, 
            post_id=post_id, 
            comment_text=fake.text() )
        if (author_id, post_id) not in ids:
            comments.append(comment)
        ids.add((author_id, post_id))
    return comments

from crewai import Agent
from tools import FindProductVideoTools, FindProductReviewTools
from langchain.llms.openai import OpenAI
from langchain.chat_models import ChatOpenAI


class ProductReviewAgents():
    def research_analyst(self):
        return Agent(
        role='Product Video Researcher',
        goal="""Find the best product review videos from youtube""",
        backstory="""Known for your indepth knowledge of various videos that 
        analyze different products on youtube. Now you have to find the best video that 
        reviews the given product.""",
        llm=OpenAI(temperature=0.7),
        verbose=True,
        tools=[
            FindProductVideoTools.find_product_video_id,
            FindProductReviewTools.find_product_reviews
        ]
  )

        return None

    def addRecentFile(self, filename):
        if filename in self.recentFiles:
            self.recentFiles.remove(filename)
        elif len(self.recentFiles) >= self.maxRecent:
            self.recentFiles.pop()
        self.recentFiles.insert(0, filename)

    # Callbacks

    def undoShapeEdit(self):
        self.canvas.restoreShape()
        self.labelList.clear()
        self.loadShapes(self.canvas.shapes)
        self.actions.undo.setEnabled(self.canvas.isShapeRestorable)

    def tutorial(self):
        url = "https://github.com/wkentaro/labelme/tree/main/examples/tutorial"  # NOQA
        webbrowser.open(url)

    def toggleDrawingSensitive(self, drawing=True):
        """Toggle drawing sensitive.

        In the middle of drawing, toggling between modes should be disabled.
        """
        self.actions.editMode.setEnabled(not drawing)
        self.actions.undoLastPoint.setEnabled(drawing)
        self.actions.undo.setEnabled(not drawing)
        self.actions.delete.setEnabled(not drawing)

    def toggleDrawMode(self, edit=True, createMode="polygon"):
        draw_actions = {
            "polygon": self.actions.createMode,
            "rectangle": self.actions.createRectangleMode,
            "circle": self.actions.createCircleMode,
            "point": self.actions.createPointMode,
            "line": self.actions.createLineMode,
            "linestrip": self.actions.createLineStripMode,
            "ai_polygon": self.actions.createAiPolygonMode,



    def __init__(self):
        super().__init__(
            encoder_path=gdown.cached_download(
                url="https://github.com/wkentaro/labelme/releases/download/sam-20230416/sam_vit_h_4b8939.quantized.encoder.onnx",  # NOQA
                md5="958b5710d25b198d765fb6b94798f49e",
            ),
            decoder_path=gdown.cached_download(
                url="https://github.com/wkentaro/labelme/releases/download/sam-20230416/sam_vit_h_4b8939.quantized.decoder.onnx",  # NOQA
                md5="a997a408347aa081b17a3ffff9f42a80",
            ),
        )


class EfficientSamVitT(EfficientSam):
    name = "EfficientSam (speed)"

    def __init__(self):
        super().__init__(
            encoder_path=gdown.cached_download(
                url="https://github.com/labelmeai/efficient-sam/releases/download/onnx-models-20231225/efficient_sam_vitt_encoder.onnx",  # NOQA
                md5="2d4a1303ff0e19fe4a8b8ede69c2f5c7",
            ),
            decoder_path=gdown.cached_download(
                url="https://github.com/labelmeai/efficient-sam/releases/download/onnx-models-20231225/efficient_sam_vitt_decoder.onnx",  # NOQA
                md5="be3575ca4ed9b35821ac30991ab01843",
            ),
        )


class EfficientSamVitS(EfficientSam):
    name = "EfficientSam (accuracy)"

    def __init__(self):
        super().__init__(
            encoder_path=gdown.cached_download(
                url="https://github.com/labelmeai/efficient-sam/releases/download/onnx-models-20231225/efficient_sam_vits_encoder.onnx",  # NOQA
                md5="7d97d23e8e0847d4475ca7c9f80da96d",
            ),
            decoder_path=gdown.cached_download(


from sqlalchemy import create_engine
from sqlalchemy.orm import DeclarativeBase, MappedAsDataclass
from sqlalchemy.orm import sessionmaker
from ...config.config import BaseConfig
from contextlib import contextmanager
from flask import current_app


class Base(MappedAsDataclass, DeclarativeBase):
    pass

SQLALCHEMY_DATABASE_URI = BaseConfig().db_conn_string
engine = create_engine(SQLALCHEMY_DATABASE_URI)
Session = sessionmaker(bind=engine, autocommit=False, autoflush=False)

def create_all():
    Base.metadata.create_all(bind=engine)
    
def drop_all():
    Base.metadata.drop_all(bind=engine)

@contextmanager
def get_db():
    try:
        db = Session()
        yield db
    finally:
        db.close()



with open(file_path, "r", encoding="utf-8") as f:
    all_comments: list[str] = json.load(fp=f)
    cleaned_comments: list[str] = list(map(clean_text, all_comments))
    comments: list[str] = choices(population=cleaned_comments, k=10)
    docs: list[Document] = [
        Document(page_content=comment)
        for comment in comments
        if is_acceptable_len(comment)
    ]
    comments: list[dict[str, str | int]] = [
        {"doc_id": i + 1, "comment": docs[i].page_content} for i in range(len(docs))
    ]

data_dir = "./agent_nelly/data_analysis/data"
features_dir = "features"
save_features_dir = path.join(data_dir, features_dir, "features.json")

with open(save_features_dir, 'r') as f:
    topics: list[str] = json.load(f)


class CustomerCommentData(BaseModel):
    doc_id: int = Field(description="The doc_id from the input")
    topics: list[str] = Field(
        description="List of the relevant topics for the customer review. Include only topics from the list provided.",
        default_factory=list,
    )
    sentiment: str = Field(
        description="Sentiment of the topic", enum=["positive", "neutral", "negative"]
    )
    

class CommentsParser(BaseModel):
    comment: list[CustomerCommentData] = Field(description="A list of the customer comment data", default_factory=list)


output_parser = PydanticOutputParser(pydantic_object=CommentsParser)
format_instructions = output_parser.get_format_instructions()


from .helpers import create_gslide_client, create_drive_client
from typing import Any
from .helpers import get_youtube_client
from ..libraries.youtube import YouTube


gslide_client: Any = create_gslide_client()
drive_client: Any = create_drive_client()
youtube_client: YouTube = get_youtube_client()





import imgviz
import numpy as np
import skimage

from labelme.logger import logger


def _get_contour_length(contour):
    contour_start = contour
    contour_end = np.r_[contour[1:], contour[0:1]]
    return np.linalg.norm(contour_end - contour_start, axis=1).sum()


def compute_polygon_from_mask(mask):
    contours = skimage.measure.find_contours(np.pad(mask, pad_width=1))
    if len(contours) == 0:
        logger.warning("No contour found, so returning empty polygon.")
        return np.empty((0, 2), dtype=np.float32)

    contour = max(contours, key=_get_contour_length)
    POLYGON_APPROX_TOLERANCE = 0.004
    polygon = skimage.measure.approximate_polygon(
        coords=contour,
        tolerance=np.ptp(contour, axis=0).max() * POLYGON_APPROX_TOLERANCE,
    )
    polygon = np.clip(polygon, (0, 0), (mask.shape[0] - 1, mask.shape[1] - 1))
    polygon = polygon[:-1]  # drop last point that is duplicate of first point

    if 0:
        import PIL.Image

        image_pil = PIL.Image.fromarray(imgviz.gray2rgb(imgviz.bool2ubyte(mask)))
        imgviz.draw.line_(image_pil, yx=polygon, fill=(0, 255, 0))
        for point in polygon:
            imgviz.draw.circle_(image_pil, center=point, diameter=10, fill=(0, 255, 0))
        imgviz.io.imsave("contour.jpg", np.asarray(image_pil))

    return polygon[:, ::-1]  # yx -> xy


    """))
  
  product_crew = ProductReviewCrew(company)
  result = product_crew.run()
  print("\n\n########################")
  print("## Here is the Report")
  print("########################\n")
  print(result)

            yield Request(link, callback=self.parse_slide, meta={"slide_item": slide_item})
        
        # next_page = response.css('a.next.page-numbers::attr(href)').get()
        # if next_page and int(next_page.split('/')[-2]) < 2:
        #     self.logger.warning(f"Crawling page number %d", int(next_page.split('/')[-2]))
        #     yield Request(next_page, callback=self.parse)
        next_page = response.css('a.next.page-numbers::attr(href)').get()
        if next_page:
            self.logger.warning(f"Crawling page number %d", int(next_page.split('/')[-2]))
            yield Request(next_page, callback=self.parse)
            
    def parse_slide(self, response: Response, **kwargs: Any) -> Any:
        slide_item = response.meta["slide_item"]
        loader = ItemLoader(item=slide_item, response=response)
        loader.add_css(field_name="tags", css=".Sm-tags a.mr-2::text")
        loader.add_css(field_name="description", css=".product-text p")
        loader.add_css(field_name="slides_count", css='h4 small::text')
        loader.add_css(field_name="colors", css='li.color a::text')
        loader.add_css(field_name="image_urls", css='a.preview-link img::attr(src)')
        # add slide link
        yield loader.load_item()

from collections import deque
from collections.abc import Iterator
from os import listdir, path
from queue import Queue

from .config import Config
from .helpers import read_src


class DirectoryIterator(Iterator):
    def __init__(self, config: Config) -> None:
        super().__init__()
        self._folders_ignore = set(config.directories_ignore)
        self._files_ignore = set(config.files_ignore)
        self._queue = deque(config.root_directory)  # adding the individual chars

    def __iter__(self) -> Iterator:
        return super().__iter__()

    def __next__(self) -> list[str]:
        if self._queue:
            files: list[str] = list()
            for _ in range(len(self._queue)):
                directory: str = self._queue.popleft()
                for entry in listdir(directory):
                    entry_path: str = path.join(directory, entry)
                    if (
                        path.isfile(entry_path)
                        and self._is_python_file(entry_path)
                        and entry not in self._files_ignore
                    ):
                        files.append(entry_path)
                    elif path.isdir(entry_path) and entry not in self._folders_ignore:
                        self._queue.append(entry_path)
            return files
        else:
            raise StopIteration()

    def _is_python_file(self, file_path: str) -> bool:
        return file_path.split(".")[-1] == "py"


    return items


def get_channel_details(channel: Search) -> Channel:
    """Get channel details"""
    response: YouTubeListResponse = youtube_client.find_channel_by_id(
        channel_id=channel.resource_id
    )
    channel: Channel = response.items[0]
    return channel


def parse_channel_details(channel: Channel) -> dict:
    return {
        "title": channel.snippet.title,
        "description": channel.snippet.description,
        "date": str(channel.snippet.published_at.date()),
        "subscribers": channel.statistics.subscribers_count,
        "videos": channel.statistics.videos_count,
    }
    
    
def get_channels(product: str, max_results: int = 10) -> list[dict]:
    channels: list[Search] = search_youtube_channels(product=product, max_results=max_results)
    channels: list[Channel] = map(get_channel_details, channels)
    channels: list[dict] = map(parse_channel_details, channels)
    return channels

def save_data(file_path: str, data: list) -> None:
    with open(file_path, 'w') as f:
        json.dump(data, f, indent=4)
        
def load_data(file_path: str) -> dict:
    with open(file_path, 'r') as f:
        data: list[dict] = json.load(f)
    return data


def create_channels_table(table_data: list[dict]) -> Table:
    table: Table = Table(row_styles=["dim", ""],leading=1, box=box.MINIMAL_DOUBLE_HEAD,


from langchain.tools import tool
from .helpers import list_video_comments
from youtube.models import Comment


class FindProductReviewTools():
    @tool
    def find_product_reviews(video_id: str) -> str:
        """Useful when you need to find a product reviews from youtube video comments."""
        comments: list[Comment] = list_video_comments(video_id)
        comments: list[str] = [comment.snippet.text_display for comment in comments]
        return ' '.join(comments)


    def popUp(self, text=None, move=True, flags=None, group_id=None, description=None):
        if self._fit_to_content["row"]:
            self.labelList.setMinimumHeight(
                self.labelList.sizeHintForRow(0) * self.labelList.count() + 2
            )
        if self._fit_to_content["column"]:
            self.labelList.setMinimumWidth(self.labelList.sizeHintForColumn(0) + 2)
        # if text is None, the previous label in self.edit is kept
        if text is None:
            text = self.edit.text()
        # description is always initialized by empty text c.f., self.edit.text
        if description is None:
            description = ""
        self.editDescription.setPlainText(description)
        if flags:
            self.setFlags(flags)
        else:
            self.resetFlags(text)
        self.edit.setText(text)
        self.edit.setSelection(0, len(text))
        if group_id is None:
            self.edit_group_id.clear()
        else:
            self.edit_group_id.setText(str(group_id))
        items = self.labelList.findItems(text, QtCore.Qt.MatchFixedString)
        if items:
            if len(items) != 1:
                logger.warning("Label list has duplicate '{}'".format(text))
            self.labelList.setCurrentItem(items[0])
            row = self.labelList.row(items[0])
            self.edit.completer().setCurrentRow(row)
        self.edit.setFocus(QtCore.Qt.PopupFocusReason)
        if move:
            self.move(QtGui.QCursor.pos())
        if self.exec_():
            return (
                self.edit.text(),
                self.getFlags(),
                self.getGroupId(),



def is_acceptable_len(text: str, l: int = 20) -> bool:
    return len(text.split()) >= l


with open(file_path, "r", encoding="utf-8") as f:
    all_comments: list[str] = json.load(fp=f)
    cleaned_comments: list[str] = list(map(clean_text, all_comments))
    comments: list[str] = choices(population=cleaned_comments, k=10)
    docs: list[Document] = [
        Document(page_content=comment)
        for comment in comments
        if is_acceptable_len(comment)
    ]
    comments: list[dict[str, str | int]] = [
        {"doc_id": i + 1, "comment": docs[i].page_content} for i in range(len(docs))
    ]

data_dir = "./agent_nelly/data_analysis/data"
features_dir = "features"
save_features_dir = path.join(data_dir, features_dir, "features.json")

with open(save_features_dir, 'r') as f:
    topics: list[str] = json.load(f)

comment: dict = choice(comments)


sentiment_msg: str = """
Below is a customer comment in JSON format with the following keys:
1. doc_id - identifier of the comment
2. comment - the user comment

Please analyze the comment and identify the sentiment. The sentiment can be negative, neutral or 
positive. Only return a single string, the sentiment.

Comment:
```
{comment}
```


# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.


        shape.description = description

        self._update_shape_color(shape)
        if shape.group_id is None:
            item.setText(
                '{} <font color="#{:02x}{:02x}{:02x}">‚óè</font>'.format(
                    html.escape(shape.label), *shape.fill_color.getRgb()[:3]
                )
            )
        else:
            item.setText("{} ({})".format(shape.label, shape.group_id))
        self.setDirty()
        if self.uniqLabelList.findItemByLabel(shape.label) is None:
            item = self.uniqLabelList.createItemFromLabel(shape.label)
            self.uniqLabelList.addItem(item)
            rgb = self._get_rgb_by_label(shape.label)
            self.uniqLabelList.setItemLabel(item, shape.label, rgb)

    def fileSearchChanged(self):
        self.importDirImages(
            self.lastOpenDir,
            pattern=self.fileSearch.text(),
            load=False,
        )

    def fileSelectionChanged(self):
        items = self.fileListWidget.selectedItems()
        if not items:
            return
        item = items[0]

        if not self.mayContinue():
            return

        currIndex = self.imageList.index(str(item.text()))
        if currIndex < len(self.imageList):
            filename = self.imageList[currIndex]
            if filename:
                self.loadFile(filename)



from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
i

options = Options()
options.add_argument("--headless=new")
driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)
driver.get("https://leetcode.com/problems/remove-linked-list-elements")
paragraphs = driver.find_elements(By.TAG_NAME, "p")
print(paragraphs)
driver.quit()

            features, labels, test_size=0.2, random_state=42, stratify=labels
        )
        return (train_features, train_labels), (test_features, test_labels)

    def save_features(self) -> DataFrame:
        pass

    def save_labels(self) -> DataFrame:
        pass

    def train_model(self, model: Model) -> float:
        (train_features, train_labels), (test_features, test_labels) = self.get_train_test_data()
        pipeline: Pipeline = Pipeline(steps=[
            ('preprocessor', self.preprocessor),
            ('classifier', model.model)
        ])
        logging.info('Queing the model "%s" for training.', model.name)
        res: AsyncResult = train_model_task.delay(pipeline, train_features, train_labels, test_features, test_labels, model.name, model.save_path)
        self.train_task_ids.append(res.id)
        return res.id
        

    def run(self) -> None:  
        self._train_results = chord((train_model_task.s(
            self.create_train_config(model=model.model, name=model.classifier_name, save_path=model.save_path)
            ) for model in self.models), send_training_report_task.s())()
      
    def get_results(self) -> list[Model]:
        """Get the training result."""
        logging.info('Getting the training results')
        print(self._train_results.get())
        
    def get_best_models(self, start: int = 0, end: int = -1) -> Model:
        best_models = redis.zrange(name=app_config.accuracy_channel, start=start, end=end, withscores=True)
        return best_models
        
    def tune_best_models(self) -> None:
        logging.info('Tuning the best models.')
        best_models = self.get_best_models(start=-3, end=-1)
        logging.info(best_models)


from queue import Queue
from threading import Thread

from .config import Config
from .file_processor import (
    generate_function_docstrings,
    queue_unprocessed_functions_methods,
    generate_class_docstrings,
)
from .helpers import get_all_modules


def generate_docstrings(
    config: Config,
    module_path_queue: Queue,
    functions_source_queue: Queue,
    class_source_queue: Queue,
    failed_modules_queue: Queue,
) -> None:
    """Generate docstrings for classes and methods."""
    queue_modules: Thread = Thread(
        target=get_all_modules,
        name='get_all_modules',
        args=(config, module_path_queue),
    )
    queue_modules.start()

    for _ in range(1):
        get_functions_source_thread: Thread = Thread(
            target=queue_unprocessed_functions_methods,
            args=(functions_source_queue, class_source_queue, module_path_queue),
            daemon=True,
        )
        get_functions_source_thread.start()

    for _ in range(1):
        generate_functions_docstring_thread: Thread = Thread(
            target=generate_function_docstrings,
            args=(functions_source_queue, config),
            daemon=True,


from youtube import YouTube

client_secrets_file = "/home/lyle/Downloads/search.json"
youtube_client = YouTube(client_secret_file=client_secrets_file)
youtube_client_object = youtube_client.authenticate()
youtube_client.youtube_client = youtube_client_object


from setuptools import find_packages, setup
from pip._vendor import tomli

# For consistent encoding
from codecs import open
from os import path

# The directory containing this file
HERE = path.abspath(path.dirname(__file__))

# Get the long description from the README file
with open(path.join(HERE, 'README.md'), encoding='utf-8') as f:
    LONG_DESCRIPTION = f.read()

with open('pyproject.toml', 'r') as f:
    VERSION = tomli.load(f)['tool']['commitizen']['version']

DESCRIPTION = 'A python library that wraps around the Google calendar API. You can use it to schedule events using google calendar.'

key_words = [
    'calendar', 'google-calendar', 'schedule events'
]

install_requires = [
    'oryks-google-oauth',
    'pydantic',
    'pydantic-settings',
    'pytz'
]

setup(
    name='oryks-google-calendar',
    packages=find_packages(
        include=[
            'google_calendar',
            'google_calendar.models',
            'google_calendar.schemas',
            'google_calendar.resources',
        ]
    ),


from datetime import datetime
from sqlalchemy.orm import Mapped, mapped_column, relationship
from ..database import Base
from sqlalchemy import ForeignKey


class View(Base):
    __tablename__ = 'views'
    
    id: Mapped[str] = mapped_column(primary_key=True)
    author_id: Mapped[str] = mapped_column(ForeignKey('users.id'))
    post_id: Mapped[str] = mapped_column(ForeignKey('posts.id'))
    view_date: Mapped[datetime] = mapped_column(default_factory=datetime.utcnow)
    
    author = relationship('User', back_populates='views')
    post = relationship('Post', back_populates='views')

            src_tree: AST = parse_src(docstring)
            func_node: FunctionDef = src_tree.body[0]
            doc_str: str = ast.get_docstring(func_node)
        except Exception:
            return super().parse(docstring)
        else:
            return doc_str


model_parser: Parser = DefaultParser()


def parse_function_docstr(func_dcstr: str) -> str:
    return model_parser.parse(docstring=func_dcstr)


from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy import Request
# from slidesmodel.items import SlidesModelItem
from scrapy.loader import ItemLoader
from scrapy.utils.project import get_project_settings
import json


class SlidesModelspider(Spider):
    name: str = "problems"
    
    def __init__(self, name: str | None = None, **kwargs: Any):
        super().__init__(name, **kwargs)
        # self.start_urls: list[str] = self.load_start_urls()
        self.start_urls: list[str] = [
            "https://www.techiedelight.com/data-structures-and-algorithms-problems/"
        ]
    
    
    def parse(self, response: Response, **kwargs: Any) -> Any:
        self.logger.info("This is my first spider.")
        problem_links = response.css('.post-problems li')
        # from random import choices
        # problem_links = choices(population=problem_links, k=100)
        # for problem_link in problem_links:
        #     # title = problem_link.css('a::text')[0].get()
        #     link = problem_link.css('a::attr(href)')[0].get()
        #     # yield{
        #     #     "link": link,
        #     #     "problem": problem
        #     # }
            # yield Request(link, callback=self.parse_problem)
        link = "https://www.techiedelight.com/single-source-shortest-paths-bellman-ford-algorithm/"
        yield Request(link, callback=self.parse_problem)
        # for slide in slides:
        #     loader: ItemLoader = ItemLoader(item=SlidesModelItem(), selector=slide)
        #     loader.add_css("title", ".item a::text")
        #     loader.add_css("category", ".category::text")


            "ai_mask": self.actions.createAiMaskMode,
        }

        self.canvas.setEditing(edit)
        self.canvas.createMode = createMode
        if edit:
            for draw_action in draw_actions.values():
                draw_action.setEnabled(True)
        else:
            for draw_mode, draw_action in draw_actions.items():
                draw_action.setEnabled(createMode != draw_mode)
        self.actions.editMode.setEnabled(not edit)

    def setEditMode(self):
        self.toggleDrawMode(True)

    def updateFileMenu(self):
        current = self.filename

        def exists(filename):
            return osp.exists(str(filename))

        menu = self.menus.recentFiles
        menu.clear()
        files = [f for f in self.recentFiles if f != current and exists(f)]
        for i, f in enumerate(files):
            icon = utils.newIcon("labels")
            action = QtWidgets.QAction(
                icon, "&%d %s" % (i + 1, QtCore.QFileInfo(f).fileName()), self
            )
            action.triggered.connect(functools.partial(self.loadRecent, f))
            menu.addAction(action)

    def popLabelListMenu(self, point):
        self.menus.labelList.exec_(self.labelList.mapToGlobal(point))

    def validateLabel(self, label):
        # no validation
        if self._config["validate_label"] is None:
            return True


from setuptools import find_packages, setup
from pip._vendor import tomli

# For consistent encoding
from codecs import open
from os import path

# The directory containing this file
HERE = path.abspath(path.dirname(__file__))

# Get the long description from the README file
with open(path.join(HERE, 'README.md'), encoding='utf-8') as f:
    LONG_DESCRIPTION = f.read()

with open('pyproject.toml', 'r') as f:
    VERSION = tomli.load(f)['tool']['commitizen']['version']

DESCRIPTION = 'A python library for working with Google Drive.'

key_words = [
    'drive', 'google-drive', 'google-drive-api', 'upload files to Google Drive',
]

install_requires = [
    'oryks-google-oauth',
    'pydantic',
    'pydantic-settings'
]

setup(
    name='oryks-google-drive',
    packages=find_packages(
        include=[
            'google_drive',
            'google_drive.exceptions',
            'google_drive.models',
            'google_drive.schemas',
            'google_drive.resources'
        ]
    ),


import whisper

model = whisper.load_model("medium.en")
result = model.transcribe("code.wav")
print(result["text"])

from scrapy import Item, Field
from itemloaders.processors import TakeFirst, MapCompose, Join
import re


def remove_html_tags(description: str) -> str:
    html_pattern = "<(?:\"[^\"]*\"['\"]*|'[^']*'['\"]*|[^'\">])+>" 
    return re.sub(html_pattern, '', description)

def remove_unicode_chars(text: str) -> str:
    return text.replace(u"\xa0", "")

def num_of_slides(text: str) -> int:
    vals = [val for val in list(text) if val.isdigit()]
    return "".join(vals)


class SlidesModelItem(Item):
    title = Field(output_processor=TakeFirst())
    category = Field(output_processor=TakeFirst())
    description = Field(
        input_processor=MapCompose(remove_html_tags, remove_unicode_chars),
        output_processor=Join()
    )
    tags = Field()
    slides_count = Field(
        input_processor=MapCompose(num_of_slides),
        output_processor=TakeFirst()
    )
    colors = Field()
    image_urls = Field()
    images = Field()


topic_assign_msg: str = """
Below is a list of customer comments in JSON format with the following keys:
1. doc_id - identifier of the comment
2. comment - the user comment

Please analyze the provided comments and identify the main topics and sentiment. Include only the 
topics mentioned in the following text:
Text: {topics}

{format_instructions}

user comments: 
```{comments}```
"""

topic_assign_tmpl = PromptTemplate(
    template=topic_assign_msg,
    input_variables=["topics", "comments", "format_instructions"],
)

with open('analysis.json', 'r') as f:
    data = json.load(f)
    
i = data[-1]["comment_id"] + 1

from time import sleep
import json
for _ in range(10):
    d = comments[i: i+3]
    x = {}
    for s in d:
        x[s['doc_id']] = s['comment']
    i += 3
    inputs = {
        "topics": topics,
        "format_instructions": format_instructions,
        "comments": json.dumps(d),
    }
    # print(d)
    # print(c)


"""This module declares the app configuration.

The classes include:

BaseConfig:
    Has all the configurations shared by all the environments.

"""
import os

from dotenv import load_dotenv

load_dotenv()


class BaseConfig:
    """Base configuration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ.get(
        "SECRET_KEY", "df0331cefc6c2b9a5d0208a726a5d1c0fd37324feba25506"
    )


class DevelopmentConfig(BaseConfig):
    """Development confuguration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ.get(
        "SECRET_KEY", "df0331cefc6c2b9a5d0208a726a5d1c0fd37324feba25506"
    )


class TestingConfig(BaseConfig):
    """Testing configuration."""

    TESTING = True
    SECRET_KEY = os.environ.get("SECRET_KEY", "secret-key")



class DirectoryIterator:
    def __init__(self, config: Config):
        self.config: Config = config
        self.queue: deque[str] = deque(self.config.path)

    def __iter__(self) -> Iterator:
        return self

    def __next__(self) -> list[str]:
        files: list[str] = list()
        if self.queue:
            for _ in range(len(self.queue)):
                root_dir: str = self.queue.popleft()
                if root_dir.split('/')[-1] in self.config.directories_ignore:
                    continue
                entries: list[str] = listdir(root_dir)
                for entry in entries:
                    entry_path: str = path.join(root_dir, entry)
                    if path.isfile(entry_path):
                        if (
                            entry_path not in self.config.files_ignore
                            and entry.split('.')[-1] == 'py'
                        ):
                            files.append(entry_path)
                    elif entry not in self.config.directories_ignore:
                        self.queue.append(entry_path)
            return files
        else:
            raise StopIteration()


class GetPosts(BaseModel):
    offset: Optional[int] = 0
    limit: Optional[int] = 10
    
class PostAuthor(BaseModel):
    id: str
    profile_picture: str
    name: str
    
class PostLike(BaseModel):
    liked: bool
    liked_by: Optional[list[PostAuthor]] = Field(default_factory=list)
    key_like: Optional[PostAuthor] = None
    likes_count: Optional[int] = Field(default=0)
    
class KeyComment(BaseModel):
    author: PostAuthor
    text: str
    comments_count: int
    
class PostSchema(BaseModel):
    id: str
    text: str
    image: str
    author: PostAuthor
    date_published: str
    location: str
    like: PostLike
    bookmarked: bool
    key_comment: Optional[KeyComment] = None

from dotenv import load_dotenv
load_dotenv()
from flask.cli import FlaskGroup
from api import create_app

app = create_app()
cli = FlaskGroup(create_app=create_app)



if __name__ == "__main__":
    cli()

    optional: CommentThreadOptionalParameters = CommentThreadOptionalParameters(
        maxResults=25
    )
    request: YouTubeRequest = YouTubeRequest(
        part=part, filter=filter, optional_parameters=optional
    )
    comment_iterator: Iterator = youtube_client.get_comments_iterator(request)
    done: bool = False
    comment_count: int = 0
    for comment_threads in comment_iterator:
        comments: list[str] = []
        if done:
            break
        for comment_thread in comment_threads:
            comment: Comment = comment_thread.snippet.top_level_comment
            comments.append(comment.snippet.text_display)
            comment_count += 1
            if comment_count > max_results:
                done = True
                break
        with open("comments.json", "r", encoding="utf-8") as f:
            existing_comments: list[str] = json.load(f)

        with open("comments.json", "w", encoding="utf-8") as f:
            existing_comments += comments
            json.dump(existing_comments, fp=f, indent=2)
    return comment_count


client_secrets_file = "/home/lyle/Downloads/search.json"
youtube_client = YouTube(client_secret_file=client_secrets_file)
youtube_client_object = youtube_client.authenticate()
youtube_client.youtube_client = youtube_client_object


# print(get_video_id(video_title='iPhone 15 Pro Review: The Good, The Bad, & The Ugly!'))
print(list_video_comments(video_id="cBpGq-vDr2Y"))



    def setShape(self, shape):
        self.setData(shape, Qt.UserRole)

    def shape(self):
        return self.data(Qt.UserRole)

    def __hash__(self):
        return id(self)

    def __repr__(self):
        return '{}("{}")'.format(self.__class__.__name__, self.text())


class StandardItemModel(QtGui.QStandardItemModel):
    itemDropped = QtCore.Signal()

    def removeRows(self, *args, **kwargs):
        ret = super().removeRows(*args, **kwargs)
        self.itemDropped.emit()
        return ret


class LabelListWidget(QtWidgets.QListView):
    itemDoubleClicked = QtCore.Signal(LabelListWidgetItem)
    itemSelectionChanged = QtCore.Signal(list, list)

    def __init__(self):
        super(LabelListWidget, self).__init__()
        self._selectedItems = []

        self.setWindowFlags(Qt.Window)
        self.setModel(StandardItemModel())
        self.model().setItemPrototype(LabelListWidgetItem())
        self.setItemDelegate(HTMLDelegate())
        self.setSelectionMode(QtWidgets.QAbstractItemView.ExtendedSelection)
        self.setDragDropMode(QtWidgets.QAbstractItemView.InternalMove)
        self.setDefaultDropAction(Qt.MoveAction)

        self.doubleClicked.connect(self.itemDoubleClickedEvent)


from qtpy import QtWidgets


class ColorDialog(QtWidgets.QColorDialog):
    def __init__(self, parent=None):
        super(ColorDialog, self).__init__(parent)
        self.setOption(QtWidgets.QColorDialog.ShowAlphaChannel)
        # The Mac native dialog does not support our restore button.
        self.setOption(QtWidgets.QColorDialog.DontUseNativeDialog)
        # Add a restore defaults button.
        # The default is set at invocation time, so that it
        # works across dialogs for different elements.
        self.default = None
        self.bb = self.layout().itemAt(1).widget()
        self.bb.addButton(QtWidgets.QDialogButtonBox.RestoreDefaults)
        self.bb.clicked.connect(self.checkRestore)

    def getColor(self, value=None, title=None, default=None):
        self.default = default
        if title:
            self.setWindowTitle(title)
        if value:
            self.setCurrentColor(value)
        return self.currentColor() if self.exec_() else None

    def checkRestore(self, button):
        if (
            self.bb.buttonRole(button) & QtWidgets.QDialogButtonBox.ResetRole
            and self.default
        ):
            self.setCurrentColor(self.default)


import os
from .config import Config
from flask import Flask


def set_configuration(app: Flask):
    """Set the application configuration.

    The application configuration will depend on the
    environment i.e Test, Development, Staging or Production.

    Parameters
    ----------
    app: flask.Flask
        A flask app instance

    Returns
    -------
    bool:
        Whether the config was set up successfully.
    """
    config_name = os.environ.get("FLASK_ENV")
    app.config.from_object(Config[config_name])

    return True

from .register_blueprints import register_blueprints

    optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
        q=video_title, maxResults=1, type=["video"]
    )
    search_request: YouTubeRequest = YouTubeRequest(
        part=part, optional_parameters=optional_parameters
    )
    search_results: YouTubeResponse = youtube_client.search(search_request)
    search_result: Search = search_results.items[0]
    return search_result.resource_id


def get_video_details(video: Search) -> Video:
    """Get video details"""
    response: YouTubeListResponse = youtube_client.find_video_by_id(video.resource_id)
    video: Video = response.items[0]
    return video


def parse_video_details(video: Video) -> dict:
    return {
        "title": video.snippet.title,
        "description": video.snippet.description,
        "date": str(video.snippet.published_at),
        "views": video.statistics.views_count,
        "comments": video.statistics.comments_count,
        "likes": video.statistics.likes_count,
    }
    
def get_videos(product: str, channel: str) -> list[dict]:
    videos: list[Search] = video_search(product=product, channel_title=channel)
    videos: list[Video] = map(get_video_details, videos)
    videos: list[dict] = map(parse_video_details, videos)
    return videos


def create_videos_table(table_data: list[dict]) -> Table:
    table: Table = Table(row_styles=["dim", ""],leading=1, box=box.MINIMAL_DOUBLE_HEAD,
         title="[bold italic gold1]Youtube videos reviewing Iphone 15 pro[/bold italic gold1]")
    table.add_column(header="[b]Video Title", justify="left", style="dark_orange")
    table.add_column(header="Views", justify="left", style="light_coral")




def validate_config_item(key, value):
    if key == "validate_label" and value not in [None, "exact"]:
        raise ValueError(
            "Unexpected value for config key 'validate_label': {}".format(value)
        )
    if key == "shape_color" and value not in [None, "auto", "manual"]:
        raise ValueError(
            "Unexpected value for config key 'shape_color': {}".format(value)
        )
    if key == "labels" and value is not None and len(value) != len(set(value)):
        raise ValueError(
            "Duplicates are detected for config key 'labels': {}".format(value)
        )


def get_config(config_file_or_yaml=None, config_from_args=None):
    # 1. default config
    config = get_default_config()

    # 2. specified as file or yaml
    if config_file_or_yaml is not None:
        config_from_yaml = yaml.safe_load(config_file_or_yaml)
        if not isinstance(config_from_yaml, dict):
            with open(config_from_yaml) as f:
                logger.info("Loading config file from: {}".format(config_from_yaml))
                config_from_yaml = yaml.safe_load(f)
        update_dict(config, config_from_yaml, validate_item=validate_config_item)

    # 3. command line argument or specified config file
    if config_from_args is not None:
        update_dict(config, config_from_args, validate_item=validate_config_item)

    return config


channel_names: list[str] = get_channel_names()
playlist_name: str = 'Daily Videos'
playlist_items: list[str] = workflow(youtube, channel_names)

# print(get_channel_id('Asianometry'))
# print(redis.setex(name='PL_26vmg8W_AcEEl_Bo2AhziS-93r6b8bu:DqkZCzjdtbw', time=1, value=''))
# print(redis.setex(name='PL_26vmg8W_AcEEl_Bo2AhziS-93r6b8bu:VzW_BtXSw6A', time=1, value=''))
# print(redis.get(name='PL_26vmg8W_AcEEl_Bo2AhziS-93r6b8bu:DqkZCzjdtbw'))
# print(find_latest_video('UC1LpsuAUaKoMzzJSEt5WImw', youtube))
# channels: list[Channel] = get_all_channels(get_db)
# latest_videos: list[Video] = [find_latest_video(channel.id, youtube) for channel in channels]
# videos: list[Video] = Video.find().all()
# for channel in channels:
#     redis.setex(f'latest:{channel.id}', value='video_str', time=1)
# for video in latest_videos:
#     pl_id: str = 'PL_26vmg8W_AcEEl_Bo2AhziS-93r6b8bu'
#     redis.setex(name=f'{pl_id}:{video.resource_id}', time=1, value='')
# for video in videos:
#     video.expire(num_seconds=1)

from crewai import Task
from textwrap import dedent

class ProductReviewTasks():
    def research(self, agent, product):
        return Task(description=dedent(f"""
            Collect and summarize the most recent comments from the 
            products review from youtube.
            Maje sure to capture the sentiment of each comment, 
            what the user liked, did not like as well as other 
            features that they wish were present.

            Your final answer MUST be a report that includes a
            comprehensive summary of the reviews, capturing 
            the most loved features.
            
            {self.__tip_section()}

            Selected product by the customer: {product}
            """),
            agent=agent
        )
    
    def __tip_section(self):
        return "If you do your BEST WORK, I'll give you a $10,000 commision!"

    long_description=LONG_DESCRIPTION,
    url='https://youtube-assistant.readthedocs.io/en/latest/',
    author='Lyle Okoth',
    author_email='lyceokoth@gmail.com',
    license='MIT',
    install_requires=install_requires,
    keywords=key_words,
    classifiers=[
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.11',
        'Programming Language :: Python :: 3.12',
        'Operating System :: OS Independent'
    ],
)


                fitWidth,
                None,
                brightnessContrast,
            ),
        )

        self.menus.file.aboutToShow.connect(self.updateFileMenu)

        # Custom context menu for the canvas widget:
        utils.addActions(self.canvas.menus[0], self.actions.menu)
        utils.addActions(
            self.canvas.menus[1],
            (
                action("&Copy here", self.copyShape),
                action("&Move here", self.moveShape),
            ),
        )

        selectAiModel = QtWidgets.QWidgetAction(self)
        selectAiModel.setDefaultWidget(QtWidgets.QWidget())
        selectAiModel.defaultWidget().setLayout(QtWidgets.QVBoxLayout())
        #
        selectAiModelLabel = QtWidgets.QLabel(self.tr("AI Model"))
        selectAiModelLabel.setAlignment(QtCore.Qt.AlignCenter)
        selectAiModel.defaultWidget().layout().addWidget(selectAiModelLabel)
        #
        self._selectAiModelComboBox = QtWidgets.QComboBox()
        selectAiModel.defaultWidget().layout().addWidget(self._selectAiModelComboBox)
        model_names = [model.name for model in MODELS]
        self._selectAiModelComboBox.addItems(model_names)
        if self._config["ai"]["default"] in model_names:
            model_index = model_names.index(self._config["ai"]["default"])
        else:
            logger.warning(
                "Default AI model is not found: %r",
                self._config["ai"]["default"],
            )
            model_index = 0
        self._selectAiModelComboBox.setCurrentIndex(model_index)
        self._selectAiModelComboBox.currentIndexChanged.connect(


    # popUp() + key_Up

    def interact():
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_Up)  # 'person' -> 'dog'  # NOQA
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_Enter)  # NOQA
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_Enter)  # NOQA

    QtCore.QTimer.singleShot(500, interact)
    label, flags, group_id, description = widget.popUp()
    assert label == "dog"
    assert flags == {}
    assert group_id is None
    assert description == ""


# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.


                self.selectedShapes[i].selected = False
                self.selectedShapes[i] = shape
        else:
            for i, shape in enumerate(self.selectedShapesCopy):
                self.selectedShapes[i].points = shape.points
        self.selectedShapesCopy = []
        self.repaint()
        self.storeShapes()
        return True

    def hideBackroundShapes(self, value):
        self.hideBackround = value
        if self.selectedShapes:
            # Only hide other shapes if there is a current selection.
            # Otherwise the user will not be able to select a shape.
            self.setHiding(True)
            self.update()

    def setHiding(self, enable=True):
        self._hideBackround = self.hideBackround if enable else False

    def canCloseShape(self):
        return self.drawing() and self.current and len(self.current) > 2

    def mouseDoubleClickEvent(self, ev):
        if self.double_click != "close":
            return

        if (
            self.createMode == "polygon" and self.canCloseShape()
        ) or self.createMode in ["ai_polygon", "ai_mask"]:
            self.finalise()

    def selectShapes(self, shapes):
        self.setHiding()
        self.selectionChanged.emit(shapes)
        self.update()

    def selectShapePoint(self, point, multiple_selection_mode):
        """Select the first shape created which contains this point."""


        self.labelList.itemSelectionChanged.connect(self.labelSelectionChanged)
        self.labelList.itemDoubleClicked.connect(self.editLabel)
        self.labelList.itemChanged.connect(self.labelItemChanged)
        self.labelList.itemDropped.connect(self.labelOrderChanged)
        self.shape_dock = QtWidgets.QDockWidget(self.tr("Polygon Labels"), self)
        self.shape_dock.setObjectName("Labels")
        self.shape_dock.setWidget(self.labelList)

        self.uniqLabelList = UniqueLabelQListWidget()
        self.uniqLabelList.setToolTip(
            self.tr(
                "Select label to start annotating for it. " "Press 'Esc' to deselect."
            )
        )
        if self._config["labels"]:
            for label in self._config["labels"]:
                item = self.uniqLabelList.createItemFromLabel(label)
                self.uniqLabelList.addItem(item)
                rgb = self._get_rgb_by_label(label)
                self.uniqLabelList.setItemLabel(item, label, rgb)
        self.label_dock = QtWidgets.QDockWidget(self.tr("Label List"), self)
        self.label_dock.setObjectName("Label List")
        self.label_dock.setWidget(self.uniqLabelList)

        self.fileSearch = QtWidgets.QLineEdit()
        self.fileSearch.setPlaceholderText(self.tr("Search Filename"))
        self.fileSearch.textChanged.connect(self.fileSearchChanged)
        self.fileListWidget = QtWidgets.QListWidget()
        self.fileListWidget.itemSelectionChanged.connect(self.fileSelectionChanged)
        fileListLayout = QtWidgets.QVBoxLayout()
        fileListLayout.setContentsMargins(0, 0, 0, 0)
        fileListLayout.setSpacing(0)
        fileListLayout.addWidget(self.fileSearch)
        fileListLayout.addWidget(self.fileListWidget)
        self.file_dock = QtWidgets.QDockWidget(self.tr("File List"), self)
        self.file_dock.setObjectName("Files")
        fileListWidget = QtWidgets.QWidget()
        fileListWidget.setLayout(fileListLayout)
        self.file_dock.setWidget(fileListWidget)



# TODO(unknown):
# - Zoom is too "steppy".


LABEL_COLORMAP = imgviz.label_colormap()


class MainWindow(QtWidgets.QMainWindow):
    FIT_WINDOW, FIT_WIDTH, MANUAL_ZOOM = 0, 1, 2

    def __init__(
        self,
        config=None,
        filename=None,
        output=None,
        output_file=None,
        output_dir=None,
    ):
        if output is not None:
            logger.warning("argument output is deprecated, use output_file instead")
            if output_file is None:
                output_file = output

        # see labelme/config/default_config.yaml for valid configuration
        if config is None:
            config = get_config()
        self._config = config

        # set default shape colors
        Shape.line_color = QtGui.QColor(*self._config["shape"]["line_color"])
        Shape.fill_color = QtGui.QColor(*self._config["shape"]["fill_color"])
        Shape.select_line_color = QtGui.QColor(
            *self._config["shape"]["select_line_color"]
        )
        Shape.select_fill_color = QtGui.QColor(
            *self._config["shape"]["select_fill_color"]
        )
        Shape.vertex_fill_color = QtGui.QColor(
            *self._config["shape"]["vertex_fill_color"]
        )


full_chain = {
    "sentiment": sentiment_chain,
    "comment": lambda input: input['comment'],
    "topics": lambda input: input['topics']
} | branch

res = full_chain.invoke({'comment': comment, "topics": topics})
print(comment)
print(res)


        self.canvas.setEnabled(True)
        # set zoom values
        is_initial_load = not self.zoom_values
        if self.filename in self.zoom_values:
            self.zoomMode = self.zoom_values[self.filename][0]
            self.setZoom(self.zoom_values[self.filename][1])
        elif is_initial_load or not self._config["keep_prev_scale"]:
            self.adjustScale(initial=True)
        # set scroll values
        for orientation in self.scroll_values:
            if self.filename in self.scroll_values[orientation]:
                self.setScroll(
                    orientation, self.scroll_values[orientation][self.filename]
                )
        # set brightness contrast values
        dialog = BrightnessContrastDialog(
            utils.img_data_to_pil(self.imageData),
            self.onNewBrightnessContrast,
            parent=self,
        )
        brightness, contrast = self.brightnessContrast_values.get(
            self.filename, (None, None)
        )
        if self._config["keep_prev_brightness"] and self.recentFiles:
            brightness, _ = self.brightnessContrast_values.get(
                self.recentFiles[0], (None, None)
            )
        if self._config["keep_prev_contrast"] and self.recentFiles:
            _, contrast = self.brightnessContrast_values.get(
                self.recentFiles[0], (None, None)
            )
        if brightness is not None:
            dialog.slider_brightness.setValue(brightness)
        if contrast is not None:
            dialog.slider_contrast.setValue(contrast)
        self.brightnessContrast_values[self.filename] = (brightness, contrast)
        if brightness is not None or contrast is not None:
            dialog.onNewValue(None)
        self.paintCanvas()
        self.addRecentFile(self.filename)


        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesn‚Äôt have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


class SlidesgoDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called


            self.client_secret_file = client_secret_file
        if not self.client_secret_file:
            raise ValueError('The client secret file must be provided.')
        api_service_name: str = 'drive'
        api_version: str = 'v3'
        credentials_dir: str = '.drive_credentials'
        scopes: list[str] = [
            GoogleDriveScopes.metadata.value,
            GoogleDriveScopes.drive.value,
            GoogleDriveScopes.files.value,
            GoogleDriveScopes.activity.value,
        ]
        oauth: GoogleOAuth = GoogleOAuth(
            secrets_file=self.client_secret_file,
            scopes=scopes,
            api_service_name=api_service_name,
            api_version=api_version,
            credentials_dir=credentials_dir,
        )
        self.drive_client = oauth.authenticate_google_server()
        return self.drive_client

    def create_file(self) -> None:
        """Creates a new file on drive."""
        raise NotImplementedError()
    
    def upload_file(self) -> None:
        """Upload a file to drive."""
        raise NotImplementedError()
    
    def resumable_upload(self) -> None:
        raise NotImplementedError()


def send_email_local(user_email_address: str, message: str) -> None:
    pass

def send_email_aws_ses(user_email_address: str, message: str) -> None:
    pass

def send_account_activation_email(user_email_address: str, message: str) -> None:
    pass

def send_password_reset_email(user_email_address: str, message: str) -> None:
    pass

def generate_account_activation_email(message: str) -> None:
    pass

def generate_password_reset_email(message: str) -> None:
    pass

import os.path as osp

import numpy as np
import PIL.Image

from labelme.utils import image as image_module

from .util import data_dir
from .util import get_img_and_data


def test_img_b64_to_arr():
    img, _ = get_img_and_data()
    assert img.dtype == np.uint8
    assert img.shape == (907, 1210, 3)


def test_img_arr_to_b64():
    img_file = osp.join(data_dir, "annotated_with_data/apc2016_obj3.jpg")
    img_arr = np.asarray(PIL.Image.open(img_file))
    img_b64 = image_module.img_arr_to_b64(img_arr)
    img_arr2 = image_module.img_b64_to_arr(img_b64)
    np.testing.assert_allclose(img_arr, img_arr2)


def test_img_data_to_png_data():
    img_file = osp.join(data_dir, "annotated_with_data/apc2016_obj3.jpg")
    with open(img_file, "rb") as f:
        img_data = f.read()
    png_data = image_module.img_data_to_png_data(img_data)
    assert isinstance(png_data, bytes)


from redis import Redis
from config.config import app_config
from celery import Celery
from utils import extract_dataset
from schemas import Model, TrainedModel, TunedModel
import logging
from schemas import Metrics
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score
from time import perf_counter
from sklearn.pipeline import Pipeline
from experiment_param_grids import hyperparameters
from sklearn.model_selection import GridSearchCV
from sklearn.base import BaseEstimator
from schemas.train_config import TrainConfig
from os import path
from utils import send_email


redis: Redis = Redis(host=app_config.redis.redis_host, port=app_config.redis.redis_port, decode_responses=True)
celery = Celery(__name__)
celery.conf.broker_url = app_config.celery_broker_url
celery.conf.result_backend = app_config.celery_result_backend
celery.conf.event_serializer = 'pickle' # this event_serializer is optional. somehow i missed this when writing this solution and it still worked without.
celery.conf.task_serializer = 'pickle'
celery.conf.result_serializer = 'pickle'
celery.conf.accept_content = ['application/json', 'application/x-python-serialize']


@celery.task(name='send_training_report_task')
def send_training_report_task(training_result):
    try:
        logging.info('Sending the email')
        send_email()
    except Exception as e:
        logging.error(f'Unable to send email: {str(e)}')
    else:
        logging.info('Email sent')
    return training_result



import ast
from ast import FunctionDef
from queue import Queue

from .helpers import read_src


class FunctionVisitor(ast.NodeVisitor):
    def __init__(self, function_code_queue: Queue, file_path: str) -> None:
        super().__init__()
        self._function_code_queue = function_code_queue
        self._file_path = file_path

    def visit_FunctionDef(self, node: FunctionDef) -> None:
        function_code: str = ast.unparse(ast_obj=node)
        self._function_code_queue.put((self._file_path, function_code))


from celery import Celery
from config import CeleryConfig


celery_app: Celery = Celery(__name__)
celery_app.config_from_object(CeleryConfig)
celery_app.conf.beat_schedule = {
        'clear-daily-playlist': {
            'task': 'tasks.clear_daily_playlist',
            'schedule': 10
        }
    }
celery_app.autodiscover_tasks(['tasks'])


            self.tr("Zoom follows window width"),
            checkable=True,
            enabled=False,
        )
        brightnessContrast = action(
            "&Brightness Contrast",
            self.brightnessContrast,
            None,
            "color",
            "Adjust brightness and contrast",
            enabled=False,
        )
        # Group zoom controls into a list for easier toggling.
        zoomActions = (
            self.zoomWidget,
            zoomIn,
            zoomOut,
            zoomOrg,
            fitWindow,
            fitWidth,
        )
        self.zoomMode = self.FIT_WINDOW
        fitWindow.setChecked(Qt.Checked)
        self.scalers = {
            self.FIT_WINDOW: self.scaleFitWindow,
            self.FIT_WIDTH: self.scaleFitWidth,
            # Set to one to scale to 100% when loading files.
            self.MANUAL_ZOOM: lambda: 1,
        }

        edit = action(
            self.tr("&Edit Label"),
            self.editLabel,
            shortcuts["edit_label"],
            "edit",
            self.tr("Modify the label of the selected polygon"),
            enabled=False,
        )

        fill_drawing = action(




def create_application_config(args: Namespace) -> Config:
    config: Config = Config(
        root_directory=set(args.path),
        overwrite_function_docstring=args.overwrite_function_docstring,
        documentation_style=args.documentation_style,
    )
    config.directories_ignore.update(set(args.directories_ignore))
    config.files_ignore.update(set(args.files_ignore))
    return config


    n_classes = 4
    maizenet = MaizeNet(n_classes)
    maizenet.load_state_dict(torch.load(model_path, map_location=torch.device('cpu') ))
    return maizenet

def preprocess_image(image):
    mean = np.array([0.5, 0.5, 0.5])
    std = np.array([0.25, 0.25, 0.25])
    data_transform = transforms.Compose([
            transforms.RandomResizedCrop(224), # resize and crop image to 224 x 224 pixels
            transforms.RandomHorizontalFlip(), # flip the images horizontally
            transforms.ToTensor(), # convert to pytorch tensor data type
            transforms.Normalize(mean, std) # normalize the input image dataset.
        ])
    transformed_image = data_transform(image).to('cpu')
    transformed_image = torch.unsqueeze(transformed_image, 0)
    return transformed_image

def evaluate_image(image, model):
    transformed_image = preprocess_image(image)
    labels = ['Maize Leaf Rust', 'Northern Leaf Blight', 'Healthy', 'Gray Leaf Spot']
    model.eval()
    prediction = F.softmax(model(transformed_image), dim = 1)
    data = {
        'Maize Leaf Rust': round(float(prediction[0][0]), 4) * 100,
        'Northern Leaf Blight': round(float(prediction[0][1]) * 100, 4),
        'Healthy': round(float(prediction[0][2]), 4) * 100,
        'Gray Leaf Spot': round(float(prediction[0][3]) * 100, 4)
    }
    prediction = prediction.argmax()
    return labels[prediction], data


        #     slide_item = loader.load_item()
        #     link = slide.css(".item a::attr(href)").get()
        #     self.logger.info("Parsing the slide")
        #     yield Request(link, callback=self.parse_slide, meta={"slide_item": slide_item})
        
            
    def parse_problem(self, response: Response, **kwargs: Any) -> Any:
        # slide_item = response.meta["slide_item"]
        # loader = ItemLoader(item=slide_item, response=response)
        # loader.add_css(field_name="tags", css=".Sm-tags a.mr-2::text")
        # loader.add_css(field_name="description", css=".product-text p")
        # loader.add_css(field_name="slides_count", css='h4 small::text')
        # loader.add_css(field_name="colors", css='li.color a::text')
        # loader.add_css(field_name="image_urls", css='a.preview-link img::attr(src)')
        # add slide link
        # yield loader.load_item()
        categories: list[dict] = []
        cats = response.css('span.cat-links a')
        for cat in cats:
            category = cat.css('::text').get()
            category_link = cat.css('::attr(href)').get()
            categories.append({
                "category": category,
                "link": category_link
            })
        
        yield {
            "categories": categories,
            "title": response.css('h1::text').get(),
            # "problem": response.css('.post-content p').getall(),
            "conditions": response.css('.post-content ol').get(),
            # "io": response.css('.io').get(),
            # "solutions": response.css('h2::text').getall(), 
            # "link": response.url,
            # "code": response.css('.c-line').getall(),
            "image": response.css('.post-content p img::attr(src)').get()
        }

#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = "httpcache"
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"

# Set settings whose default value is deprecated to a future-proof value
REQUEST_FINGERPRINTER_IMPLEMENTATION = "2.7"
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
FEED_EXPORT_ENCODING = "utf-8"

IMAGES_URLS_FIELD = "image_urls"
IMAGES_RESULT_FIELD = "images"
IMAGES_STORE = "/home/lyle/oryks/scrapy-tutorial/slidesmodel/images"
CONNECTION_STRING = "sqlite:////home/lyle/oryks/scrapy-tutorial/data/slides.db"
START_URLS_PATH = "/home/lyle/oryks/scrapy-tutorial/links.json"


# utils.py

from playwright.sync_api import sync_playwright
import uuid
from PIL import Image
from PIL import Image
import io
from os import path
import json

index: int = 1

def take_screenshot_from_url(url, session_data):
    with sync_playwright() as playwright:
        webkit = playwright.webkit
        browser = webkit.launch()
        browser_context = browser.new_context(device_scale_factor=2)
        browser_context.add_cookies([session_data])
        page = browser_context.new_page()
        page.goto(url)
        screenshot_bytes = page.locator(".code").screenshot()
        browser.close()
        return screenshot_bytes
    
    
def save_data(image_bytes: bytes, code: str) -> None:
    file_name: str = str(uuid.uuid4())
    image: Image = Image.open(io.BytesIO(image_bytes))
    file_path: str = "data"
    image_path: str = path.join(file_path, f"{file_name}.png")
    image.save(image_path)
    code_path: str = path.join(file_path, "metadata.jsonl")
    metadata: dict = {
        "file_name": f"{file_name}.png",
        "code": code
    }
    with open(code_path, "a+", encoding="utf-8") as f:
        f.write(json.dumps(metadata) + "\n")

        api_version=api_version,
        credentials_dir=credentials_dir,
        credentials_file_name=credentials_file_name
    )

    gslides_client = auth.authenticate_google_server()
    return gslides_client


def create_drive_client() -> Any:
    secrets_file: str = "/home/lyle/oryks/backend/api/libraries/drive.json"
    scopes: list[str] = [
        GoogleDriveScopes.metadata.value,
        GoogleDriveScopes.drive.value,
        GoogleDriveScopes.files.value
    ]
    api_service_name: str = "drive"
    api_version: str = "v3"
    credentials_dir: str = GoogleDirectories.drive.value
    credentials_file_name: Optional[str] = 'credentials.json'

    auth: GoogleOAuth = GoogleOAuth(
        secrets_file=secrets_file,
        scopes=scopes,
        api_service_name=api_service_name,
        api_version=api_version,
        credentials_dir=credentials_dir,
        credentials_file_name=credentials_file_name
    )

    drive_client = auth.authenticate_google_server()
    return drive_client


def get_youtube_client() -> YouTube:
    client_secrets_file: str = "/home/lyle/oryks/backend/api/libraries/youtube.json"
    youtube: YouTube = YouTube(client_secret_file=client_secrets_file)
    return youtube

youtube_client: YouTube = get_youtube_client()


from setuptools import find_packages, setup
from pip._vendor import tomli

# For consistent encoding
from codecs import open
from os import path

# The directory containing this file
HERE = path.abspath(path.dirname(__file__))

# Get the long description from the README file
with open(path.join(HERE, 'README.md'), encoding='utf-8') as f:
    LONG_DESCRIPTION = f.read()

with open('pyproject.toml', 'r') as f:
    VERSION = tomli.load(f)['tool']['commitizen']['version']

DESCRIPTION = 'A python library for authenticating requests for various google services including ``gmail``, ``youtube``, ``drive`` and ``calendar``.'

key_words = [
    'google-auth',
]

install_requires = [
    'google-api-python-client',
    'google-auth-oauthlib',
    'pydantic',
    'pydantic-settings'
]

setup(
    name='oryks-google-oauth',
    packages=find_packages(
        include=[
            'oryks_google_oauth',
        ]
    ),
    version=VERSION,
    description=DESCRIPTION,
    long_description_content_type='text/markdown',


    # React to canvas signals.
    def shapeSelectionChanged(self, selected_shapes):
        self._noSelectionSlot = True
        for shape in self.canvas.selectedShapes:
            shape.selected = False
        self.labelList.clearSelection()
        self.canvas.selectedShapes = selected_shapes
        for shape in self.canvas.selectedShapes:
            shape.selected = True
            item = self.labelList.findItemByShape(shape)
            self.labelList.selectItem(item)
            self.labelList.scrollToItem(item)
        self._noSelectionSlot = False
        n_selected = len(selected_shapes)
        self.actions.delete.setEnabled(n_selected)
        self.actions.duplicate.setEnabled(n_selected)
        self.actions.copy.setEnabled(n_selected)
        self.actions.edit.setEnabled(n_selected == 1)

    def addLabel(self, shape):
        if shape.group_id is None:
            text = shape.label
        else:
            text = "{} ({})".format(shape.label, shape.group_id)
        label_list_item = LabelListWidgetItem(text, shape)
        self.labelList.addItem(label_list_item)
        if self.uniqLabelList.findItemByLabel(shape.label) is None:
            item = self.uniqLabelList.createItemFromLabel(shape.label)
            self.uniqLabelList.addItem(item)
            rgb = self._get_rgb_by_label(shape.label)
            self.uniqLabelList.setItemLabel(item, shape.label, rgb)
        self.labelDialog.addLabelHistory(shape.label)
        for action in self.actions.onShapesPresent:
            action.setEnabled(True)

        self._update_shape_color(shape)
        label_list_item.setText(
            '{} <font color="#{:02x}{:02x}{:02x}">‚óè</font>'.format(
                html.escape(text), *shape.fill_color.getRgb()[:3]
            )


#     part=part, 
#     optional_parameters=optional_parameters
# )
# search_results: YouTubeResponse = youtube.search(search_request)
# print(search_results)
# print(youtube.find_my_channel())
# part: CommentThreadPart = CommentThreadPart()
# filter: CommentThreadFilter = CommentThreadFilter(
#     videoId='Tuc-rjJbsXU'
# )
# optional: CommentThreadOptionalParameters = CommentThreadOptionalParameters(
#     maxResults=5
# )
# request:YouTubeRequest = YouTubeRequest(
#     part=part,
#     filter=filter,
#     optional_parameters=optional
# )
# comment_iterator: Iterator = youtube.get_comments_iterator(request)
# video_comments: list[Comment] = list()
# for comment_threads in comment_iterator:
#     for comment_thread in comment_threads:
#         comment: Comment = comment_thread.snippet.top_level_comment
#         video_comments.append(comment)
# print(video_comments)
# comment_id: str = 'UgzdXi_vWhXLkBA_Pwt4AaABAg'
# response = youtube.get_comment(comment_id)
# print(response)
# import json

# with open('comment.json', 'w') as f:
#     json.dump(response, f, indent=4)
# from youtube.resources.comment_thread.comment import CommentResource
# import json
# comment_res = CommentResource(youtube_client)
# with open('comment.json', 'r') as f:
#     comments = json.load(f)
# print(comment_res.parse_youtube_list_response(comments))
# replies = youtube.get_comment_replies('UgxwXLTWugMg7IEoKgR4AaABAg')
# import json


    for shape in sorted(data["shapes"], key=lambda x: x["label"]):
        label_name = shape["label"]
        if label_name in label_name_to_value:
            label_value = label_name_to_value[label_name]
        else:
            label_value = len(label_name_to_value)
            label_name_to_value[label_name] = label_value
    lbl, _ = utils.shapes_to_label(img.shape, data["shapes"], label_name_to_value)

    label_names = [None] * (max(label_name_to_value.values()) + 1)
    for name, value in label_name_to_value.items():
        label_names[value] = name

    lbl_viz = imgviz.label2rgb(
        lbl, imgviz.asgray(img), label_names=label_names, loc="rb"
    )

    PIL.Image.fromarray(img).save(osp.join(out_dir, "img.png"))
    utils.lblsave(osp.join(out_dir, "label.png"), lbl)
    PIL.Image.fromarray(lbl_viz).save(osp.join(out_dir, "label_viz.png"))

    with open(osp.join(out_dir, "label_names.txt"), "w") as f:
        for lbl_name in label_names:
            f.write(lbl_name + "\n")

    logger.info("Saved to: {}".format(out_dir))


if __name__ == "__main__":
    main()


from dotenv import load_dotenv
load_dotenv()
from flask.cli import FlaskGroup
from api import create_app

app = create_app()
cli = FlaskGroup(create_app=create_app)



if __name__ == "__main__":
    cli()

import os.path as osp
from math import sqrt

import numpy as np
from qtpy import QtCore
from qtpy import QtGui
from qtpy import QtWidgets

here = osp.dirname(osp.abspath(__file__))


def newIcon(icon):
    icons_dir = osp.join(here, "../icons")
    return QtGui.QIcon(osp.join(":/", icons_dir, "%s.png" % icon))


def newButton(text, icon=None, slot=None):
    b = QtWidgets.QPushButton(text)
    if icon is not None:
        b.setIcon(newIcon(icon))
    if slot is not None:
        b.clicked.connect(slot)
    return b


def newAction(
    parent,
    text,
    slot=None,
    shortcut=None,
    icon=None,
    tip=None,
    checkable=False,
    enabled=True,
    checked=False,
):
    """Create a new action and assign callbacks, shortcuts, etc."""
    a = QtWidgets.QAction(text, parent)
    if icon is not None:
        a.setIconText(text.replace(" ", "\n"))


import os
from .config import Config
from flask import Flask


def set_configuration(app: Flask):
    """Set the application configuration.

    The application configuration will depend on the
    environment i.e Test, Development, Staging or Production.

    Parameters
    ----------
    app: flask.Flask
        A flask app instance

    Returns
    -------
    bool:
        Whether the config was set up successfully.
    """
    config_name = os.environ.get("FLASK_ENV")
    app.config.from_object(Config[config_name])

    return True

#     sort_key=lambda x: len(x.src),
#     device=device,
# )

# encoder_net = Encoder(
#     input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout
# ).to(device)

# decoder_net = Decoder(
#     input_size_decoder,
#     decoder_embedding_size,
#     hidden_size,
#     output_size,
#     num_layers,
#     dec_dropout,
# ).to(device)

# model = Seq2Seq(encoder_net, decoder_net).to(device)
# optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# pad_idx = english.vocab.stoi["<pad>"]
# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)

# if load_model:
#     load_checkpoint(torch.load("my_checkpoint.pth.tar"), model, optimizer)


# sentence = "ein boot mit mehreren m√§nnern darauf wird von einem gro√üen pferdegespann ans ufer gezogen."

# for epoch in range(num_epochs):
#     print(f"[Epoch {epoch} / {num_epochs}]")

#     checkpoint = {"state_dict": model.state_dict(), "optimizer": optimizer.state_dict()}
#     save_checkpoint(checkpoint)

#     model.eval()

#     translated_sentence = translate_sentence(
#         model, sentence, german, english, device, max_length=50
#     )


            mask = labelme.utils.shape_to_mask(img.shape[:2], points, shape_type)

            if group_id is None:
                group_id = uuid.uuid1()

            instance = (label, group_id)

            if instance in masks:
                masks[instance] = masks[instance] | mask
            else:
                masks[instance] = mask

            if shape_type == "rectangle":
                (x1, y1), (x2, y2) = points
                x1, x2 = sorted([x1, x2])
                y1, y2 = sorted([y1, y2])
                points = [x1, y1, x2, y1, x2, y2, x1, y2]
            if shape_type == "circle":
                (x1, y1), (x2, y2) = points
                r = np.linalg.norm([x2 - x1, y2 - y1])
                # r(1-cos(a/2))<x, a=2*pi/N => N>pi/arccos(1-x/r)
                # x: tolerance of the gap between the arc and the line segment
                n_points_circle = max(int(np.pi / np.arccos(1 - 1 / r)), 12)
                i = np.arange(n_points_circle)
                x = x1 + r * np.sin(2 * np.pi / n_points_circle * i)
                y = y1 + r * np.cos(2 * np.pi / n_points_circle * i)
                points = np.stack((x, y), axis=1).flatten().tolist()
            else:
                points = np.asarray(points).flatten().tolist()

            segmentations[instance].append(points)
        segmentations = dict(segmentations)

        for instance, mask in masks.items():
            cls_name, group_id = instance
            if cls_name not in class_name_to_id:
                continue
            cls_id = class_name_to_id[cls_name]

            mask = np.asfortranarray(mask.astype(np.uint8))


        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("Calculator does not support async")


class YouTubeChannelVideoSearchTool(BaseTool):
    name = "youtube_channel_video_search"
    description = "useful for when you need to answer questions about videos for a youtube channel"
    args_schema: Type[BaseModel] = YouTubeChannelSearch

    def _run(
        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        return ''

    async def _arun(
        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("Calculator does not support async")

llm = ChatOpenAI(
    temperature=0, 
    openai_api_key=config.open_ai_token,
    )

tools = [
    YouTubeChannelTitleSearchTool(), 
    YouTubeChannelVideoSearchTool(),
    YouTubeChannelSearchTool()
    ]
agent = initialize_agent(
    tools, 
    llm, 
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
    verbose=True,
    handle_parsing_errors=True
)


from .set_config import set_configuration

import ast
import os
import subprocess
from argparse import ArgumentParser, Namespace
from ast import AsyncFunctionDef, ClassDef, Constant, Expr, FunctionDef
from collections import deque
from os import listdir, path
from queue import Queue
from typing import Iterator

from langchain.prompts import PromptTemplate

from .config import Config
from .extensions import llm
from .templates import get_function_prompt_template, get_class_prompt_template


def generate_function_docstring(function_code: str, config: Config) -> str:
    prompt_formatted_str: str = get_function_prompt_template(
        function_code=function_code, config=config
    )
    function_and_docstring = llm.invoke(prompt_formatted_str)
    return function_and_docstring


def generate_class_docstring(class_code: str, config: Config) -> str:
    prompt_formatted_str: str = get_class_prompt_template(
        class_code=class_code, config=config
    )
    class_and_docstring = llm.invoke(prompt_formatted_str)
    return class_and_docstring


def get_class_docstring(class_and_docstring: str) -> str:
    """Get the class docstring."""
    class_tree = ast.parse(class_and_docstring)
    for node in class_tree.body:
        if isinstance(node, ClassDef):
            cls_docstring: str = ast.get_docstring(node)
            return cls_docstring


def activate_user_account(session: Session, activation_data: ActivateUser):
    with session() as db:
        user: User = db.query(User).filter(User.id == activation_data.user_id).first()
        if user.id == User.decode_auth_token(activation_data.activation_token):
            user.activated = True
            db.commit()
            return True
    raise InvalidTokenError('Invalid or Expired token.')


def loggin_user(session: Session, login_data: LoginUser):
    with session() as db:
        user: User = db.query(User).filter(User.email_address == login_data.email_address).first()
        if user and user.check_password(login_data.password):
            return True
    raise ValueError('Invalid email address and or password.')


            self.addLabel(shape)
        self.labelList.clearSelection()
        self._noSelectionSlot = False
        self.canvas.loadShapes(shapes, replace=replace)

    def loadLabels(self, shapes):
        s = []
        for shape in shapes:
            label = shape["label"]
            points = shape["points"]
            shape_type = shape["shape_type"]
            flags = shape["flags"]
            description = shape.get("description", "")
            group_id = shape["group_id"]
            other_data = shape["other_data"]

            if not points:
                # skip point-empty shape
                continue

            shape = Shape(
                label=label,
                shape_type=shape_type,
                group_id=group_id,
                description=description,
                mask=shape["mask"],
            )
            for x, y in points:
                shape.addPoint(QtCore.QPointF(x, y))
            shape.close()

            default_flags = {}
            if self._config["label_flags"]:
                for pattern, keys in self._config["label_flags"].items():
                    if re.match(pattern, label):
                        for key in keys:
                            default_flags[key] = False
            shape.flags = default_flags
            shape.flags.update(flags)
            shape.other_data = other_data


        dest="config",
        help="config file or yaml-format string (default: {})".format(
            default_config_file
        ),
        default=default_config_file,
    )
    # config for the gui
    parser.add_argument(
        "--nodata",
        dest="store_data",
        action="store_false",
        help="stop storing image data to JSON file",
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--autosave",
        dest="auto_save",
        action="store_true",
        help="auto save",
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--nosortlabels",
        dest="sort_labels",
        action="store_false",
        help="stop sorting labels",
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--flags",
        help="comma separated list of flags OR file containing flags",
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--labelflags",
        dest="label_flags",
        help=r"yaml string of label specific flags OR file containing json "
        r"string of label specific flags (ex. {person-\d+: [male, tall], "
        r"dog-\d+: [black, brown, white], .*: [occluded]})",  # NOQA
        default=argparse.SUPPRESS,


from sqlalchemy.orm import Session
from ..models.post import Post
from ..schemas.post import (
    CreatePost, GetPosts, GetPost, UpdatePost
)
from werkzeug.datastructures import FileStorage
from flask import current_app
from uuid import uuid4
from werkzeug.utils import secure_filename
import os
import secrets
from typing import Callable

def create_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = Like(
            author_id=activity.user_id,
            post_id=activity.post_id
        )
        db.add(like)
        db.commit()
        db.refresh(like)
    return like

            session.add(slide)
            session.commit()

        except:
            session.rollback()
            raise

        finally:
            session.close()

        return item


class DuplicatesPipeline(object):

    def __init__(self):
        """
        Initializes database connection and sessionmaker.
        Creates tables.
        """
        engine = db_connect()
        create_table(engine)
        self.Session = sessionmaker(bind=engine)
        logging.info("****DuplicatesPipeline: database connected****")

    def process_item(self, item: Item, spider: Spider):
        session = self.Session()
        exist_slide = session.query(Slide).filter_by(title=item["title"]).first()
        session.close()
        if exist_slide is not None:  # the current slide exists
            raise DropItem("Duplicate item found: %s" % item["title"])
        else:
            return item

@post.route("/views", methods=["GET"])
def get_post_views():
    """Get a posts comments."""
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post: Post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        views: list[View] = list_post_views(session=get_db, post_data=post_data)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    resp = [
        RepeatableActivityCreated(
            user_id=view.author_id,
            post_id=view.post_id,
            date_created=view.view_date,
            id=view.id
        ).model_dump()
        for view in views
    ]
    return resp, HTTPStatus.OK

def has_viewed(session: Session, activity: CreateActivity) -> View:
    with session() as db:
        view: View = db.query(View).filter(View.author_id==activity.user_id, View.post_id==activity.post_id).first()
        if view:
            return True
    return False

def list_user_views(session: Session, user_data: GetUser) -> list[View]:
    with session() as db:
        user: User = db.query(User).filter(User.id == user_data.user_id).first()
        views: list[View] = user.views
    return views

def list_post_views(session: Session, post_data: GetPost):
    with session() as db:
        post: Post = db.query(Post).filter(Post.id == post_data.post_id).first()
        views: list[View] = post.views
    return views

                function_name=function_name,
                function_code=function_code,
                config=config,
            )
            new_tree = transformer.visit(module_tree)
            ast.fix_missing_locations(new_tree)
            new_module_code = ast.unparse(new_tree)
        except Empty:
            continue
        except Exception as e:
            print(e)
            functions_source_queue.task_done()
            continue
        else:
            save_processed_file(
                file_path=module_path, processed_module_code=new_module_code
            )
            format_file(module_path)
            functions_source_queue.task_done()

def generate_class_docstrings(class_source_queue: Queue, config: Config) -> None:
    """Generate docstrings for this file."""
    while True:
        try:
            module_path, class_name, class_code = class_source_queue.get()
            module_tree = ast.parse(get_module_source_code(module_path))
            transformer = ClassDocStringWriter(
                module_path=module_path,
                class_name=class_name,
                class_code=class_code,
                config=config,
            )
            new_tree = transformer.visit(module_tree)
            ast.fix_missing_locations(new_tree)
            new_module_code = ast.unparse(new_tree)
        except Empty:
            continue
        except Exception as e:
            print(e)
            class_source_queue.task_done()


from .register_blueprints import register_blueprints

def get_exception(exc):
    """Log exceptions"""
    if exc:
        app_logger.warning(f"{exc.__class__.__name__ }: {str(exc)}")
        
        
def register_app_hooks(app: Flask):
    @app.before_first_request
    def application_startup():
        """Log the beginning of the application."""
        app_logger.info('Web app is up!')

    @app.before_request
    def log_request():
        """Log the data held in the request"""
        if request.method in ['POST', 'PUT']:
            log_post_request()
        elif request.method in ['GET', 'DELETE']:
            log_get_request()

    @app.after_request
    def log_response(response):
        try:
            get_response(response)
        except Exception:
            pass
        finally:
            return response

    @app.teardown_request
    def log_exception(exc):
        get_exception(exc)

from dotenv import load_dotenv
load_dotenv()
from assistant.agents import default_agent
import chainlit as cl


@cl.on_chat_start
async def start():
    cl.user_session.set('agent', default_agent)


@cl.on_message
async def main(message: cl.Message):
    agent = cl.user_session.get('agent')
    msg = cl.Message(content='')
    await msg.send()
    await cl.sleep(1)
    msg.content = agent.invoke({'input': message.content})['output']
    await msg.update()


@post.route("/create", methods=["POST", "GET"])
def create_new_post():
    """Create a new post."""
    if request.method == 'GET':
        return {'success': 'post creation form'}, HTTPStatus.OK
    elif request.method == 'POST':
        try:
            post_data = CreatePost(**request.form) 
        except ValidationError:
            return {'Error': 'The data provided is invalid or incomplete!'}, HTTPStatus.BAD_REQUEST
        try:
            user_data = GetUser(user_id=post_data.author_id)
            user = get_user(session=get_db, user_data=user_data)
            if not user:
                return {'Error': f'User with id {user_data.user_id} does not exists'}, HTTPStatus.NOT_FOUND 
            post: Post = create_post(post_data=post_data, post_image=request.files, session=get_db) 
        except (OperationalError, IntegrityError) as e:
            print(e)
            # Send email to
            return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
        resp = CreatedPost(
            id=post.id,
            location=post.location,
            text=post.text,
            image_url=post.image_url,
            author_id=post.author_id,
            date_published=post.date_published
        )
        return resp.model_dump_json(indent=4), HTTPStatus.CREATED

from ..schemas.activity import CreateActivity, GetRepeatableActivity
from sqlalchemy.orm import Session
from ..models.view import View
from ..models.user import User
from ..models.post import Post
from ..schemas.user import GetUser
from ..schemas.post import GetPost
from uuid import uuid4


def create_view(session: Session, activity: CreateActivity) -> View:
    with session() as db:
        view: View = View(
            author_id=activity.user_id,
            post_id=activity.post_id,
            id='View_' + str(uuid4())
        )
        db.add(view)
        db.commit()
        db.refresh(view)
    return view

import torch
import torchvision.transforms as transforms
from PIL import Image


def print_examples(model, device, dataset):
    transform = transforms.Compose(
        [
            transforms.Resize((299, 299)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ]
    )

    model.eval()
    test_img1 = transform(Image.open("test_examples/dog.jpg").convert("RGB")).unsqueeze(
        0
    )
    print("Example 1 CORRECT: Dog on a beach by the ocean")
    print(
        "Example 1 OUTPUT: "
        + " ".join(model.caption_image(test_img1.to(device), dataset.vocabulary))
    )
    test_img2 = transform(
        Image.open("test_examples/child.jpg").convert("RGB")
    ).unsqueeze(0)
    print("Example 2 CORRECT: Child holding red frisbee outdoors")
    print(
        "Example 2 OUTPUT: "
        + " ".join(model.caption_image(test_img2.to(device), dataset.vocabulary))
    )
    test_img3 = transform(Image.open("test_examples/bus.png").convert("RGB")).unsqueeze(
        0
    )
    print("Example 3 CORRECT: Bus driving by parked cars")
    print(
        "Example 3 OUTPUT: "
        + " ".join(model.caption_image(test_img3.to(device), dataset.vocabulary))
    )
    test_img4 = transform(


    imageData = data.get("imageData")

    if not imageData:
        imagePath = os.path.join(os.path.dirname(json_file), data["imagePath"])
        with open(imagePath, "rb") as f:
            imageData = f.read()
            imageData = base64.b64encode(imageData).decode("utf-8")
    img = utils.img_b64_to_arr(imageData)

    label_name_to_value = {"_background_": 0}
    for shape in sorted(data["shapes"], key=lambda x: x["label"]):
        label_name = shape["label"]
        if label_name in label_name_to_value:
            label_value = label_name_to_value[label_name]
        else:
            label_value = len(label_name_to_value)
            label_name_to_value[label_name] = label_value
    lbl, _ = utils.shapes_to_label(img.shape, data["shapes"], label_name_to_value)

    label_names = [None] * (max(label_name_to_value.values()) + 1)
    for name, value in label_name_to_value.items():
        label_names[value] = name

    lbl_viz = imgviz.label2rgb(
        lbl, imgviz.asgray(img), label_names=label_names, loc="rb"
    )

    PIL.Image.fromarray(img).save(osp.join(out_dir, "img.png"))
    utils.lblsave(osp.join(out_dir, "label.png"), lbl)
    PIL.Image.fromarray(lbl_viz).save(osp.join(out_dir, "label_viz.png"))

    with open(osp.join(out_dir, "label_names.txt"), "w") as f:
        for lbl_name in label_names:
            f.write(lbl_name + "\n")

    logger.info("Saved to: {}".format(out_dir))


if __name__ == "__main__":
    main()


    
class CNNToRNN(Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers) -> None:
        super().__init__()
        self.encoder_cnn = EncoderCNN(embed_size=embed_size)
        self.decoder_rnn = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers)
        
    def forward(self, images, captions):
        features = self.encoder_cnn(images)
        outputs = self.decoder_rnn(features, captions)
        return outputs
    
    def caption_image(self, image, vocabulary, max_length=50):
        result_caption = []

        with torch.no_grad():
            x = self.encoder_cnn(image).unsqueeze(0)
            # x = self.encoder_cnn(image)
            states = None

            for _ in range(max_length):
                hiddens, states = self.decoder_rnn.lstm(x, states)
                output = self.decoder_rnn.linear(hiddens.squeeze(0))
                predicted = output.argmax(1)
                result_caption.append(predicted.item())
                x = self.decoder_rnn.embed(predicted).unsqueeze(0)

                if vocabulary.itos[predicted.item()] == "<EOS>":
                    break

        return [vocabulary.itos[idx] for idx in result_caption]

            slot=lambda x: self.actions.saveAuto.setChecked(x),
            icon="save",
            tip=self.tr("Save automatically"),
            checkable=True,
            enabled=True,
        )
        saveAuto.setChecked(self._config["auto_save"])

        saveWithImageData = action(
            text="Save With Image Data",
            slot=self.enableSaveImageWithData,
            tip="Save image data in label file",
            checkable=True,
            checked=self._config["store_data"],
        )

        close = action(
            "&Close",
            self.closeFile,
            shortcuts["close"],
            "close",
            "Close current file",
        )

        toggle_keep_prev_mode = action(
            self.tr("Keep Previous Annotation"),
            self.toggleKeepPrevMode,
            shortcuts["toggle_keep_prev_mode"],
            None,
            self.tr('Toggle "keep pevious annotation" mode'),
            checkable=True,
        )
        toggle_keep_prev_mode.setChecked(self._config["keep_prev"])

        createMode = action(
            self.tr("Create Polygons"),
            lambda: self.toggleDrawMode(False, createMode="polygon"),
            shortcuts["create_polygon"],
            "objects",
            self.tr("Start drawing polygons"),


import torch
import os
from torch import nn
from torchvision import transforms
import numpy as np
import os
from PIL import Image
import torch
import os
from torch import nn
import torch.nn.functional as F
import random


class MaizeNet(nn.Module):
  def __init__(self, K) -> None:
      super(MaizeNet, self).__init__()

      self.conv_layers = nn.Sequential(
          # convolution 1
          nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(32),
          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(32),
          nn.MaxPool2d(2),
          # Convolution 2
          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(64),
          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(64),
          nn.MaxPool2d(2),
          # Convolution 3
          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(128),
          nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),



from sqlalchemy import create_engine, Column, Table, ForeignKey, MetaData
from sqlalchemy.orm import relationship
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import (
    Integer, String, Date, DateTime, Float, Boolean, Text)
from scrapy.utils.project import get_project_settings
from sqlalchemy_utils import ScalarListType


Base = declarative_base()

def db_connect():
    """
    Performs database connection using database settings from settings.py.
    Returns sqlalchemy engine instance
    """
    settings: dict = get_project_settings()
    connection_string: str = settings.get("CONNECTION_STRING")
    return create_engine(connection_string)

def create_table(engine):
    Base.metadata.create_all(engine)
    
    
slide_tag = Table('slide_tag', Base.metadata,
    Column('slide_id', Integer, ForeignKey('slide.id')),
    Column('tag_id', Integer, ForeignKey('tag.id'))
)

class Slide(Base):
    __tablename__ = "slide"

    id = Column(String(), primary_key=True)
    title = Column('title', String())
    description = Column('description', Text())
    category_id = Column(String(), ForeignKey('category.id'))
    tags = relationship('Tag', secondary='slide_tag',
        lazy='dynamic', backref="slide")  # M-to-M for quote and tag
    colors = Column(ScalarListType())


import chainlit as cl
from assistant.utils.assistant_utils import welcome_user
from assistant.agent import get_agent_executor


@cl.on_chat_start
async def start():
    res = await cl.AskUserMessage(content="What is your name?", timeout=30).send()
    if res:
        msg = cl.Message(content="")
        await msg.send()
        msg.content = welcome_user(user_name=res['content'])
        await msg.update()
        

@cl.on_message
async def main(message: cl.Message):
    msg = cl.Message(content="")
    await msg.send()
    query: str = message.content
    agent_executor = get_agent_executor(query)
    msg.content = agent_executor.invoke({"input": query})['output']
    await msg.update()

    except ImportError:
        # when this package is being installed
        return long_description


def main():
    version = get_version()

    if sys.argv[1] == "release":
        try:
            import github2pypi  # NOQA
        except ImportError:
            print(
                "Please install github2pypi\n\n\tpip install github2pypi\n",
                file=sys.stderr,
            )
            sys.exit(1)

        if not distutils.spawn.find_executable("twine"):
            print(
                "Please install twine:\n\n\tpip install twine\n",
                file=sys.stderr,
            )
            sys.exit(1)

        commands = [
            "git push origin main",
            "git tag v{:s}".format(version),
            "git push origin --tags",
            "python setup.py sdist",
            "twine upload dist/labelme-{:s}.tar.gz".format(version),
        ]
        for cmd in commands:
            print("+ {:s}".format(cmd))
            subprocess.check_call(shlex.split(cmd))
        sys.exit(0)

    setup(
        name="labelme",
        version=version,


        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


from setuptools import find_packages, setup
from pip._vendor import tomli

# For consistent encoding
from codecs import open
from os import path

# The directory containing this file
HERE = path.abspath(path.dirname(__file__))

# Get the long description from the README file
with open(path.join(HERE, 'README.md'), encoding='utf-8') as f:
    LONG_DESCRIPTION = f.read()

with open('pyproject.toml', 'r') as f:
    VERSION = tomli.load(f)['tool']['commitizen']['version']

DESCRIPTION = 'A python library that wraps around the YouTube V3 API. You can use it find and manage YouTube resources including Videos, Playlists, Channels and Comments.'

key_words = [
    'youtube', 'youtube-api', 'youtube comments', 'youtube videos',
    'youtube channels', 'youtube comment thread', 'create youtube playlist'
]

install_requires = [
    'oryks-google-oauth',
    'pydantic',
    'pydantic-settings'
]

setup(
    name='oryks-youtube',
    packages=find_packages(
        include=[
            'youtube',
            'youtube.models',
            'youtube.schemas',
            'youtube.resources',
            'youtube.resources.search',
            'youtube.resources.video',


from labelme.utils import shape as shape_module

from .util import get_img_and_data


def test_shapes_to_label():
    img, data = get_img_and_data()
    label_name_to_value = {}
    for shape in data["shapes"]:
        label_name = shape["label"]
        label_value = len(label_name_to_value)
        label_name_to_value[label_name] = label_value
    cls, _ = shape_module.shapes_to_label(
        img.shape, data["shapes"], label_name_to_value
    )
    assert cls.shape == img.shape[:2]


def test_shape_to_mask():
    img, data = get_img_and_data()
    for shape in data["shapes"]:
        points = shape["points"]
        mask = shape_module.shape_to_mask(img.shape[:2], points)
        assert mask.shape == img.shape[:2]


from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy.linkextractors import LinkExtractor 


class SlidesLinkExtractor(Spider):
    name: str = "links-extractor"
    
    start_urls: list[str] = [
        "https://slidemodel.com/templates/"
    ]
    
    def __init__(self, name=None, **kwargs): 
        super().__init__(name, **kwargs) 
  
        self.link_extractor = LinkExtractor(unique=True) 
  
    def parse(self, response: Response, **kwargs: Any) -> Any: 
        links = self.link_extractor.extract_links(response) 
  
        for link in links: 
            if "tag" in link.url:
                yield {
                        "url": link.url, 
                        "text": link.text
                    }

            maker.filename(base + ".jpg"),
            maker.database(),  # e.g., The VOC2007 Database
            maker.annotation(),  # e.g., Pascal VOC2007
            maker.image(),  # e.g., flickr
            maker.size(
                maker.height(str(img.shape[0])),
                maker.width(str(img.shape[1])),
                maker.depth(str(img.shape[2])),
            ),
            maker.segmented(),
        )

        bboxes = []
        labels = []
        for shape in label_file.shapes:
            if shape["shape_type"] != "rectangle":
                print(
                    "Skipping shape: label={label}, " "shape_type={shape_type}".format(
                        **shape
                    )
                )
                continue

            class_name = shape["label"]
            class_id = class_names.index(class_name)

            (xmin, ymin), (xmax, ymax) = shape["points"]
            # swap if min is larger than max.
            xmin, xmax = sorted([xmin, xmax])
            ymin, ymax = sorted([ymin, ymax])

            bboxes.append([ymin, xmin, ymax, xmax])
            labels.append(class_id)

            xml.append(
                maker.object(
                    maker.name(shape["label"]),
                    maker.pose(),
                    maker.truncated(),
                    maker.difficult(),


    def __init__(self, *args, **kwargs):
        self.epsilon = kwargs.pop("epsilon", 10.0)
        self.double_click = kwargs.pop("double_click", "close")
        if self.double_click not in [None, "close"]:
            raise ValueError(
                "Unexpected value for double_click event: {}".format(self.double_click)
            )
        self.num_backups = kwargs.pop("num_backups", 10)
        self._crosshair = kwargs.pop(
            "crosshair",
            {
                "polygon": False,
                "rectangle": True,
                "circle": False,
                "line": False,
                "point": False,
                "linestrip": False,
                "ai_polygon": False,
                "ai_mask": False,
            },
        )
        super(Canvas, self).__init__(*args, **kwargs)
        # Initialise local state.
        self.mode = self.EDIT
        self.shapes = []
        self.shapesBackups = []
        self.current = None
        self.selectedShapes = []  # save the selected shapes here
        self.selectedShapesCopy = []
        # self.line represents:
        #   - createMode == 'polygon': edge from last point to current
        #   - createMode == 'rectangle': diagonal line of the rectangle
        #   - createMode == 'line': the line
        #   - createMode == 'point': the point
        self.line = Shape()
        self.prevPoint = QtCore.QPoint()
        self.prevMovePoint = QtCore.QPoint()
        self.offsets = QtCore.QPoint(), QtCore.QPoint()
        self.scale = 1.0
        self.pixmap = QtGui.QPixmap()


def create_post(post_data: CreatePost, post_image: dict, session: Session):
    post_image_url: str = save_post_photo(post_image)
    post: Post = Post(
        id='Post_' + str(uuid4()),
        author_id=post_data.author_id,
        location=post_data.location,
        text=post_data.text,
        image_url=post_image_url
    )
    with session() as db:
        db.add(post)
        db.commit()
        db.refresh(post)
    return post

        memory=memory,
    )
    cl.user_session.set("agent", agent)
    

@cl.on_message
async def main(message: cl.Message):
    agent = cl.user_session.get("agent")
    msg = cl.Message(content="")
    await msg.send()
    await cl.sleep(1)
    msg.content = agent.invoke({"input": message.content})["output"]
    await msg.update()

            "line",
            "point",
            "linestrip",
            "ai_polygon",
            "ai_mask",
        ]:
            raise ValueError("Unsupported createMode: %s" % value)
        self._createMode = value

    def initializeAiModel(self, name):
        if name not in [model.name for model in labelme.ai.MODELS]:
            raise ValueError("Unsupported ai model: %s" % name)
        model = [model for model in labelme.ai.MODELS if model.name == name][0]

        if self._ai_model is not None and self._ai_model.name == model.name:
            logger.debug("AI model is already initialized: %r" % model.name)
        else:
            logger.debug("Initializing AI model: %r" % model.name)
            self._ai_model = model()

        if self.pixmap is None:
            logger.warning("Pixmap is not set yet")
            return

        self._ai_model.set_image(
            image=labelme.utils.img_qt_to_arr(self.pixmap.toImage())
        )

    def storeShapes(self):
        shapesBackup = []
        for shape in self.shapes:
            shapesBackup.append(shape.copy())
        if len(self.shapesBackups) > self.num_backups:
            self.shapesBackups = self.shapesBackups[-self.num_backups - 1 :]
        self.shapesBackups.append(shapesBackup)

    @property
    def isShapeRestorable(self):
        # We save the state AFTER each edit (not before) so for an
        # edit to be undoable, we expect the CURRENT and the PREVIOUS state


            enabled=False,
        )
        createAiPolygonMode = action(
            self.tr("Create AI-Polygon"),
            lambda: self.toggleDrawMode(False, createMode="ai_polygon"),
            None,
            "objects",
            self.tr("Start drawing ai_polygon. Ctrl+LeftClick ends creation."),
            enabled=False,
        )
        createAiPolygonMode.changed.connect(
            lambda: self.canvas.initializeAiModel(
                name=self._selectAiModelComboBox.currentText()
            )
            if self.canvas.createMode == "ai_polygon"
            else None
        )
        createAiMaskMode = action(
            self.tr("Create AI-Mask"),
            lambda: self.toggleDrawMode(False, createMode="ai_mask"),
            None,
            "objects",
            self.tr("Start drawing ai_mask. Ctrl+LeftClick ends creation."),
            enabled=False,
        )
        createAiMaskMode.changed.connect(
            lambda: self.canvas.initializeAiModel(
                name=self._selectAiModelComboBox.currentText()
            )
            if self.canvas.createMode == "ai_mask"
            else None
        )
        editMode = action(
            self.tr("Edit Polygons"),
            self.setEditMode,
            shortcuts["edit_polygon"],
            "edit",
            self.tr("Move and edit the selected polygons"),
            enabled=False,
        )


from pydantic import BaseModel
from typing import Optional


class UserBase(BaseModel):
  first_name: str
  last_name: str
  email_address: str

  
class UserCreate(UserBase):
    password: str
    role: str = 'user'
    activated: bool = False
    
class UserCreated(UserBase):
    id: str
    activation_token: str

class User(UserBase):
    id: str
    
    class Config:
        from_attributes = True
        
class GetUser(BaseModel):
    user_id: str
    
class GetUsers(BaseModel):
    offset: Optional[int] = 0
    limit: Optional[int] = 10
    
class ActivateUser(BaseModel):
    user_id: str
    activation_token: str
    
class LoginUser(BaseModel):
    email_address: str
    password: str

def get_exception(exc):
    """Log exceptions"""
    if exc:
        app_logger.warning(f"{exc.__class__.__name__ }: {str(exc)}")
        
        
def register_app_hooks(app: Flask):
    @app.before_first_request
    def application_startup():
        """Log the beginning of the application."""
        app_logger.info('Web app is up!')

    @app.before_request
    def log_request():
        """Log the data held in the request"""
        if request.method in ['POST', 'PUT']:
            log_post_request()
        elif request.method in ['GET', 'DELETE']:
            log_get_request()

    @app.after_request
    def log_response(response):
        try:
            get_response(response)
        except Exception:
            pass
        finally:
            return response

    @app.teardown_request
    def log_exception(exc):
        get_exception(exc)

            elif key == QtCore.Qt.Key_Left:
                self.moveByKeyboard(QtCore.QPointF(-MOVE_SPEED, 0.0))
            elif key == QtCore.Qt.Key_Right:
                self.moveByKeyboard(QtCore.QPointF(MOVE_SPEED, 0.0))

    def keyReleaseEvent(self, ev):
        modifiers = ev.modifiers()
        if self.drawing():
            if int(modifiers) == 0:
                self.snapping = True
        elif self.editing():
            if self.movingShape and self.selectedShapes:
                index = self.shapes.index(self.selectedShapes[0])
                if self.shapesBackups[-1][index].points != self.shapes[index].points:
                    self.storeShapes()
                    self.shapeMoved.emit()

                self.movingShape = False

    def setLastLabel(self, text, flags):
        assert text
        self.shapes[-1].label = text
        self.shapes[-1].flags = flags
        self.shapesBackups.pop()
        self.storeShapes()
        return self.shapes[-1]

    def undoLastLine(self):
        assert self.shapes
        self.current = self.shapes.pop()
        self.current.setOpen()
        self.current.restoreShapeRaw()
        if self.createMode in ["polygon", "linestrip"]:
            self.line.points = [self.current[-1], self.current[0]]
        elif self.createMode in ["rectangle", "line", "circle"]:
            self.current.points = self.current.points[0:1]
        elif self.createMode == "point":
            self.current = None
        self.drawingPolygon.emit(True)



from pydantic import BaseModel
from .post import PostAuthor
    
class CommentSchema(BaseModel):
    author: PostAuthor
    text: str

from scrapy import Item, Field
from itemloaders.processors import TakeFirst, MapCompose, Join
import re


def remove_html_tags(description: str) -> str:
    html_pattern = "<(?:\"[^\"]*\"['\"]*|'[^']*'['\"]*|[^'\">])+>" 
    return re.sub(html_pattern, '', description)

def remove_unicode_chars(text: str) -> str:
    return text.replace(u"\xa0", "")

def num_of_slides(text: str) -> int:
    vals = [val for val in list(text) if val.isdigit()]
    return "".join(vals)


class SlidesModelItem(Item):
    title = Field(output_processor=TakeFirst())
    category = Field(output_processor=TakeFirst())
    description = Field(
        input_processor=MapCompose(remove_html_tags, remove_unicode_chars),
        output_processor=Join()
    )
    tags = Field()
    slides_count = Field(
        input_processor=MapCompose(num_of_slides),
        output_processor=TakeFirst()
    )
    colors = Field()
    image_urls = Field()
    images = Field()


from datetime import datetime
from sqlalchemy.orm import Mapped, mapped_column, relationship
from ..database import Base
from sqlalchemy import ForeignKey


class Like(Base):
    __tablename__ = 'likes'
    
    author_id: Mapped[str] = mapped_column(ForeignKey('users.id'), primary_key=True)
    post_id: Mapped[str] = mapped_column(ForeignKey('posts.id'), primary_key=True)
    like_date: Mapped[datetime] = mapped_column(default_factory=datetime.utcnow)
    
    author = relationship('User', back_populates='likes')
    post = relationship('Post', back_populates='likes')

# my_channel = youtube.find_my_channel()
# part: CommentThreadPart = CommentThreadPart()
# filter: CommentThreadFilter = CommentThreadFilter(
#     videoId='-dJPoLm_gtE'
# )
# optional: CommentThreadOptionalParameters = CommentThreadOptionalParameters(
#     maxResults=25
# )
# request:YouTubeRequest = YouTubeRequest(
#     part=part,
#     filter=filter,
#     optional_parameters=optional
# )
# comments = youtube.find_video_comments(request)
# part: CommentThreadPart = CommentThreadPart()
# filter: CommentThreadFilter = CommentThreadFilter(
#     allThreadsRelatedToChannelId='UCRijo3ddMTht_IHyNSNXpNQ'
# )
# optional: CommentThreadOptionalParameters = CommentThreadOptionalParameters(
#     maxResults=25
# )
# request:YouTubeRequest = YouTubeRequest(
#     part=part,
#     filter=filter,
#     optional_parameters=optional
# )
# channel_comments = youtube.find_all_channel_comments(request)
# comment = youtube.get_comment('UgxAQ5xm0pH2NQcwokx4AaABAg')
# replies = youtube.get_comment_replies('UgxwXLTWugMg7IEoKgR4AaABAg')
# comment_text = 'Sample comment text'
# video_id = 'jl3b4eLKiP8'
# comment = youtube.insert_comment(video_id, comment_text)
# comment_id = 'UgxnWN0P4ii1OiIEWft4AaABAg'
# reply = 'Sample comment reply'
# comment_reply = youtube.reply_to_comment(comment_id, reply)
# channel_playlists = youtube.find_channel_playlists('UCRijo3ddMTht_IHyNSNXpNQ')
# my_playlists = youtube.find_my_playlists()
# print(my_playlists)
# playlist_snippet = CreatePlaylistSnippet(
#     title='sample title',


        elif self.output_file:
            self._saveFile(self.output_file)
            self.close()
        else:
            self._saveFile(self.saveFileDialog())

    def saveFileAs(self, _value=False):
        assert not self.image.isNull(), "cannot save empty image"
        self._saveFile(self.saveFileDialog())

    def saveFileDialog(self):
        caption = self.tr("%s - Choose File") % __appname__
        filters = self.tr("Label files (*%s)") % LabelFile.suffix
        if self.output_dir:
            dlg = QtWidgets.QFileDialog(self, caption, self.output_dir, filters)
        else:
            dlg = QtWidgets.QFileDialog(self, caption, self.currentPath(), filters)
        dlg.setDefaultSuffix(LabelFile.suffix[1:])
        dlg.setAcceptMode(QtWidgets.QFileDialog.AcceptSave)
        dlg.setOption(QtWidgets.QFileDialog.DontConfirmOverwrite, False)
        dlg.setOption(QtWidgets.QFileDialog.DontUseNativeDialog, False)
        basename = osp.basename(osp.splitext(self.filename)[0])
        if self.output_dir:
            default_labelfile_name = osp.join(
                self.output_dir, basename + LabelFile.suffix
            )
        else:
            default_labelfile_name = osp.join(
                self.currentPath(), basename + LabelFile.suffix
            )
        filename = dlg.getSaveFileName(
            self,
            self.tr("Choose File"),
            default_labelfile_name,
            self.tr("Label files (*%s)") % LabelFile.suffix,
        )
        if isinstance(filename, tuple):
            filename, _ = filename
        return filename



        relevanceLanguage=relevance_language,
        type=['video'],
        videoCaption=video_caption,
        videoCategoryId=video_category_id,
        videoDefinition=video_definition,
        videoDimension=video_dimension,
        videoDuration=video_duration,
        videoPaidProductPlacement=video_paid_product_placement,
        videoSyndicated=video_syndicated,
        videoType=video_type
    )
    search_schema: YouTubeRequest = YouTubeRequest(
        part=search_part, optional_parameters=optional_params
    )
    response: YouTubeResponse = youtube_client.search(search_schema)
    items: list[Search] = response.items
    return items


def list_video_comments(video_id: str) -> list[Comment]:
    """List a given videos comments"""
    part: CommentThreadPart = CommentThreadPart()
    filter: CommentThreadFilter = CommentThreadFilter(
        videoId=video_id
    )
    optional: CommentThreadOptionalParameters = CommentThreadOptionalParameters(
        maxResults=30
    )
    request:YouTubeRequest = YouTubeRequest(
        part=part,
        filter=filter,
        optional_parameters=optional
    )
    comment_iterator: Iterator = youtube_client.get_comments_iterator(request)
    video_comments: list[Comment] = list()
    done: bool = False
    comment_count: int = 0
    for comment_threads in comment_iterator:
        if done:
            break


        utils.addActions(self.canvas.menus[0], menu)
        self.menus.edit.clear()
        actions = (
            self.actions.createMode,
            self.actions.createRectangleMode,
            self.actions.createCircleMode,
            self.actions.createLineMode,
            self.actions.createPointMode,
            self.actions.createLineStripMode,
            self.actions.createAiPolygonMode,
            self.actions.createAiMaskMode,
            self.actions.editMode,
        )
        utils.addActions(self.menus.edit, actions + self.actions.editMenu)

    def setDirty(self):
        # Even if we autosave the file, we keep the ability to undo
        self.actions.undo.setEnabled(self.canvas.isShapeRestorable)

        if self._config["auto_save"] or self.actions.saveAuto.isChecked():
            label_file = osp.splitext(self.imagePath)[0] + ".json"
            if self.output_dir:
                label_file_without_path = osp.basename(label_file)
                label_file = osp.join(self.output_dir, label_file_without_path)
            self.saveLabels(label_file)
            return
        self.dirty = True
        self.actions.save.setEnabled(True)
        title = __appname__
        if self.filename is not None:
            title = "{} - {}*".format(title, self.filename)
        self.setWindowTitle(title)

    def setClean(self):
        self.dirty = False
        self.actions.save.setEnabled(False)
        self.actions.createMode.setEnabled(True)
        self.actions.createRectangleMode.setEnabled(True)
        self.actions.createCircleMode.setEnabled(True)
        self.actions.createLineMode.setEnabled(True)


# for caption in captions[:5]:
#     res = dataset.vocabulary.textualize(caption)
#     print(res)

import re

from qtpy import QT_VERSION
from qtpy import QtCore
from qtpy import QtGui
from qtpy import QtWidgets

import labelme.utils
from labelme.logger import logger

QT5 = QT_VERSION[0] == "5"


# TODO(unknown):
# - Calculate optimal position so as not to go out of screen area.


class LabelQLineEdit(QtWidgets.QLineEdit):
    def setListWidget(self, list_widget):
        self.list_widget = list_widget

    def keyPressEvent(self, e):
        if e.key() in [QtCore.Qt.Key_Up, QtCore.Qt.Key_Down]:
            self.list_widget.keyPressEvent(e)
        else:
            super(LabelQLineEdit, self).keyPressEvent(e)


class LabelDialog(QtWidgets.QDialog):
    def __init__(
        self,
        text="Enter object label",
        parent=None,
        labels=None,
        sort_labels=True,
        show_text_field=True,
        completion="startswith",
        fit_to_content=None,
        flags=None,
    ):


def get_exception(exc):
    """Log exceptions"""
    if exc:
        app_logger.warning(f"{exc.__class__.__name__ }: {str(exc)}")
        
        
def register_app_hooks(app: Flask):
    @app.before_first_request
    def application_startup():
        """Log the beginning of the application."""
        app_logger.info('Web app is up!')

    @app.before_request
    def log_request():
        """Log the data held in the request"""
        if request.method in ['POST', 'PUT']:
            log_post_request()
        elif request.method in ['GET', 'DELETE']:
            log_get_request()

    @app.after_request
    def log_response(response):
        try:
            get_response(response)
        except Exception:
            pass
        finally:
            return response

    @app.teardown_request
    def log_exception(exc):
        get_exception(exc)

from queue import Queue

source_code_queue: Queue = Queue()
function_code_queue: Queue = Queue()


```
{comment}
```

{format_instructions}
"""

positive_tmpl = PromptTemplate(
    template=topic_assg_msg,
    input_variables=["comment", "topics"],
    partial_variables={
        "format_instructions": positive_parser.get_format_instructions()
    },
)

negative_tmpl = PromptTemplate(
    template=topic_assg_msg,
    input_variables=["comment", "topics"],
    partial_variables={
        "format_instructions": negative_parser.get_format_instructions()
    },
)

sentiment_chain = sentiment_template | llm | StrOutputParser()
pos_chain = positive_tmpl | llm | positive_parser
neg_chain = negative_tmpl | llm | negative_parser

# res = sentiment_chain.invoke({"comment": comment})
# print(res, comment)
# if 'positive' in res.lower():
#     res = pos_chain.invoke({"comment": comment, 'topics': topics})
# elif 'negative' in res.lower():
#     res = neg_chain.invoke({"comment": comment, 'topics': topics})
# print(res)

branch = RunnableBranch(
    (lambda input: 'positive' in input['sentiment'].lower(), pos_chain),
    neg_chain
)



from oauth import OAuth
from helpers import create_message, send_message

    
gmail_client = get_gmail_client('credentials.json')
message = create_message()
message = send_message(gmail_client, message)
print(message)

        print("Generating dataset from:", filename)

        label_file = labelme.LabelFile(filename=filename)

        base = osp.splitext(osp.basename(filename))[0]
        out_img_file = osp.join(args.output_dir, "JPEGImages", base + ".jpg")
        out_clsp_file = osp.join(args.output_dir, "SegmentationClass", base + ".png")
        if not args.nonpy:
            out_cls_file = osp.join(
                args.output_dir, "SegmentationClassNpy", base + ".npy"
            )
        if not args.noviz:
            out_clsv_file = osp.join(
                args.output_dir,
                "SegmentationClassVisualization",
                base + ".jpg",
            )
        if not args.noobject:
            out_insp_file = osp.join(
                args.output_dir, "SegmentationObject", base + ".png"
            )
            if not args.nonpy:
                out_ins_file = osp.join(
                    args.output_dir, "SegmentationObjectNpy", base + ".npy"
                )
            if not args.noviz:
                out_insv_file = osp.join(
                    args.output_dir,
                    "SegmentationObjectVisualization",
                    base + ".jpg",
                )

        img = labelme.utils.img_data_to_arr(label_file.imageData)
        imgviz.io.imsave(out_img_file, img)

        cls, ins = labelme.utils.shapes_to_label(
            img_shape=img.shape,
            shapes=label_file.shapes,
            label_name_to_value=class_name_to_id,
        )


ef generate_bookmarks(users: list[User], posts: list[Post], bookmarks_count: int = 100) -> list[Bookmark]:
    """Generate bookmarks."""
    bookmarks: list[Bookmark] = []
    ids = set()
    for _ in range(bookmarks_count):
        author_id: str = random.choice(users).id
        post_id: str = random.choice(posts).id
        bookmark: bookmark = Bookmark(author_id=author_id, post_id=post_id)
        if (author_id, post_id) not in ids:
            bookmarks.append(bookmark)
        ids.add((author_id, post_id))
    return bookmarks

def generate_comments(users: list[User], posts: list[Post], comments_count: int = 500) -> list[Like]:
    """Generate likes."""
    comments: list[Comment] = []
    ids = set()
    for _ in range(comments_count):
        author_id: str = random.choice(users).id
        post_id: str = random.choice(posts).id
        comment: comment = Comment(
            id='Comment_' + str(uuid4()),
            author_id=author_id, 
            post_id=post_id, 
            comment_text=fake.text() )
        if (author_id, post_id) not in ids:
            comments.append(comment)
        ids.add((author_id, post_id))
    return comments

    features_dir=app_config.features_dir,
    dataset_metadata=dataset_metadata,
    label_columns=label_cols,
    feature_cols=feature_cols,
    columns_to_drop=columns_to_drop,
    numerical_features=numerical_features,
    categorical_features=categorical_features
)

experiment: Experiment = Experiment(
    experiment_config=experiment_config,
    preprocessor=create_experiment_pipeline(experiment_config),
    models=models
)
experiment.run()
# experiment.get_results()
# experiment.tune_best_models()
# experiment.get_tuned_models()
# print(experiment.get_best_models(start=-3, end=-1)) 

from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime


class PostBase(BaseModel):
    location: str
    text: str
    author_id: str


class CreatePost(PostBase):
    pass

class UpdatePost(BaseModel):
    post_id: str
    author_id: str
    location: Optional[str]
    text: Optional[str]

class CreatedPost(PostBase):
    date_published: datetime
    id: str
    image_url: str
    likes: list
    
class GetPost(BaseModel):
    post_id: str

        self._shape_raw = (self.shape_type, self.points, self.point_labels)
        self.shape_type = shape_type
        self.points = points
        self.point_labels = point_labels
        self.mask = mask

    def restoreShapeRaw(self):
        if self._shape_raw is None:
            return
        self.shape_type, self.points, self.point_labels = self._shape_raw
        self._shape_raw = None

    @property
    def shape_type(self):
        return self._shape_type

    @shape_type.setter
    def shape_type(self, value):
        if value is None:
            value = "polygon"
        if value not in [
            "polygon",
            "rectangle",
            "point",
            "line",
            "circle",
            "linestrip",
            "points",
            "mask",
        ]:
            raise ValueError("Unexpected shape_type: {}".format(value))
        self._shape_type = value

    def close(self):
        self._closed = True

    def addPoint(self, point, label=1):
        if self.points and point == self.points[0]:
            self.close()
        else:


from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy.linkextractors import LinkExtractor 


class SlidesLinkExtractor(Spider):
    name: str = "leetcode"
    
    start_urls: list[str] = [
        "https://www.techiedelight.com/data-structures-and-algorithms-problems/"
    ]
    
    def __init__(self, name=None, **kwargs): 
        super().__init__(name, **kwargs) 
  
        self.link_extractor = LinkExtractor(unique=True) 
  
    def parse(self, response: Response, **kwargs: Any) -> Any: 
        links = self.link_extractor.extract_links(response) 
  
        for link in links: 
            yield {
                    "url": link.url, 
                    "text": link.text
                }

from google_calendar import GoogleCalendar
from google_calendar.schemas import (
    ListCalendarEvents, CreateEvent, AttendeeSchema, ReminderSchema, RemindersSchema, 
    EventTimeSchema
)
from google_calendar.models import Event
from datetime import datetime, timedelta, date as d
import pytz

client_secret: str = 'client_secret.json'
google_calendar: GoogleCalendar = GoogleCalendar(secret_file=client_secret)
google_calendar.authenticate()

# req = ListCalendarEvents()
# events = google_calendar.list_calendar_events(req)
# items = events.items
# print(len(items))
# print(items[0])
# print(items[-1])
# import json
# import random
# from google_calendar.resources import EventResource
# with open('events.json', 'r') as f:
#     data = json.load(f)
# event_resource: EventResource = EventResource(calendar_client=google_calendar.calendar_client)
# events = event_resource.parse_items(data)
# print(random.choice(events))
# print(google_calendar.get_event(event_id='_6tlnaqrle5p6cpb4dhmj4phpegp68s9o6lnm6q3b75gj6sr3d5i6sqbb6go6gs3le9r34dj4ddlmmq3f6ks64sb5epq3apri75jj2tjmcss6e'))
# print(google_calendar.quick_add('Meeting with emmanuel at 10.00 am tommorrow.'))
# now = datetime.astimezone()
# start = str(now.strftime("%Y-%m-%dT%H:%M:%S.%fZ"))
# end = str((now + timedelta(hours=2)).strftime("%Y-%m-%dT%H:%M:%S.%fZ"))
# create_event = CreateEvent(
#     start=EventTimeSchema(dateTime=start),
#     end=EventTimeSchema(dateTime=end),
#     summary='Meeting Travis',
#     description='Meet with Travis for launch of his new startup.',
#     location='Weston Hotel, Nairobi',
#     attendees=[
#         AttendeeSchema(email='lpage@example.com'),


def get_exception(exc):
    """Log exceptions"""
    if exc:
        app_logger.warning(f"{exc.__class__.__name__ }: {str(exc)}")
        
        
def register_app_hooks(app: Flask):
    @app.before_first_request
    def application_startup():
        """Log the beginning of the application."""
        app_logger.info('Web app is up!')

    @app.before_request
    def log_request():
        """Log the data held in the request"""
        if request.method in ['POST', 'PUT']:
            log_post_request()
        elif request.method in ['GET', 'DELETE']:
            log_get_request()

    @app.after_request
    def log_response(response):
        try:
            get_response(response)
        except Exception:
            pass
        finally:
            return response

    @app.teardown_request
    def log_exception(exc):
        get_exception(exc)

from itemadapter import ItemAdapter
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem
from os import path, mkdir
from scrapy.http import Response
from scrapy import Request, Spider
from scrapy import Item
from pathlib import PurePosixPath
from urllib.parse import urlparse
from slidesmodel.models import db_connect, Tag, Category, Slide, create_table, create_engine
from sqlalchemy.orm import sessionmaker
import uuid
import logging


class SlidesmodelPipeline:
    def process_item(self, item: Item, spider: Spider):
        return item
    
class MyImagesPipeline(ImagesPipeline):
    def file_path(self, request: Request, response: Response = None, info=None, *, item=None):
        slide_name: str = request.meta['title']
        return f"{slide_name}/" + PurePosixPath(urlparse(request.url).path).name
    
    def get_media_requests(self, item: Item, info):
        for image_url in item["image_urls"]:
            yield Request(image_url, meta={"title": item["title"]})
            

class SaveSlidesPipeline(object):
    def __init__(self):
        """
        Initializes database connection and sessionmaker
        Creates tables
        """
        engine = db_connect()
        create_table(engine)
        self.Session = sessionmaker(bind=engine)




#    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
#    "Accept-Language": "en",
#}

# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    "slidesgo.middlewares.SlidesgoSpiderMiddleware": 543,
#}

# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    "slidesgo.middlewares.SlidesgoDownloaderMiddleware": 543,
#}

# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    "scrapy.extensions.telnet.TelnetConsole": None,
#}

# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
#ITEM_PIPELINES = {
#    "slidesgo.pipelines.SlidesgoPipeline": 300,
#}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False


def get_random_user(session: Session):
    from random import choice
    with session() as db:
        user = choice(db.query(User).all())
    return user

def get_user(session: Session, user_data: GetUser):
    with session() as db:
        user = db.query(User).filter(User.id == user_data.user_id).first()
    return user

def get_users(session: Session, user_data: GetUsers):
    with session() as db:
        users = db.query(User).offset(user_data.offset).limit(user_data.limit).all()
    return users

ef generate_bookmarks(users: list[User], posts: list[Post], bookmarks_count: int = 100) -> list[Bookmark]:
    """Generate bookmarks."""
    bookmarks: list[Bookmark] = []
    ids = set()
    for _ in range(bookmarks_count):
        author_id: str = random.choice(users).id
        post_id: str = random.choice(posts).id
        bookmark: bookmark = Bookmark(author_id=author_id, post_id=post_id)
        if (author_id, post_id) not in ids:
            bookmarks.append(bookmark)
        ids.add((author_id, post_id))
    return bookmarks

def generate_comments(users: list[User], posts: list[Post], comments_count: int = 500) -> list[Like]:
    """Generate likes."""
    comments: list[Comment] = []
    ids = set()
    for _ in range(comments_count):
        author_id: str = random.choice(users).id
        post_id: str = random.choice(posts).id
        comment: comment = Comment(
            id='Comment_' + str(uuid4()),
            author_id=author_id, 
            post_id=post_id, 
            comment_text=fake.text() )
        if (author_id, post_id) not in ids:
            comments.append(comment)
        ids.add((author_id, post_id))
    return comments

from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy import Request
from slidesmodel.items import SlidesModelItem
from scrapy.loader import ItemLoader
from scrapy.utils.project import get_project_settings
import json


class SlidesModelspider(Spider):
    name: str = "slides"
    
    def __init__(self, name: str | None = None, **kwargs: Any):
        super().__init__(name, **kwargs)
        self.start_urls: list[str] = self.load_start_urls()
        # self.start_urls: list[str] = [
        #     "https://slidemodel.com/templates/tag/process-flow/"
        # ]
    
    @staticmethod
    def load_start_urls() -> list:
        settings: dict = get_project_settings()
        links_path: str = settings.get("START_URLS_PATH")
        with open(links_path, "r") as f:
            start_urls_dict: list[dict] = json.load(f)
        return [
            link.get("url") for link in start_urls_dict
        ]
    
    def parse(self, response: Response, **kwargs: Any) -> Any:
        self.logger.info("This is my first spider.")
        slides = response.xpath("//div[@class='col-lg-3 col-sm-6 mt-4']")
        for slide in slides:
            loader: ItemLoader = ItemLoader(item=SlidesModelItem(), selector=slide)
            loader.add_css("title", ".item a::text")
            loader.add_css("category", ".category::text")
            slide_item = loader.load_item()
            link = slide.css(".item a::attr(href)").get()
            self.logger.info("Parsing the slide")


                self.line.close()
            elif self.createMode == "circle":
                self.line.points = [self.current[0], pos]
                self.line.point_labels = [1, 1]
                self.line.shape_type = "circle"
            elif self.createMode == "line":
                self.line.points = [self.current[0], pos]
                self.line.point_labels = [1, 1]
                self.line.close()
            elif self.createMode == "point":
                self.line.points = [self.current[0]]
                self.line.point_labels = [1]
                self.line.close()
            assert len(self.line.points) == len(self.line.point_labels)
            self.repaint()
            self.current.highlightClear()
            return

        # Polygon copy moving.
        if QtCore.Qt.RightButton & ev.buttons():
            if self.selectedShapesCopy and self.prevPoint:
                self.overrideCursor(CURSOR_MOVE)
                self.boundedMoveShapes(self.selectedShapesCopy, pos)
                self.repaint()
            elif self.selectedShapes:
                self.selectedShapesCopy = [s.copy() for s in self.selectedShapes]
                self.repaint()
            return

        # Polygon/Vertex moving.
        if QtCore.Qt.LeftButton & ev.buttons():
            if self.selectedVertex():
                self.boundedMoveVertex(pos)
                self.repaint()
                self.movingShape = True
            elif self.selectedShapes and self.prevPoint:
                self.overrideCursor(CURSOR_MOVE)
                self.boundedMoveShapes(self.selectedShapes, pos)
                self.repaint()
                self.movingShape = True


from langchain.prompts import PromptTemplate
from os import path
from langchain.llms.base import BaseLLM
from langchain_openai import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter


data_dir = "data"
video_data_dir = "data"
transcribed_data = "transcriptions"
video_title = "iphone_15_marques_review"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")

api_key: str = "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"
api_key: str = "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"
chatgpt: BaseLLM = OpenAI(temperature=0, api_key=api_key)

prompt: str = """
You are given the transcript for a video that covers the review of the iphone 15 pro. Find out all the 
features covered in the review. Only return the features of the iphine 15 pri. Return a JSON object with a single key called features.
Transcript: {transcript}
"""

with open(save_transcript_dir, "r") as f:
    video_transcript = f.read()

template = PromptTemplate(template=prompt, input_variables=["transcript"])

chain = template | chatgpt

text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=150)
splits = text_splitter.split_text(video_transcript)
topics = []
for doc in splits:
    res = chain.invoke({"transcript": doc})
    topics.append(res)
    print(res)
print(topics)


def create_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = Like(
            author_id=activity.user_id,
            post_id=activity.post_id
        )
        db.add(like)
        db.commit()
        db.refresh(like)
    return like

def log_get_request():
    request_data = {
        "method": request.method,
        "url root": request.url_root,
        "user agent": request.user_agent,
        "scheme": request.scheme,
        "remote address": request.remote_addr,
        "headers": request.headers,
        "route": request.endpoint,
        "base url": request.base_url,
        "url": request.url,
    }
    if request.args:
        request_data["args"] = request.args
    if request.cookies:
        request_data["cookies"] = request.cookies
    app_logger.info(str(request_data))


def get_response(response):
    response_data = {
        "status": response.status,
        "status code": response.status_code,
        "response": json.loads(response.data),
    }
    app_logger.info(str(response_data))

        ins[cls == -1] = 0  # ignore it.

        # class label
        labelme.utils.lblsave(out_clsp_file, cls)
        if not args.nonpy:
            np.save(out_cls_file, cls)
        if not args.noviz:
            clsv = imgviz.label2rgb(
                cls,
                imgviz.rgb2gray(img),
                label_names=class_names,
                font_size=15,
                loc="rb",
            )
            imgviz.io.imsave(out_clsv_file, clsv)

        if not args.noobject:
            # instance label
            labelme.utils.lblsave(out_insp_file, ins)
            if not args.nonpy:
                np.save(out_ins_file, ins)
            if not args.noviz:
                instance_ids = np.unique(ins)
                instance_names = [str(i) for i in range(max(instance_ids) + 1)]
                insv = imgviz.label2rgb(
                    ins,
                    imgviz.rgb2gray(img),
                    label_names=instance_names,
                    font_size=15,
                    loc="rb",
                )
                imgviz.io.imsave(out_insv_file, insv)


if __name__ == "__main__":
    main()


from config.config import app_config
from os import path
from zipfile import ZipFile
import logging
import pandas as pd
from pandas import DataFrame
from sklearn.pipeline import Pipeline
from joblib import load
from notification import get_gmail_client, create_message, send_message


def extract_dataset(archive_name: str = 'archive.zip', file_name: str = 'Titanic-Dataset.csv') -> None:
    """Extract the downloaded archive file into the data folder."""
    # Ubuntu OS
    downloads_path: str = path.join(path.expanduser('~'), 'Downloads')
    archive_path: str = path.join(downloads_path, archive_name)
    try:
        with ZipFile(archive_path, 'r') as zip_:
            try:
                zip_.extract(file_name, app_config.data_dir)
                logging.info(f'The file {file_name} has been extracted to {path.join(app_config.data_dir, file_name)}.')
            except KeyError:
                print(f'There is no file "{file_name}" in the archive "{archive_path}".')
                logging.error(f'There is no file "{file_name}" in the archive "{archive_path}".')
    except FileNotFoundError:
        print(f'There is no archive "{archive_path}".')
        logging.error(f'There is no archive "{archive_path}".')
    return path.join(app_config.data_dir, file_name)


def load_data(file_path: str = 'Titanic-Dataset.csv') -> DataFrame:
    """Load the Titanic dataset into a dataframe."""
    try:
        data: DataFrame = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f'There is no file such file "{file_path}".')
        logging.error(f'There is no file such file "{file_path}".')
    return data




        for pattern, keys in self._flags.items():
            if re.match(pattern, label_new):
                for key in keys:
                    flags_new[key] = flags_old.get(key, False)
        self.setFlags(flags_new)

    def deleteFlags(self):
        for i in reversed(range(self.flagsLayout.count())):
            item = self.flagsLayout.itemAt(i).widget()
            self.flagsLayout.removeWidget(item)
            item.setParent(None)

    def resetFlags(self, label=""):
        flags = {}
        for pattern, keys in self._flags.items():
            if re.match(pattern, label):
                for key in keys:
                    flags[key] = False
        self.setFlags(flags)

    def setFlags(self, flags):
        self.deleteFlags()
        for key in flags:
            item = QtWidgets.QCheckBox(key, self)
            item.setChecked(flags[key])
            self.flagsLayout.addWidget(item)
            item.show()

    def getFlags(self):
        flags = {}
        for i in range(self.flagsLayout.count()):
            item = self.flagsLayout.itemAt(i).widget()
            flags[item.text()] = item.isChecked()
        return flags

    def getGroupId(self):
        group_id = self.edit_group_id.text()
        if group_id:
            return int(group_id)
        return None


from api import create_app


app = create_app()

def get_posts(session: Session, post_data: GetPosts):
    with session() as db:
        posts: list[Post] = db.query(Post).offset(post_data.offset).limit(post_data.limit).all()
        for post in posts:
            post.author
        return posts

def delete_post(session: Session, post_data: GetPost):
    with session() as db:
        post = db.query(Post).filter(Post.id == post_data.post_id).first()
        db.delete(post)
        db.commit()
        
    return post

from youtube import YouTube

client_secret_file = '/home/downloads/client_secret.json'
youtube = YouTube(client_secret_file)
youtube.authenticate()

def get_channel_id():
    videos = youtube.find_video_by_id('pIzyo4cCGxU')
    channel_id = videos[0].channel_id
    return channel_id

def get_channel_details(channel_id):
    channel = youtube.find_channel_by_id(channel_id)
    return channel

def get_channel_playlists(channel_id):
        channel_playlists = youtube.find_channel_playlists(channel_id)
        return channel_playlists

def get_playlist_items(playlist_id):
    search_iterator = youtube.find_playlist_items(playlist_id, max_results=10)
    playlists = list(next(search_iterator))
    return playlists

def get_playlist_item_video_id(playlist_item):
    video_id = playlist_item.video_id
    return video_id

def get_videos(video_ids):
    videos = youtube.find_video_by_id(video_ids)
    return videos

def get_video_comments(video_id):
    search_iterator = youtube.find_video_comments(video_id, max_results=20)
    video_comments = list(next(search_iterator))
    return video_comments

def main():
    # channel_id = get_channel_id()
    # channel = get_channel_details(channel_id)


from flask import Flask, jsonify, redirect, url_for
from http import HTTPStatus
from .blueprints import register_blueprints
from .config import set_configuration
from oauthlib.oauth2.rfc6749.errors import InvalidGrantError, TokenExpiredError
from flask_dance.contrib.google import google


def create_app() -> Flask:
    """Create the Flask App instance."""
    app = Flask(__name__)
    set_configuration(app=app)
    register_blueprints(app=app)
    
    
    @app.route("/login")
    def login():
        try:
            if not google.authorized:
                return redirect(url_for("google.login"))
            resp = google.get("/oauth2/v1/userinfo")
            if resp.ok:
                return redirect(url_for("home.home_page"))
            return redirect(url_for("login"))
        except (TokenExpiredError, InvalidGrantError):
            return redirect(url_for("google.login"))
    
    @app.route("/health")
    def health():
        return jsonify({"Up": True}), HTTPStatus.OK

    app.shell_context_processor({"app": app})

    return app

# loader = TextLoader(file_path=save_transcript_dir)
# docs = loader.load_and_split(text_splitter=text_splitter)
docs: list[Document] = [
    Document(page_content=comment['comment']) for comment in analy
]
res = stuff_chain.run(docs)
# chain = template | llm
# res = chain.invoke({"comments": analy})
print(res)


# def save_summary(summary: str) -> None:
#     with open(save_transcript_dir, "w") as f:
#         f.write(summary)


# def summarize_video(video_transcript: str) -> str:
#     with open(save_transcript_dir, "r") as f:
#         summry: str = f.read()
#     return summry


from youtube import YouTube
from youtube.schemas import (
    SearchFilter, SearchOptionalParameters, SearchPart, YouTubeResponse, YouTubeRequest,
    CommentThreadFilter, CommentThreadOptionalParameters, CommentThreadPart, CreatePlaylist, 
    CreatePlaylistSnippet, CreatePlaylistItem, VideoResourceId, CreatePlaylistItemSnippet
)
from typing import Iterator
from datetime import datetime
from youtube.models import Comment


client_secrets_file = '/home/lyle/Downloads/search.json'
youtube = YouTube(client_secret_file=client_secrets_file)
youtube_client = youtube.authenticate()
youtube.youtube_client = youtube_client

# query: str = 'Python programming videos'
# max_results: int = 10
# part: SearchPart = SearchPart()
# optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
#     q=query,
#     maxResults=max_results,
#     type=['video', 'playlist', 'channel']
# )
# search_request: YouTubeRequest = YouTubeRequest(
#     part=part, 
#     optional_parameters=optional_parameters
# )
# search_results: YouTubeResponse = youtube.search(search_request)
# search_iterator: Iterator = youtube.get_search_iterator(search_request)
# video = youtube.find_video_by_id('nDXsVhFG7TE')
# videos = youtube.find_videos_by_ids(['nDXsVhFG7TE', 'aQXD-Wr6h64', 'AIqxfBhlwx0'])
# ratings = youtube.get_video_ratings(['nDXsVhFG7TE', 'aQXD-Wr6h64', 'AIqxfBhlwx0'])
# categories = youtube.get_video_categories()
# video_abuse_reasons = youtube.list_video_abuse_report_reasons()
# languages = youtube.list_languages()
# regions = youtube.list_regions()
# popular_video = youtube.find_most_popular_video_by_region()
# channel = youtube.find_channel_by_id('UCRijo3ddMTht_IHyNSNXpNQ')
# channels = youtube.find_channels_by_ids(['UCRijo3ddMTht_IHyNSNXpNQ'])



        self.parser = PydanticOutputParser(pydantic_object=Trip)

        self.system_message_prompt = SystemMessagePromptTemplate.from_template(
            self.system_template,
            partial_variables={
                "format_instructions": self.parser.get_format_instructions()
            },
        )
        self.human_message_prompt = HumanMessagePromptTemplate.from_template(
            self.human_template, input_variables=["agent_suggestion"]
        )

        self.chat_prompt = ChatPromptTemplate.from_messages(
            [self.system_message_prompt, self.human_message_prompt]
        )

#         live.update(create_comments_table(table_data=queue))


def create_analyzed_comments_table(table_data: list[dict]) -> Table:
    table: Table = Table(row_styles=["dim", ""],leading=1, box=box.MINIMAL_DOUBLE_HEAD)
    table.add_column(header="[b]Comment Id", justify="left", style="dark_orange")
    table.add_column(header="Comment", justify="left", style="light_coral")
    table.add_column(header="[b]Likes", justify="left", style="yellow2")
    table.add_column(header="Sentiment", justify="left", style="light_coral")
    table.add_column(header="[b]Topics", justify="left", style="yellow2")
    table.add_column(header="Date", justify="center", style="violet")
    table.columns[0].header_style = "bold chartreuse1"
    table.columns[1].header_style = "bold dark_goldenrod"
    table.columns[2].header_style = "bold chartreuse1"
    table.columns[3].header_style = "bold dark_goldenrod"
    table.columns[4].header_style = "bold chartreuse1"
    table.columns[5].header_style = "bold dark_goldenrod"
    table.border_style = "bright_yellow"
    table.pad_edge = True
    colors = {
        'negative': 'red',
        'positive': 'green',
        'neutral': 'purple'
    }
    for row in table_data:
        color = colors[row["sentiment"]]
        table.add_row(row["comment_id"], row["comment"], str(row["likes"]), f"[bold {color}]{row['sentiment']}[/bold {color}]", ", ".join(row["features"]), row["date_published"])
    return table
def analyze_comment(comment: dict) -> dict:
    from random import choice, choices
    sentiments: list[str] = ['negative', 'positive', 'neutral']
    all_topics: list[str] = ["Incremental changes in design", "Softer corners", "Slimmer bezels", "USB Type-C port"]
    sentiment: str = choice(sentiments)
    topics: list[str] = choices(population=all_topics, k=3)
    comment['topics'] = topics
    comment['sentiment'] = sentiment
    return comment
# analyzed_comments: list[dict] = list(map(analyze_comment, comments)) 
with open('analysis.json', 'r') as f:
    analyzed_comments: list[dict] = json.load(f)


# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class SlidesmodelSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method


from data_utils import extract_archive
from dotenv import load_dotenv
import os


load_dotenv()

DOWNLOAD_PATH: str = os.environ["DOWNLOAD_PATH"]
EXTRACT_PATH: str = os.environ["EXTRACT_PATH"]
ARCHIVE_NAME: str = "archive (2).zip"

extract_archive(
    archive_path=DOWNLOAD_PATH, 
    archive_name=ARCHIVE_NAME, 
    extract_path=EXTRACT_PATH
)



from flask import Flask
from .home import code


def register_blueprints(app: Flask) -> bool:
    """Register the application blueprints.

    Parameters
    ----------
    app: flask.Flask
        A flask app instance

    Returns
    -------
    bool:
        Whether all the blueprints were registered.
    """
    app.register_blueprint(code)
    return True

                new_docstring_node = make_docstring_node(class_docstring)
            node.body.insert(0, new_docstring_node)
            methods_docstrings: dict[str, str] = get_class_methods_docstrings(
                class_and_docstring
            )
            for class_node in node.body:
                if isinstance(class_node, FunctionDef):
                    function_doc: str = ast.get_docstring(node=class_node)
                    if (
                        not function_doc
                        or self.config.overwrite_class_methods_docstring
                    ):
                        function_name: str = class_node.name
                        new_docstring_node = make_docstring_node(
                            methods_docstrings[function_name]
                        )
                        class_node.body.insert(0, new_docstring_node)
        return node


class DocstringWriter(NodeTransformer, BaseModel):
    module_code: str = Field(description='The source code for this module')
    config: Config = Field(description='The application configurations.')

    def visit_classDef(self, node: FunctionDef) -> Any:
        docstring: str = ast.get_docstring(node=node)
        if self.config.overwrite_function_docstring or not docstring:
            function_code: str = ast.get_source_segment(
                source=self.module_code, node=node, padded=True
            )
            function_and_docstring: str = generate_function_docstring(
                function_code, self.config
            )
            function_docstring: str = get_function_docstring(function_and_docstring)
            new_docstring_node = make_docstring_node(function_docstring)
            node.body.insert(0, new_docstring_node)

        return node

    def visit_ClassDef(self, node: ClassDef) -> Any:


import logging.config
import logstash

from dotenv import load_dotenv

load_dotenv()


def create_dev_logger():
    """Create the application logger."""
    config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
            },
            "json": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
                "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
            },
        },
        "handlers": {
            "standard": {
                "class": "logging.StreamHandler",
                "formatter": "json",
            },
        },
        "loggers": {"": {"handlers": ["standard"], "level": logging.INFO}},
    }

    logging.config.dictConfig(config)

    logger = logging.getLogger(__name__)

    return logger

    res: TimeStamps = chain.invoke(inputs)
    # res.time_stamps.sort(key=lambda x: x.start_time)
    return res


ids: dict[str, str] = {
    "set_mismatch": "d-ulaeRBA64",
    "leaf_similar_tree": "Nr8dbnL0_cM",
    "mk_vld_par": "mgQ4O9iUEbg",
    "pams": "kWhy4ZUBdOY",
    "sapltk": "Cg6_nF7YIks",
    "smallest_str_leaf": "UvdWfxQ_ZDs",
    "sub_arr_k_diff_ints": "etI6HqWVa8U",
    "remv_nodes_lnkd_lst": "y783sRTezDg",
    "rvl_card_inc_order": "i2QrUdwWlak",
    "rmv_dup_srt_arr_2": "ycAq8iqh0TI",
    "town_jdg": "QiGaxdUINJ8",
    "rang_sm_bst": "uLVG45n4Sbg",
    "artmtc_slcs_2": "YIMwwT9JdIE",
    "lst_unq_ints_k_rmvl": "Nsp_ta7SlEk",
    "all_ppl_scrt": "1XujGRSU1bQ",
    "stdnts_mss_lnch": "d_cvtFwnOZg",
}
video_id: str = ids["stdnts_mss_lnch"]

class Segment(BaseModel):
    time_stamp: str = Field(description="The time stamp")
    title: str = Field(description="The time stamp title")
    
gemma_parser = PydanticOutputParser(pydantic_object=Segment)

segment_str_gemma: str = ("""Extract all the time stamps and their titles from the following text. Only""" 
                    """ include valid time stamps.Return a json string only with the keys """
                    """```time_stamp``` and ```title```.\ntext: ```{text}```"""
)

segment_str_gemma_v1: str = ("""Extract all the start time stamps, end time stamps and their titles from the following text. Only""" 
                    """ include valid time stamps.Return a json string only with the keys """
                    """```start_time```, ```end_time``` and ```title```.\ntext: ```{text}```"""
)


from dotenv import load_dotenv
load_dotenv()
import chainlit as cl
from farm_agent.agents import agent
from farm_agent.utils import load_model, evaluate_image
from PIL import Image
import io


user_location: str = None
user_name: str = None
welcome_text: str = """
Hello there. This is an application that helps farmers monitor the health level of their crops. 
Start by giving me your name and location, then upload an image of your crops. I will analyze it to 
determine the diasease or pest that affects it and then tell you how to deal with the pest or 
disease and where to purchase pesticides or fungicides.
"""

@cl.on_chat_start
async def start():
    cl.user_session.set("agent", agent)
    await cl.Message(content=welcome_text).send()
    user_name = await cl.AskUserMessage(content="What is your name?", timeout=120).send()
    user_location = await cl.AskUserMessage(content="Where are you from?", timeout=120).send()
    res = await cl.AskActionMessage(
        content="Would you like to determine if your crops are infected by a disease or by pests?",
        actions=[
            cl.Action(name="Check for diseases", value="diseases", label="‚úÖ Check for diseases"),
            cl.Action(name="Check for Pests", value="pests", label="‚ùå Check for Pests")
        ]
    ).send()
    if res and res.get("value") == "diseases":
        files = None
        # Wait for the user to upload a file
        while files == None:
            files = await cl.AskFileMessage(
                content=f"{user_name['content']}, start by uploading an image of your crop.", 
                accept=["image/jpeg", "image/png", "image/jpg"]
            ).send()
        # Decode the file


    llm_with_tools = llm.bind(functions=functions)

    agent = (
        {
            "input": lambda x: x["input"],
            "agent_scratchpad": lambda x: format_to_openai_function_messages(
                x["intermediate_steps"]
            ),
        }
        | prompt
        | llm_with_tools
        | OpenAIFunctionsAgentOutputParser()
    )

    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return agent_executor


from dotenv import load_dotenv
load_dotenv()
import chainlit as cl
from farm_agent.agents import agent
from farm_agent.utils import load_model, evaluate_image
from PIL import Image
import io


user_location: str = None
user_name: str = None
welcome_text: str = """
Hello there. This is an application that helps farmers monitor the health level of their crops. 
Start by giving me your name and location, then upload an image of your crops. I will analyze it to 
determine the diasease or pest that affects it and then tell you how to deal with the pest or 
disease and where to purchase pesticides or fungicides.
"""

@cl.on_chat_start
async def start():
    cl.user_session.set("agent", agent)
    await cl.Message(content=welcome_text).send()
    user_name = await cl.AskUserMessage(content="What is your name?", timeout=120).send()
    user_location = await cl.AskUserMessage(content="Where are you from?", timeout=120).send()
    res = await cl.AskActionMessage(
        content="Would you like to determine if your crops are infected by a disease or by pests?",
        actions=[
            cl.Action(name="Check for diseases", value="diseases", label="‚úÖ Check for diseases"),
            cl.Action(name="Check for Pests", value="pests", label="‚ùå Check for Pests")
        ]
    ).send()
    if res and res.get("value") == "diseases":
        files = None
        # Wait for the user to upload a file
        while files == None:
            files = await cl.AskFileMessage(
                content=f"{user_name['content']}, start by uploading an image of your crop.", 
                accept=["image/jpeg", "image/png", "image/jpg"]
            ).send()
        # Decode the file


        filename = None
        if self.filename is None:
            filename = self.imageList[0]
        else:
            currIndex = self.imageList.index(self.filename)
            if currIndex + 1 < len(self.imageList):
                filename = self.imageList[currIndex + 1]
            else:
                filename = self.imageList[-1]
        self.filename = filename

        if self.filename and load:
            self.loadFile(self.filename)

        self._config["keep_prev"] = keep_prev

    def openFile(self, _value=False):
        if not self.mayContinue():
            return
        path = osp.dirname(str(self.filename)) if self.filename else "."
        formats = [
            "*.{}".format(fmt.data().decode())
            for fmt in QtGui.QImageReader.supportedImageFormats()
        ]
        filters = self.tr("Image & Label files (%s)") % " ".join(
            formats + ["*%s" % LabelFile.suffix]
        )
        fileDialog = FileDialogPreview(self)
        fileDialog.setFileMode(FileDialogPreview.ExistingFile)
        fileDialog.setNameFilter(filters)
        fileDialog.setWindowTitle(
            self.tr("%s - Choose Image or Label file") % __appname__,
        )
        fileDialog.setWindowFilePath(path)
        fileDialog.setViewMode(FileDialogPreview.Detail)
        if fileDialog.exec_():
            fileName = fileDialog.selectedFiles()[0]
            if fileName:
                self.loadFile(fileName)



from json import load
from typing import Any
import streamlit as st
from youtube import YouTube
from youtube.models import Search, Video
from youtube.resources.schemas import (
    CreatePlaylistSchema, CreatePlaylistSnippet, CreateStatus, CreatePlaylistItem, CreatePlaylistItemSnippet,
    VideoResourceId, YouTubeRequest, SearchPart, SearchOptionalParameters, SearchFilter
)
from typing import Any


client_secret_file: str = 'client_secret.json'
youtube: YouTube = YouTube(client_secret_file=client_secret_file)
youtube.authenticate()


def load_data(file_name: str = 'live-news.json') -> dict[str, Any]:
    """Load a json file."""
    with open(file_name, 'r', encoding='utf-8') as f:
        data: dict[str, Any] = load(f)
    return data

def search_news(text: str) -> list[dict[str, Any]]:
    part: SearchPart = SearchPart()
    optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
        q=text,
        maxResults=5,
        eventType='live',
        type=['video']
    )
    search = YouTubeRequest(part=part, optional_parameters=optional_parameters)
    results: list[Search] = youtube.search(search).items
    results: dict[str, Any] = [result.model_dump() for result in results]
    return results

def search_video(video_id: str) -> dict[str, Any]:
    return youtube.find_video_by_id(video_id).model_dump()

def get_thumbnail_url(data: dict[str, Any]) -> str:


            self.points.append(point)
            self.point_labels.append(label)

    def canAddPoint(self):
        return self.shape_type in ["polygon", "linestrip"]

    def popPoint(self):
        if self.points:
            if self.point_labels:
                self.point_labels.pop()
            return self.points.pop()
        return None

    def insertPoint(self, i, point, label=1):
        self.points.insert(i, point)
        self.point_labels.insert(i, label)

    def removePoint(self, i):
        if not self.canAddPoint():
            logger.warning(
                "Cannot remove point from: shape_type=%r",
                self.shape_type,
            )
            return

        if self.shape_type == "polygon" and len(self.points) <= 3:
            logger.warning(
                "Cannot remove point from: shape_type=%r, len(points)=%d",
                self.shape_type,
                len(self.points),
            )
            return

        if self.shape_type == "linestrip" and len(self.points) <= 2:
            logger.warning(
                "Cannot remove point from: shape_type=%r, len(points)=%d",
                self.shape_type,
                len(self.points),
            )
            return


import collections
import threading

import imgviz
import numpy as np
import onnxruntime
import skimage

from ..logger import logger
from . import _utils


class SegmentAnythingModel:
    def __init__(self, encoder_path, decoder_path):
        self._image_size = 1024

        self._encoder_session = onnxruntime.InferenceSession(encoder_path)
        self._decoder_session = onnxruntime.InferenceSession(decoder_path)

        self._lock = threading.Lock()
        self._image_embedding_cache = collections.OrderedDict()

        self._thread = None

    def set_image(self, image: np.ndarray):
        with self._lock:
            self._image = image
            self._image_embedding = self._image_embedding_cache.get(
                self._image.tobytes()
            )

        if self._image_embedding is None:
            self._thread = threading.Thread(
                target=self._compute_and_cache_image_embedding
            )
            self._thread.start()

    def _compute_and_cache_image_embedding(self):
        with self._lock:
            logger.debug("Computing image embedding...")


from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import OpenAIWhisperParser
from langchain_community.document_loaders.blob_loaders.youtube_audio import (
    YoutubeAudioLoader,
)
from langchain_core.documents import Document
from os import path

# Two Karpathy lecture videos
urls = ["https://www.youtube.com/watch?v=altvPR7x9IA"]

# Directory to save audio files
data_dir = "data"
video_data_dir = "video"
transcribed_data = "transcriptions"
video_title = "sample"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")

api_key: str = "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"

loader = GenericLoader(
    YoutubeAudioLoader(urls, save_video_dir), OpenAIWhisperParser(api_key=api_key)
)
docs = loader.load()

full_transcript = ""
for doc in docs:
    full_transcript += doc.page_content

with open(save_transcript_dir, "w", encoding="utf-8") as f:
    f.write(full_transcript)

print(full_transcript)


def transcribe_video(video_id: str, save_video_dir: str, api_key: str) -> str:
    url: str = f"https://www.youtube.com/watch?v={video_id}"
    loader: GenericLoader = GenericLoader(
        YoutubeAudioLoader([url], save_video_dir), OpenAIWhisperParser(api_key=api_key)


         title="[bold italic gold1]Youtube channels reviewing Iphone 15 pro[/bold italic gold1]")
    table.add_column(header="[b]Channel Title", justify="left", style="dark_orange")
    table.add_column(header="Subscribers", justify="left", style="light_coral")
    table.add_column(header="[b]Videos", justify="left", style="yellow2")
    table.add_column(header="Date", justify="center", style="violet")
    table.columns[0].header_style = "bold chartreuse1"
    table.columns[1].header_style = "bold dark_goldenrod"
    table.columns[2].header_style = "bold chartreuse1"
    table.columns[3].header_style = "bold dark_goldenrod"
    table.border_style = "bright_yellow"
    table.pad_edge = True
    for row in table_data:
        table.add_row(row["title"], str(row["subscribers"]), str(row["videos"]), row["date"])
    return table


def video_search(
    product: str, channel_title: str, max_results: int = 5
) -> list[Search]:
    """Search the given channel for the given videos."""
    query: str = f"latest {product} review"
    channel_id: str = get_channel_id(channel_name=channel_title)
    search_part: SearchPart = SearchPart()
    optional_params: SearchOptionalParameters = SearchOptionalParameters(
        channelId=channel_id,
        q=query,
        maxResults=max_results,
        type=["video"],
    )
    search_schema: YouTubeRequest = YouTubeRequest(
        part=search_part, optional_parameters=optional_params
    )
    response: YouTubeResponse = youtube_client.search(search_schema)
    items: list[Search] = response.items
    return items


def get_video_id(video_title: str) -> str:
    """Get video id given the title."""
    part: SearchPart = SearchPart()


from argparse import Namespace

from dotenv import load_dotenv

load_dotenv()
from .config import Config
from .docstring_generator import generate_docstrings
from .extensions import (
    failed_modules_queue,
    functions_source_code_queue,
    modules_path_queue,
    class_source_code_queue,
)
from .helpers import create_application_config, parse_arguments


def run():
    """Runs the application by parsing arguments, creating a configuration, and generating docstrings.

    Returns:
        function: The run function.
        docstring: The docstring for the run function.
        exceptions: Any exceptions that may be thrown during execution."""
    args: Namespace = parse_arguments()
    config: Config = create_application_config(args)
    generate_docstrings(
        config=config,
        module_path_queue=modules_path_queue,
        functions_source_queue=functions_source_code_queue,
        failed_modules_queue=failed_modules_queue,
        class_source_queue=class_source_code_queue,
    )


if __name__ == '__main__':
    run()


from .set_config import set_configuration

```
{comment}
```

{format_instructions}
"""

positive_tmpl = PromptTemplate(
    template=topic_assg_msg,
    input_variables=["comment", "topics"],
    partial_variables={
        "format_instructions": positive_parser.get_format_instructions()
    },
)

negative_tmpl = PromptTemplate(
    template=topic_assg_msg,
    input_variables=["comment", "topics"],
    partial_variables={
        "format_instructions": negative_parser.get_format_instructions()
    },
)

sentiment_chain = sentiment_template | llm | StrOutputParser()
pos_chain = positive_tmpl | llm | positive_parser
neg_chain = negative_tmpl | llm | negative_parser

# res = sentiment_chain.invoke({"comment": comment})
# print(res, comment)
# if 'positive' in res.lower():
#     res = pos_chain.invoke({"comment": comment, 'topics': topics})
# elif 'negative' in res.lower():
#     res = neg_chain.invoke({"comment": comment, 'topics': topics})
# print(res)

branch = RunnableBranch(
    (lambda input: 'positive' in input['sentiment'].lower(), pos_chain),
    neg_chain
)



from itemadapter import ItemAdapter
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem
from os import path, mkdir
from scrapy.http import Response
from scrapy import Request, Spider
from scrapy import Item
from pathlib import PurePosixPath
from urllib.parse import urlparse
from slidesmodel.models import db_connect, Tag, Category, Slide, create_table, create_engine
from sqlalchemy.orm import sessionmaker
import uuid
import logging


class SlidesmodelPipeline:
    def process_item(self, item: Item, spider: Spider):
        return item
    
class MyImagesPipeline(ImagesPipeline):
    def file_path(self, request: Request, response: Response = None, info=None, *, item=None):
        slide_name: str = request.meta['title']
        return f"{slide_name}/" + PurePosixPath(urlparse(request.url).path).name
    
    def get_media_requests(self, item: Item, info):
        for image_url in item["image_urls"]:
            yield Request(image_url, meta={"title": item["title"]})
            

class SaveSlidesPipeline(object):
    def __init__(self):
        """
        Initializes database connection and sessionmaker
        Creates tables
        """
        engine = db_connect()
        create_table(engine)
        self.Session = sessionmaker(bind=engine)




def generate_password_reset_token(session: Session, reset_password_request: RequestPasswordReset):
    with session() as db:
        user: User = db.query(User).filter(User.email_address == reset_password_request.email_address).first()
    resp = {
        'user_id': user.id,
        'email_address': user.email_address,
        'password_reset_token': user.generate_password_reset_token()
    }
    return resp


def password_repeated(session: Session, password_reset: PasswordReset):
    with session() as db:
        user: User = db.query(User).filter(User.email_address == password_reset.email_address).first()
    return user.check_password(password_reset.password)

@post.route("/likes", methods=["GET"])
def get_post_likes():
    """Get a posts likes."""
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post: Post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        likes: list[Bookmark] = list_post_likes(session=get_db, post_data=post_data)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    resp = [
        ActivityCreated(
            user_id=like.author_id,
            post_id=like.post_id,
            date_created=like.like_date
        ).model_dump()
        for like in likes
    ]
    return resp, HTTPStatus.OK

from dotenv import load_dotenv
load_dotenv()
# from assistant.utils.channel_utils import get_channel_latest_video, get_favorite_channels_latest_videos
# from assistant.utils.playlist_utils import add_video_to_youtube_playlist
from assistant.agent import get_agent_executor
# from assistant.tools.playlist.helpers import list_playlist_videos
# from assistant.tools.comment.helpers import list_video_comments
from assistant.agent import get_tools
from assistant.tools.comment.helpers import (
    list_video_comments, find_my_comments, find_author_comments, list_comment_replies
)
from assistant.tools.channel.helpers import find_my_youtube_username

title: str = 'Real Engineering'
# print(get_favorite_channels_latest_videos())
# title: str = 'How Nebula Works from Real Engineering'
# playlist: str = 'Daily Videos'
# add_video_to_youtube_playlist(title, playlist)
# query = 'When was the youtube channel Ark Invest created?'
# print(agent_executor.invoke({"input": query})['output'])
# print(list_playlist_videos(title, title))
#PLx7ERghZ6LoOKkmL4oeLoqWousfkKpdM_
# print(list_video_comments('How Nebula Works', max_results=10))
# query = 'When was my youtube channel created?'
# agent_executor = get_agent_executor()
# print(agent_executor.invoke({"input": query})['output'])
# tools = get_tools(query)
# print(len(tools))
# t = [tool.description for tool in tools]
# print(t)
# print(find_author_comments('Trapping Rain Water - Google Interview Question - Leetcode 42', '@NeetCode'))

query = "List all the replies to the comments by neetcode on the video titled 'Trapping Rain Water - Google Interview Question - Leetcode 42'"
agent_executor = get_agent_executor()
print(agent_executor.invoke({"input": query})['output'])
# print(list_comment_replies('neetcode', 'Trapping Rain Water - Google Interview Question - Leetcode 42'))

# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class SlidesgoSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method


@post.route("/get", methods=["GET"])
def get_one_post():
    """Get a single post."""
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post = get_post(session=get_db, post_data=post_data)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    if post:
        resp = CreatedPost(
            id=post.id,
            location=post.location,
            text=post.text,
            image_url=post.image_url,
            author_id=post.author_id,
            date_published=post.date_published
        )
        return resp.model_dump_json(indent=4), HTTPStatus.OK
    
    return {'Error':f'No post with id {post_data.post_id}'}, HTTPStatus.NOT_FOUND


        if text and not self.validateLabel(text):
            self.errorMessage(
                self.tr("Invalid label"),
                self.tr("Invalid label '{}' with validation type '{}'").format(
                    text, self._config["validate_label"]
                ),
            )
            text = ""
        if text:
            self.labelList.clearSelection()
            shape = self.canvas.setLastLabel(text, flags)
            shape.group_id = group_id
            shape.description = description
            self.addLabel(shape)
            self.actions.editMode.setEnabled(True)
            self.actions.undoLastPoint.setEnabled(False)
            self.actions.undo.setEnabled(True)
            self.setDirty()
        else:
            self.canvas.undoLastLine()
            self.canvas.shapesBackups.pop()

    def scrollRequest(self, delta, orientation):
        units = -delta * 0.1  # natural scroll
        bar = self.scrollBars[orientation]
        value = bar.value() + bar.singleStep() * units
        self.setScroll(orientation, value)

    def setScroll(self, orientation, value):
        self.scrollBars[orientation].setValue(int(value))
        self.scroll_values[orientation][self.filename] = value

    def setZoom(self, value):
        self.actions.fitWidth.setChecked(False)
        self.actions.fitWindow.setChecked(False)
        self.zoomMode = self.MANUAL_ZOOM
        self.zoomWidget.setValue(value)
        self.zoom_values[self.filename] = (self.zoomMode, value)



from flask_bcrypt import Bcrypt
from flask_cors import CORS


bcrypt = Bcrypt()
cors = CORS()

#         self.dropout = nn.Dropout(p)
#         self.hidden_size = hidden_size
#         self.num_layers = num_layers

#         self.embedding = nn.Embedding(input_size, embedding_size)
#         self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)

#     def forward(self, x):
#         # x shape: (seq_length, N) where N is batch size

#         embedding = self.dropout(self.embedding(x))
#         # embedding shape: (seq_length, N, embedding_size)

#         outputs, (hidden, cell) = self.rnn(embedding)
#         # outputs shape: (seq_length, N, hidden_size)

#         return hidden, cell


# class Decoder(nn.Module):
#     def __init__(
#         self, input_size, embedding_size, hidden_size, output_size, num_layers, p
#     ):
#         super(Decoder, self).__init__()
#         self.dropout = nn.Dropout(p)
#         self.hidden_size = hidden_size
#         self.num_layers = num_layers

#         self.embedding = nn.Embedding(input_size, embedding_size)
#         self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x, hidden, cell):
#         # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length
#         # is 1 here because we are sending in a single word and not a sentence
#         x = x.unsqueeze(0)

#         embedding = self.dropout(self.embedding(x))
#         # embedding shape: (1, N, embedding_size)



# Scrapy settings for leetcode project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = "leetcode"

SPIDER_MODULES = ["leetcode.spiders"]
NEWSPIDER_MODULE = "leetcode.spiders"


# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {


def handle_method_not_allowed(exeption: Exception) -> Response:
    """Handle all method not allowed errors.

    Called when a route tries to handle a request with a methods that is not
    allowed for the given route.

    Parameters
    ----------
    exception: Exception
        The exception that was raised. This is a subclass of Exception.

    Returns
    -------
    Response:
        A string consiting of json data and response code.
    """
    return make_response(jsonify({"error": str(exeption)}), HTTPStatus.METHOD_NOT_ALLOWED)


def handle_internal_server_error(exeption: Exception) -> Response:
    """Handle all internal server errors.

    This method is called when an error occurs within the application server.

    Parameters
    ----------
    exception: Exception
        The exception that was raised. This is a subclass of Exception.

    Returns
    -------
    Response:
        A string consiting of json data and response code.
    """
    return make_response(jsonify({"error": str(exeption)}), HTTPStatus.INTERNAL_SERVER_ERROR)

      request is reasonable and achievable within the constraints they set.

      A valid request should contain the following:
      - A start and end location
      - A trip duration that is reasonable given the start and end location
      - Some other details, like the user's interests and/or preferred mode of transport

      Any request that contains potentially harmful activities is not valid, regardless of what
      other details are provided.

      If the request is not valid, set
      plan_is_valid = 0 and use your travel expertise to update the request to make it valid,
      keeping your revised request shorter than 100 words.

      If the request seems reasonable, then set plan_is_valid = 1 and
      don't revise the request.

      {format_instructions}
    """

        self.human_template = """
      ####{query}####
    """

        self.parser = PydanticOutputParser(pydantic_object=Validation)

        self.system_message_prompt = SystemMessagePromptTemplate.from_template(
            self.system_template,
            partial_variables={
                "format_instructions": self.parser.get_format_instructions()
            },
        )
        self.human_message_prompt = HumanMessagePromptTemplate.from_template(
            self.human_template, input_variables=["query"]
        )

        self.chat_prompt = ChatPromptTemplate.from_messages(
            [self.system_message_prompt, self.human_message_prompt]
        )



        self.zoomWidget = ZoomWidget()
        self.setAcceptDrops(True)

        self.canvas = self.labelList.canvas = Canvas(
            epsilon=self._config["epsilon"],
            double_click=self._config["canvas"]["double_click"],
            num_backups=self._config["canvas"]["num_backups"],
            crosshair=self._config["canvas"]["crosshair"],
        )
        self.canvas.zoomRequest.connect(self.zoomRequest)

        scrollArea = QtWidgets.QScrollArea()
        scrollArea.setWidget(self.canvas)
        scrollArea.setWidgetResizable(True)
        self.scrollBars = {
            Qt.Vertical: scrollArea.verticalScrollBar(),
            Qt.Horizontal: scrollArea.horizontalScrollBar(),
        }
        self.canvas.scrollRequest.connect(self.scrollRequest)

        self.canvas.newShape.connect(self.newShape)
        self.canvas.shapeMoved.connect(self.setDirty)
        self.canvas.selectionChanged.connect(self.shapeSelectionChanged)
        self.canvas.drawingPolygon.connect(self.toggleDrawingSensitive)

        self.setCentralWidget(scrollArea)

        features = QtWidgets.QDockWidget.DockWidgetFeatures()
        for dock in ["flag_dock", "label_dock", "shape_dock", "file_dock"]:
            if self._config[dock]["closable"]:
                features = features | QtWidgets.QDockWidget.DockWidgetClosable
            if self._config[dock]["floatable"]:
                features = features | QtWidgets.QDockWidget.DockWidgetFloatable
            if self._config[dock]["movable"]:
                features = features | QtWidgets.QDockWidget.DockWidgetMovable
            getattr(self, dock).setFeatures(features)
            if self._config[dock]["show"] is False:
                getattr(self, dock).setVisible(False)

        self.addDockWidget(Qt.RightDockWidgetArea, self.flag_dock)


from experiment_config import ExperimentConfig
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator
from schemas import DataSet, Model
from pandas import DataFrame, Series
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from extensions import train_model_task, send_training_report_task, redis, tune_model, train_tuned_model
from celery.result import AsyncResult
import logging
from celery import chord
from experiment_models import models
from time import sleep
from config.config import app_config
from schemas.train_config import TrainConfig


class Experiment:
    def __init__(self, experiment_config: ExperimentConfig, preprocessor: ColumnTransformer, models: dict[str, BaseEstimator]):
        self.experiment_config = experiment_config
        self.preprocessor = preprocessor
        self.models = models
        self.dataset: DataSet = DataSet(metadata=experiment_config.dataset_metadata)
        self.trained_models: list[Model] = []
        self.train_task_ids: list[str] = []
    
    def get_features(self) -> DataFrame:
        data: DataFrame = self.dataset.get_dataset()
        features: DataFrame = data[self.experiment_config.feature_cols]
        return features

    def get_labels(self) -> Series:
        data: DataFrame = self.dataset.get_dataset()
        labels: Series = data[self.experiment_config.label_columns]
        return labels

    def get_train_test_data(self) -> ((DataFrame, Series), (DataFrame, Series)):
        features = self.get_features()
        labels = self.get_labels()
        train_features, test_features, train_labels, test_labels = train_test_split(


import gdown

from .efficient_sam import EfficientSam
from .segment_anything_model import SegmentAnythingModel


class SegmentAnythingModelVitB(SegmentAnythingModel):
    name = "SegmentAnything (speed)"

    def __init__(self):
        super().__init__(
            encoder_path=gdown.cached_download(
                url="https://github.com/wkentaro/labelme/releases/download/sam-20230416/sam_vit_b_01ec64.quantized.encoder.onnx",  # NOQA
                md5="80fd8d0ab6c6ae8cb7b3bd5f368a752c",
            ),
            decoder_path=gdown.cached_download(
                url="https://github.com/wkentaro/labelme/releases/download/sam-20230416/sam_vit_b_01ec64.quantized.decoder.onnx",  # NOQA
                md5="4253558be238c15fc265a7a876aaec82",
            ),
        )


class SegmentAnythingModelVitL(SegmentAnythingModel):
    name = "SegmentAnything (balanced)"

    def __init__(self):
        super().__init__(
            encoder_path=gdown.cached_download(
                url="https://github.com/wkentaro/labelme/releases/download/sam-20230416/sam_vit_l_0b3195.quantized.encoder.onnx",  # NOQA
                md5="080004dc9992724d360a49399d1ee24b",
            ),
            decoder_path=gdown.cached_download(
                url="https://github.com/wkentaro/labelme/releases/download/sam-20230416/sam_vit_l_0b3195.quantized.decoder.onnx",  # NOQA
                md5="851b7faac91e8e23940ee1294231d5c7",
            ),
        )


class SegmentAnythingModelVitH(SegmentAnythingModel):
    name = "SegmentAnything (accuracy)"


import os

from youtube import YouTube

client_secrets_file = os.environ['CLIENT_SECRET_FILE']
youtube_client = YouTube(client_secret_file=client_secrets_file)
youtube_client.authenticate()



class Config(BaseModel):
    path: set[str] = Field(description='The path to the source code directory')
    overwrite_function_docstring: Optional[bool] = Field(
        description='Whether or not to overwrite the existing function docstring',
        default=False,
    )
    overwrite_class_docstring: Optional[bool] = Field(
        description='Whether or not to overwrite the existing class docstring',
        default=False,
    )
    overwrite_class_methods_docstring: Optional[bool] = Field(
        description='Whether or not to overwrite the existing class methods docstring',
        default=False,
    )
    documentation_style: Optional[str] = Field(
        description='The format of documentation to use',
        default='Numpy-Style',
        enum=['Numpy-Style', 'Google-Style', 'Sphinx-Style'],
    )
    directories_ignore: set[str] = Field(
        description='Directories to ignore',
        default={'venv', '.venv', '__pycache__', '.git', 'build', 'dist', 'docs'},
    )
    files_ignore: set[str] = Field(
        description='Files to ignore',
        default_factory=set,
    )


        else:
            if ev.orientation() == QtCore.Qt.Vertical:
                mods = ev.modifiers()
                if QtCore.Qt.ControlModifier == int(mods):
                    # with Ctrl/Command key
                    self.zoomRequest.emit(ev.delta(), ev.pos())
                else:
                    self.scrollRequest.emit(
                        ev.delta(),
                        QtCore.Qt.Horizontal
                        if (QtCore.Qt.ShiftModifier == int(mods))
                        else QtCore.Qt.Vertical,
                    )
            else:
                self.scrollRequest.emit(ev.delta(), QtCore.Qt.Horizontal)
        ev.accept()

    def moveByKeyboard(self, offset):
        if self.selectedShapes:
            self.boundedMoveShapes(self.selectedShapes, self.prevPoint + offset)
            self.repaint()
            self.movingShape = True

    def keyPressEvent(self, ev):
        modifiers = ev.modifiers()
        key = ev.key()
        if self.drawing():
            if key == QtCore.Qt.Key_Escape and self.current:
                self.current = None
                self.drawingPolygon.emit(False)
                self.update()
            elif key == QtCore.Qt.Key_Return and self.canCloseShape():
                self.finalise()
            elif modifiers == QtCore.Qt.AltModifier:
                self.snapping = False
        elif self.editing():
            if key == QtCore.Qt.Key_Up:
                self.moveByKeyboard(QtCore.QPointF(0.0, -MOVE_SPEED))
            elif key == QtCore.Qt.Key_Down:
                self.moveByKeyboard(QtCore.QPointF(0.0, MOVE_SPEED))



    decoder_inputs = {
        "image_embeddings": image_embedding,
        "batched_point_coords": batched_point_coords,
        "batched_point_labels": batched_point_labels,
        "orig_im_size": np.array(image.shape[:2], dtype=np.int64),
    }

    masks, _, _ = decoder_session.run(None, decoder_inputs)
    mask = masks[0, 0, 0, :, :]  # (1, 1, 3, H, W) -> (H, W)
    mask = mask > 0.0

    MIN_SIZE_RATIO = 0.05
    skimage.morphology.remove_small_objects(
        mask, min_size=mask.sum() * MIN_SIZE_RATIO, out=mask
    )

    if 0:
        imgviz.io.imsave("mask.jpg", imgviz.label2rgb(mask, imgviz.rgb2gray(image)))
    return mask


        flags=None,
    ):
        if imageData is not None:
            imageData = base64.b64encode(imageData).decode("utf-8")
            imageHeight, imageWidth = self._check_image_height_and_width(
                imageData, imageHeight, imageWidth
            )
        if otherData is None:
            otherData = {}
        if flags is None:
            flags = {}
        data = dict(
            version=__version__,
            flags=flags,
            shapes=shapes,
            imagePath=imagePath,
            imageData=imageData,
            imageHeight=imageHeight,
            imageWidth=imageWidth,
        )
        for key, value in otherData.items():
            assert key not in data
            data[key] = value
        try:
            with open(filename, "w") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            self.filename = filename
        except Exception as e:
            raise LabelFileError(e)

    @staticmethod
    def is_label_file(filename):
        return osp.splitext(filename)[1].lower() == LabelFile.suffix


        utils.addActions(
            self.menus.file,
            (
                open_,
                openNextImg,
                openPrevImg,
                opendir,
                self.menus.recentFiles,
                save,
                saveAs,
                saveAuto,
                changeOutputDir,
                saveWithImageData,
                close,
                deleteFile,
                None,
                quit,
            ),
        )
        utils.addActions(self.menus.help, (help,))
        utils.addActions(
            self.menus.view,
            (
                self.flag_dock.toggleViewAction(),
                self.label_dock.toggleViewAction(),
                self.shape_dock.toggleViewAction(),
                self.file_dock.toggleViewAction(),
                None,
                fill_drawing,
                None,
                hideAll,
                showAll,
                toggleAll,
                None,
                zoomIn,
                zoomOut,
                zoomOrg,
                keepPrevScale,
                None,
                fitWindow,



def get_exception(exc):
    """Log exceptions"""
    if exc:
        app_logger.warning(f"{exc.__class__.__name__ }: {str(exc)}")


You are given the comments by various users on the review of {product}. Use the comments to answer 
the questions that follow. When answering questions, try to list out your answer, with each answer 
on its own separate line. If you do not know the answer, just say that you do not know. DO NOT MAKE 
STUFF UP.
---------
{context}
Question: {question}
Helpful answer: 
"""

product: str = "iphone 15 pro max"
template = PromptTemplate.from_template(template_str)
template = template.partial(product=product)

vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embeddings
)

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=api_key),
    chain_type="stuff",
    return_source_documents=True,
    retriever=vectordb.as_retriever(search_kwargs={"k": 10}),
    chain_type_kwargs={"prompt": template}
)

while True:
    query = input("User: ")
    res = qa_chain.invoke(query)
    print(res["result"])

import os

from langchain.agents import AgentExecutor, Tool
from langchain.agents.format_scratchpad import format_to_openai_function_messages
from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.document import Document
from langchain.tools.render import format_tool_to_openai_function
from langchain.vectorstores.faiss import FAISS

from .tools import YouTubeSearchVideoTool
from .tools.channel import MyYouTubeChannelDetailsTool, YouTubeChannelDetailsTool
from .tools.comment import (
    FindMyCommentsTool,
    FindUserCommentsTool,
    ListVideoCommentRepliesTool,
    ListVideoCommentsTool,
    ReplyCommentTool,
)
from .tools.playlist import (
    CreatePlaylistTool,
    DeleteYoutubePlaylistsTool,
    InsertVideoIntoPlaylistTool,
    ListChannelPlaylistsTool,
    ListPlaylistVideosTool,
    ListUserPlaylistsTool,
)
from .tools.video import YouTubeVideoDetailsTool

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL")  # "gpt-3.5-turbo-0613"
llm = ChatOpenAI(temperature=0, model=OPENAI_MODEL, api_key=OPENAI_API_KEY)
tools = [
    YouTubeSearchVideoTool(),
    ListUserPlaylistsTool(),
    ListPlaylistVideosTool(),
    ListVideoCommentsTool(),
    YouTubeVideoDetailsTool(),


        gmail_client = build(self.api_service_name, self.api_version, credentials=credentials)
        return gmail_client
    
    def authenticate(self) -> Any:
        credentials: Credentials = self.get_credentials()
        if not credentials or self.credentials_expired(credentials=credentials):
            credentials = self.generate_credentials()
            self.save_credentials(credentials=credentials)
        gmail_client = self.get_gmail_client(credentials=credentials)
        return gmail_client


class ItineraryTemplate(object):
    def __init__(self):
        self.system_template = """
      You are a travel agent who helps users make exciting travel plans.

      The user's request will be denoted by four hashtags. Convert the
      user's request into a detailed itinerary describing the places
      they should visit and the things they should do.

      Try to include the specific address of each location.

      Remember to take the user's preferences and timeframe into account,
      and give them an itinerary that would be fun and doable given their constraints.

      Return the itinerary as a bulleted list with clear start and end locations.
      Be sure to mention the type of transit for the trip.
      If specific start and end locations are not given, choose ones that you think are suitable and give specific addresses.
      Your output must be the list and nothing else.
    """

        self.human_template = """
      ####{query}####
    """

        self.system_message_prompt = SystemMessagePromptTemplate.from_template(
            self.system_template,
        )
        self.human_message_prompt = HumanMessagePromptTemplate.from_template(
            self.human_template, input_variables=["query"]
        )

        self.chat_prompt = ChatPromptTemplate.from_messages(
            [self.system_message_prompt, self.human_message_prompt]
        )


class MappingTemplate(object):
    def __init__(self):
        self.system_template = """


        if self.mayContinue():
            self.loadFile(filename)

    def openPrevImg(self, _value=False):
        keep_prev = self._config["keep_prev"]
        if QtWidgets.QApplication.keyboardModifiers() == (
            Qt.ControlModifier | Qt.ShiftModifier
        ):
            self._config["keep_prev"] = True

        if not self.mayContinue():
            return

        if len(self.imageList) <= 0:
            return

        if self.filename is None:
            return

        currIndex = self.imageList.index(self.filename)
        if currIndex - 1 >= 0:
            filename = self.imageList[currIndex - 1]
            if filename:
                self.loadFile(filename)

        self._config["keep_prev"] = keep_prev

    def openNextImg(self, _value=False, load=True):
        keep_prev = self._config["keep_prev"]
        if QtWidgets.QApplication.keyboardModifiers() == (
            Qt.ControlModifier | Qt.ShiftModifier
        ):
            self._config["keep_prev"] = True

        if not self.mayContinue():
            return

        if len(self.imageList) <= 0:
            return



from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy import Request
from slidesmodel.items import SlidesModelItem
from scrapy.loader import ItemLoader
from scrapy.utils.project import get_project_settings
import json


class SlidesModelspider(Spider):
    name: str = "slides"
    
    def __init__(self, name: str | None = None, **kwargs: Any):
        super().__init__(name, **kwargs)
        self.start_urls: list[str] = self.load_start_urls()
        # self.start_urls: list[str] = [
        #     "https://slidemodel.com/templates/tag/process-flow/"
        # ]
    
    @staticmethod
    def load_start_urls() -> list:
        settings: dict = get_project_settings()
        links_path: str = settings.get("START_URLS_PATH")
        with open(links_path, "r") as f:
            start_urls_dict: list[dict] = json.load(f)
        return [
            link.get("url") for link in start_urls_dict
        ]
    
    def parse(self, response: Response, **kwargs: Any) -> Any:
        self.logger.info("This is my first spider.")
        slides = response.xpath("//div[@class='col-lg-3 col-sm-6 mt-4']")
        for slide in slides:
            loader: ItemLoader = ItemLoader(item=SlidesModelItem(), selector=slide)
            loader.add_css("title", ".item a::text")
            loader.add_css("category", ".category::text")
            slide_item = loader.load_item()
            link = slide.css(".item a::attr(href)").get()
            self.logger.info("Parsing the slide")



def create_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = Like(
            author_id=activity.user_id,
            post_id=activity.post_id
        )
        db.add(like)
        db.commit()
        db.refresh(like)
    return like


def list_user_likes(session: Session, user_data: GetUser) -> list[Like]:
    with session() as db:
        user: User = db.query(User).filter(User.id == user_data.user_id).first()
        likes: list[Like] = user.likes
    return likes

def list_post_likes(session: Session, post_data: GetPost):
    with session() as db:
        post: Post = db.query(Post).filter(Post.id == post_data.post_id).first()
        likes: list[Like] = post.likes
        for like in likes:
            like.author
    return likes

def get_key_like(session: Session, post_data: GetPost):
    from random import choice
    with session() as db:
        post: Post = db.query(Post).filter(Post.id == post_data.post_id).first()
        likes: list[Like] = post.likes
        for like in likes:
            like.author
    return choice(likes).author if likes else None

from typing import Any, Optional

from oryks_google_oauth import GoogleDriveScopes, GoogleOAuth
from pydantic import BaseModel


class GoogleDrive(BaseModel):
    """Provides methods for interacting with the Drive API.

    This class acts as an interface to the Drive API, providing methods for interacting with
    the Drive API.

    Attributes
    ----------
    client_secret_file: str
        The path to the json file containing your authentication information.
    """

    client_secret_file: Optional[str] = None
    authenticated: Optional[bool] = False
    drive_client: Optional[Any] = None

    def authenticate(self, client_secret_file: Optional[str] = None) -> None:
        """Authenticate the requests made to drive.

        Used to generate the credentials that are used when authenticating requests to drive.

        Parameters
        ----------
        client_secret_file: str
            The path to clients secret json file from Google

        Raises
        ------
        ValueError:
            When the client secrets file is not provided
        FileNotFoundError:
            When the secrets file path is not found
        """
        if client_secret_file:


import argparse
import os

import imgviz
import matplotlib.pyplot as plt
import numpy as np

from labelme.logger import logger


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("label_png", help="label PNG file")
    parser.add_argument(
        "--labels",
        help="labels list (comma separated text or file)",
        default=None,
    )
    parser.add_argument("--image", help="image file", default=None)
    args = parser.parse_args()

    if args.labels is not None:
        if os.path.exists(args.labels):
            with open(args.labels) as f:
                label_names = [label.strip() for label in f]
        else:
            label_names = args.labels.split(",")
    else:
        label_names = None

    if args.image is not None:
        image = imgviz.io.imread(args.image)
    else:
        image = None

    label = imgviz.io.imread(args.label_png)
    label = label.astype(np.int32)
    label[label == 255] = -1


import datetime
import logging
import os
import sys

import termcolor

if os.name == "nt":  # Windows
    import colorama

    colorama.init()

from . import __appname__

COLORS = {
    "WARNING": "yellow",
    "INFO": "white",
    "DEBUG": "blue",
    "CRITICAL": "red",
    "ERROR": "red",
}


class ColoredFormatter(logging.Formatter):
    def __init__(self, fmt, use_color=True):
        logging.Formatter.__init__(self, fmt)
        self.use_color = use_color

    def format(self, record):
        levelname = record.levelname
        if self.use_color and levelname in COLORS:

            def colored(text):
                return termcolor.colored(
                    text,
                    color=COLORS[levelname],
                    attrs={"bold": True},
                )

            record.levelname2 = colored("{:<7}".format(record.levelname))


    logging.info('Getting the playlist id for %s', playlist_name)
    playlist_id: str = get_playlist_id(playlist_name)
    logging.info('The playlist %s id is %s', playlist_name, playlist_id)
    playlist_items: list[str] = []
    for channel_name in channel_names:
        logging.info('Getting the channel id for "%s".', channel_name)
        channel_id: str = get_channel_id(channel_name)
        logging.info('The channel "%s" has id "%s".', channel_name, channel_id)
        logging.info('Getting the latest video for channel "%s".', channel_name)
        latest_video: Video = find_latest_video(channel_id, youtube)
        logging.info('Fetched the latest video for %s.', channel_name)
        logging.info('The latest video for channel: "%s" is :"%s"', channel_name, latest_video.title)
        logging.info('Creating a playlist item for video: "%s"', latest_video.title)
        playlist_item: PlaylistItem = add_video_to_playlist(latest_video, playlist_id, youtube, position=0)
        if playlist_item:
            logging.info('Added "%s" to "%s".', latest_video.title, playlist_name)
            playlist_items.append({
                'id': playlist_item.id,
                'title': playlist_item.snippet.title
            })
        else:
            logging.info('The video "%s" already exists in playlist "%s".', latest_video.title, playlist_name)
        logging.info('Video id: %s.', latest_video.resource_id)
    if playlist_items:
        logging.info('Added all the latest videos to the playlist.')
    else:
        logging.info('Did not add any videos to playlist.')
    return playlist_items


client_secret_file: str = 'client_secret.json'
youtube: YouTube = get_youtube_client(client_secret_file)
# playlist_id: str = get_playlist_id('Daily Videos')
# channel_id: str = get_channel_id('CNBC')
# latest_video: Video = find_latest_video(channel_id, youtube)
# playlist_item: PlaylistItem = add_video_to_playlist(latest_video.resource_id, playlist_id, youtube, position=0)
# delete_video_playlist('UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS41NkI0NEY2RDEwNTU3Q0M2', youtube)
# ids = ['UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS41NkI0NEY2RDEwNTU3Q0M2', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS4yODlGNEE0NkRGMEEzMEQy', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS4wMTcyMDhGQUE4NTIzM0Y5', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS41MjE1MkI0OTQ2QzJGNzNG', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS4wOTA3OTZBNzVEMTUzOTMy', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS4xMkVGQjNCMUM1N0RFNEUx', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS41MzJCQjBCNDIyRkJDN0VD', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS5DQUNERDQ2NkIzRUQxNTY1', 'UExfMjZ2bWc4V19BY0VFbF9CbzJBaHppUy05M3I2YjhidS45NDk1REZENzhEMzU5MDQz']
# for id in ids:
#     delete_video_playlist(id, youtube)


        new_height = image_size
        new_width = int(round(width * scale))
    return scale, new_height, new_width


def _resize_image(image_size, image):
    scale, new_height, new_width = _compute_scale_to_resize_image(
        image_size=image_size, image=image
    )
    scaled_image = imgviz.resize(
        image,
        height=new_height,
        width=new_width,
        backend="pillow",
    ).astype(np.float32)
    return scale, scaled_image


def _compute_image_embedding(image_size, encoder_session, image):
    image = imgviz.asrgb(image)

    scale, x = _resize_image(image_size, image)
    x = (x - np.array([123.675, 116.28, 103.53], dtype=np.float32)) / np.array(
        [58.395, 57.12, 57.375], dtype=np.float32
    )
    x = np.pad(
        x,
        (
            (0, image_size - x.shape[0]),
            (0, image_size - x.shape[1]),
            (0, 0),
        ),
    )
    x = x.transpose(2, 0, 1)[None, :, :, :]

    output = encoder_session.run(output_names=None, input_feed={"x": x})
    image_embedding = output[0]

    return image_embedding



# -*- coding: utf-8 -*-

import functools
import html
import math
import os
import os.path as osp
import re
import webbrowser

import imgviz
import natsort
from qtpy import QtCore
from qtpy import QtGui
from qtpy import QtWidgets
from qtpy.QtCore import Qt

from labelme import PY2
from labelme import __appname__
from labelme.ai import MODELS
from labelme.config import get_config
from labelme.label_file import LabelFile
from labelme.label_file import LabelFileError
from labelme.logger import logger
from labelme.shape import Shape
from labelme.widgets import BrightnessContrastDialog
from labelme.widgets import Canvas
from labelme.widgets import FileDialogPreview
from labelme.widgets import LabelDialog
from labelme.widgets import LabelListWidget
from labelme.widgets import LabelListWidgetItem
from labelme.widgets import ToolBar
from labelme.widgets import UniqueLabelQListWidget
from labelme.widgets import ZoomWidget

from . import utils

# FIXME
# - [medium] Set max zoom value to something big enough for FitWidth/Window



# flake8: noqa

from ._io import lblsave

from .image import apply_exif_orientation
from .image import img_arr_to_b64
from .image import img_arr_to_data
from .image import img_b64_to_arr
from .image import img_data_to_arr
from .image import img_data_to_pil
from .image import img_data_to_png_data
from .image import img_pil_to_data
from .image import img_qt_to_arr

from .shape import labelme_shapes_to_label
from .shape import masks_to_bboxes
from .shape import polygons_to_mask
from .shape import shape_to_mask
from .shape import shapes_to_label

from .qt import newIcon
from .qt import newButton
from .qt import newAction
from .qt import addActions
from .qt import labelValidator
from .qt import struct
from .qt import distance
from .qt import distancetoline
from .qt import fmtShortcut


    args_schema: Type[BaseModel] = YouTubeChannelTitleSearch

    def _run(
        self, title: str, 
        run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        youtube_response: YouTubeResponse = youtube.find_channel_by_name(title)
        search_results: list[Search] = youtube_response.items
        return search_results[0]

    async def _arun(
        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("Calculator does not support async")
    

class YouTubeChannelSearch(BaseModel):
    id: str


class YouTubeChannelSearchTool(BaseTool):
    name = "youtube_channel_search"
    description = """
    useful when you ned to find information about a channel when provided with the channel id. 
    To use this tool you must provide the channel id.
    """
    args_schema: Type[BaseModel] = YouTubeChannelSearch

    def _run(
        self, 
        id: str, 
        run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        youtube_channel: YouTubeResponse = youtube.find_channel_by_id(id)
        return youtube_channel

    async def _arun(


from redis import Redis

redis = Redis()

page = '1'
res = redis.zrevrange(f'quote_authors', 0, 10, withscores=True)
print(res)

from datetime import datetime
from sqlalchemy.orm import Mapped, mapped_column, relationship
from ..database import Base
from sqlalchemy import ForeignKey


class Bookmark(Base):
    __tablename__ = 'bookmarks'
    
    author_id: Mapped[str] = mapped_column(ForeignKey('users.id'), primary_key=True)
    post_id: Mapped[str] = mapped_column(ForeignKey('posts.id'), primary_key=True)
    bookmark_date: Mapped[datetime] = mapped_column(default_factory=datetime.utcnow)
    
    author = relationship('User', back_populates='bookmarks')
    post = relationship('Post', back_populates='bookmarks')

        if filename is not None:
            self.load(filename)
        self.filename = filename

    @staticmethod
    def load_image_file(filename):
        try:
            image_pil = PIL.Image.open(filename)
        except IOError:
            logger.error("Failed opening image file: {}".format(filename))
            return

        # apply orientation to image according to exif
        image_pil = utils.apply_exif_orientation(image_pil)

        with io.BytesIO() as f:
            ext = osp.splitext(filename)[1].lower()
            if PY2 and QT4:
                format = "PNG"
            elif ext in [".jpg", ".jpeg"]:
                format = "JPEG"
            else:
                format = "PNG"
            image_pil.save(f, format=format)
            f.seek(0)
            return f.read()

    def load(self, filename):
        keys = [
            "version",
            "imageData",
            "imagePath",
            "shapes",  # polygonal annotations
            "flags",  # image level flags
            "imageHeight",
            "imageWidth",
        ]
        shape_keys = [
            "label",
            "points",


from langchain.agents import AgentType, Tool, initialize_agent, tool
from langchain.llms import OpenAI
from langchain.memory import ConversationBufferMemory
from langchain.utilities import SerpAPIWrapper
import googlemaps
import os
import chainlit as cl
from dotenv import load_dotenv
load_dotenv()


@tool
def get_agrovets(query: str) -> str:
    """Useful when you need to get agrovets in a given location. Give it a query, such as agrovets in Nairobi, Kenya.
    """
    gmaps = googlemaps.Client(key=os.environ['GOOGLE_MAPS_API_KEY'])
    results = gmaps.places(query=f'Get me aggrovets in {query}')
    aggrovet_locations: list[dict] = list()
    for result in results['results']:
        bussiness: dict = dict()
        bussiness['business_status'] = result['business_status']
        bussiness['formatted_address'] = result['formatted_address']
        bussiness['name'] = result['name']
        bussiness['opening_hours'] = result.get('opening_hours', 'NaN')
        aggrovet_locations.append(bussiness)
    return aggrovet_locations


@cl.on_chat_start
async def start():
    tools: list[Tool] = [
        get_agrovets
    ]
    llm = OpenAI(temperature=0)
    memory = ConversationBufferMemory(memory_key="chat_history")
    agent = initialize_agent(
        tools,
        llm,
        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
        verbose=True,


from dotenv import load_dotenv

load_dotenv()
from langchain.agents import tool
from langchain.pydantic_v1 import BaseModel, Field
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_community.utilities.google_search import GoogleSearchAPIWrapper
import os
from langchain_openai import ChatOpenAI, OpenAI
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.llms.base import BaseLLM
from langchain.text_splitter import RecursiveCharacterTextSplitter
from os import path


OPENAI_API_KEY: str = "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"

GOOGLE_API_KEY: str = os.environ.get(
    "GOOGLE_API_KEY", "AIzaSyDDuuMXlQ-7iiT7QvQg6c8nbyV0mFxSAYo"
)
GOOGLE_CSE_ID: str = os.environ.get("GOOGLE_CSE_ID", "a347832f863fd4f5d")

chat_model: BaseLLM = ChatOpenAI(temperature=0, api_key=OPENAI_API_KEY)
llm: BaseLLM = OpenAI(temperature=0, api_key=OPENAI_API_KEY)

google_search = GoogleSearchAPIWrapper(
    google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID, k=3
)

data_dir = "data"
video_data_dir = "data"
transcribed_data = "transcriptions"
video_title = "iphone_15_marques_review"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")


class UserQuery(BaseModel):
    query: str = Field(description="What the user wants to search for on the web")
    result_count: int = Field(description="The number of results to return", default=5)


        image_file = files[0]
        image_data = image_file.content # byte values of the image
        image = Image.open(io.BytesIO(image_data))
        model = load_model()
        predicted_label, predictions = evaluate_image(image, model)
        analysis_text: str = f"""
            After analyzing the image you uploaded, here is what I found:
            Maize Leaf Rust probability: {predictions['Maize Leaf Rust']}%
            Northern Leaf Blight probability: {predictions['Northern Leaf Blight']}%
            Healthy probability: {predictions['Healthy']}%
            Gray Leaf Spot probability: {predictions['Gray Leaf Spot']}%
            Your plant is most likely infected with {predicted_label}.
            """
        elements = [
            cl.Image(
                name="image2", display="inline", content=image_data
                ), 
            cl.Text(name="simple_text", content=analysis_text, display="inline", size='large')
        ]
        await cl.Message(content=f"Maize image with {predicted_label}!", elements=elements).send()
        msg = cl.Message(content="")
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run('Tell me some facts about the maize disease leaf rust especially in relation to kenya.')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Tell me some facts about the maize disease {predicted_label} especially in relation to kenya.')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Get me aggrovets in {user_location}, Kenya')
        await msg.update()
        await cl.Message(content='Feel free to ask me more questions about maize plant diseases and how to deal with them.').send()
    else:
        await cl.Message(content='Currently cannot detect pests. Still working on that model.').send()
    

@cl.on_message
async def main(message: cl.Message):


# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class LeetcodeSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method


from youtube import YouTube
import json
import os
from typing import Optional
from youtube.models.video_model import Video

def to_json(channels):
    with open('channels.json', 'w', encoding='utf-8') as f:
        f.write(json.dumps(channels, indent=4))
        
def save_to_channels(video: Video, file_name: Optional[str] = "kenyan_channels.json") -> None:
    kenyan_channels = []
    if video:
        if os.path.exists(file_name):
            with open(file_name, 'r', encoding='utf-8') as f:
                try:
                    kenyan_channels = json.loads(f.read())
                except json.decoder.JSONDecodeError:
                    pass
        with open(file_name, 'w', encoding='utf-8') as f:
            data = {
                video.channel_title: video.channel_id
            }
            if not data in kenyan_channels:
                kenyan_channels.append(data)
            f.write(json.dumps(kenyan_channels, indent=2))
            print(kenyan_channels)
    

def print_videos(videos):
    vids = []
    for video in videos:
        vid = {
            'title': video.video_title,
            'channel': video.channel_title
        }
        vids.append(vid)
    print(vids)

# client_secrets_file = '/home/lyle/Downloads/python_learning_site.json'


# flake8: noqa

from .brightness_contrast_dialog import BrightnessContrastDialog

from .canvas import Canvas

from .color_dialog import ColorDialog

from .file_dialog_preview import FileDialogPreview

from .label_dialog import LabelDialog
from .label_dialog import LabelQLineEdit

from .label_list_widget import LabelListWidget
from .label_list_widget import LabelListWidgetItem

from .tool_bar import ToolBar

from .unique_label_qlist_widget import UniqueLabelQListWidget

from .zoom_widget import ZoomWidget


            "group_id",
            "shape_type",
            "flags",
            "description",
            "mask",
        ]
        try:
            with open(filename, "r") as f:
                data = json.load(f)

            if data["imageData"] is not None:
                imageData = base64.b64decode(data["imageData"])
                if PY2 and QT4:
                    imageData = utils.img_data_to_png_data(imageData)
            else:
                # relative path from label file to relative path from cwd
                imagePath = osp.join(osp.dirname(filename), data["imagePath"])
                imageData = self.load_image_file(imagePath)
            flags = data.get("flags") or {}
            imagePath = data["imagePath"]
            self._check_image_height_and_width(
                base64.b64encode(imageData).decode("utf-8"),
                data.get("imageHeight"),
                data.get("imageWidth"),
            )
            shapes = [
                dict(
                    label=s["label"],
                    points=s["points"],
                    shape_type=s.get("shape_type", "polygon"),
                    flags=s.get("flags", {}),
                    description=s.get("description"),
                    group_id=s.get("group_id"),
                    mask=utils.img_b64_to_arr(s["mask"]) if s.get("mask") else None,
                    other_data={k: v for k, v in s.items() if k not in shape_keys},
                )
                for s in data["shapes"]
            ]
        except Exception as e:
            raise LabelFileError(e)



def distancetoline(point, line):
    p1, p2 = line
    p1 = np.array([p1.x(), p1.y()])
    p2 = np.array([p2.x(), p2.y()])
    p3 = np.array([point.x(), point.y()])
    if np.dot((p3 - p1), (p2 - p1)) < 0:
        return np.linalg.norm(p3 - p1)
    if np.dot((p3 - p2), (p1 - p2)) < 0:
        return np.linalg.norm(p3 - p2)
    if np.linalg.norm(p2 - p1) == 0:
        return np.linalg.norm(p3 - p1)
    return np.linalg.norm(np.cross(p2 - p1, p1 - p3)) / np.linalg.norm(p2 - p1)


def fmtShortcut(text):
    mod, key = text.split("+", 1)
    return "<b>%s</b>+<b>%s</b>" % (mod, key)


        self.selectionModel().selectionChanged.connect(self.itemSelectionChangedEvent)

    def __len__(self):
        return self.model().rowCount()

    def __getitem__(self, i):
        return self.model().item(i)

    def __iter__(self):
        for i in range(len(self)):
            yield self[i]

    @property
    def itemDropped(self):
        return self.model().itemDropped

    @property
    def itemChanged(self):
        return self.model().itemChanged

    def itemSelectionChangedEvent(self, selected, deselected):
        selected = [self.model().itemFromIndex(i) for i in selected.indexes()]
        deselected = [self.model().itemFromIndex(i) for i in deselected.indexes()]
        self.itemSelectionChanged.emit(selected, deselected)

    def itemDoubleClickedEvent(self, index):
        self.itemDoubleClicked.emit(self.model().itemFromIndex(index))

    def selectedItems(self):
        return [self.model().itemFromIndex(i) for i in self.selectedIndexes()]

    def scrollToItem(self, item):
        self.scrollTo(self.model().indexFromItem(item))

    def addItem(self, item):
        if not isinstance(item, LabelListWidgetItem):
            raise TypeError("item must be LabelListWidgetItem")
        self.model().setItem(self.model().rowCount(), 0, item)
        item.setSizeHint(self.itemDelegate().sizeHint(None, None))



from celery import Celery
from config import CeleryConfig


celery_app: Celery = Celery(__name__)
celery_app.config_from_object(CeleryConfig)
celery_app.conf.beat_schedule = {
        'create-daily-playlist': {
            'task': 'tasks.create_daily_playlist',
            'schedule': 10
        }
    }
celery_app.autodiscover_tasks(['tasks'])


            lambda: self.canvas.initializeAiModel(
                name=self._selectAiModelComboBox.currentText()
            )
            if self.canvas.createMode in ["ai_polygon", "ai_mask"]
            else None
        )

        self.tools = self.toolbar("Tools")
        self.actions.tool = (
            open_,
            opendir,
            openPrevImg,
            openNextImg,
            save,
            deleteFile,
            None,
            createMode,
            editMode,
            duplicate,
            delete,
            undo,
            brightnessContrast,
            None,
            fitWindow,
            zoom,
            None,
            selectAiModel,
        )

        self.statusBar().showMessage(str(self.tr("%s started.")) % __appname__)
        self.statusBar().show()

        if output_file is not None and self._config["auto_save"]:
            logger.warn(
                "If `auto_save` argument is True, `output_file` argument "
                "is ignored and output filename is automatically "
                "set as IMAGE_BASENAME.json."
            )
        self.output_file = output_file
        self.output_dir = output_dir


from pydantic import Field, BaseModel


class Trip(BaseModel):
    start: str = Field(description="start location of trip")
    end: str = Field(description="end location of trip")
    waypoints: list[str] = Field(description="list of waypoints")
    transit: str = Field(description="mode of transportation")

import os

from langchain.llms.base import BaseLLM
from langchain_openai import OpenAI

api_key: str = os.environ["OPENAI_API_KEY"]

chatgpt: BaseLLM = OpenAI(temperature=0, api_key=api_key)


    def addZoom(self, increment=1.1):
        zoom_value = self.zoomWidget.value() * increment
        if increment > 1:
            zoom_value = math.ceil(zoom_value)
        else:
            zoom_value = math.floor(zoom_value)
        self.setZoom(zoom_value)

    def zoomRequest(self, delta, pos):
        canvas_width_old = self.canvas.width()
        units = 1.1
        if delta < 0:
            units = 0.9
        self.addZoom(units)

        canvas_width_new = self.canvas.width()
        if canvas_width_old != canvas_width_new:
            canvas_scale_factor = canvas_width_new / canvas_width_old

            x_shift = round(pos.x() * canvas_scale_factor) - pos.x()
            y_shift = round(pos.y() * canvas_scale_factor) - pos.y()

            self.setScroll(
                Qt.Horizontal,
                self.scrollBars[Qt.Horizontal].value() + x_shift,
            )
            self.setScroll(
                Qt.Vertical,
                self.scrollBars[Qt.Vertical].value() + y_shift,
            )

    def setFitWindow(self, value=True):
        if value:
            self.actions.fitWidth.setChecked(False)
        self.zoomMode = self.FIT_WINDOW if value else self.MANUAL_ZOOM
        self.adjustScale()

    def setFitWidth(self, value=True):
        if value:
            self.actions.fitWindow.setChecked(False)


        # to be in the undo stack.
        if len(self.shapesBackups) < 2:
            return False
        return True

    def restoreShape(self):
        # This does _part_ of the job of restoring shapes.
        # The complete process is also done in app.py::undoShapeEdit
        # and app.py::loadShapes and our own Canvas::loadShapes function.
        if not self.isShapeRestorable:
            return
        self.shapesBackups.pop()  # latest

        # The application will eventually call Canvas.loadShapes which will
        # push this right back onto the stack.
        shapesBackup = self.shapesBackups.pop()
        self.shapes = shapesBackup
        self.selectedShapes = []
        for shape in self.shapes:
            shape.selected = False
        self.update()

    def enterEvent(self, ev):
        self.overrideCursor(self._cursor)

    def leaveEvent(self, ev):
        self.unHighlight()
        self.restoreCursor()

    def focusOutEvent(self, ev):
        self.restoreCursor()

    def isVisible(self, shape):
        return self.visible.get(shape, True)

    def drawing(self):
        return self.mode == self.CREATE

    def editing(self):
        return self.mode == self.EDIT


from dotenv import load_dotenv
load_dotenv()
from flask.cli import FlaskGroup
from api import create_app

app = create_app()
cli = FlaskGroup(create_app=create_app)



if __name__ == "__main__":
    cli()

def activate_user_account(session: Session, activation_data: ActivateUser):
    with session() as db:
        user: User = db.query(User).filter(User.id == activation_data.user_id).first()
        if user.id == User.decode_auth_token(activation_data.activation_token):
            user.activated = True
            db.commit()
            return True
    raise InvalidTokenError('Invalid or Expired token.')


def loggin_user(session: Session, login_data: LoginUser):
    with session() as db:
        user: User = db.query(User).filter(User.email_address == login_data.email_address).first()
        if user and user.check_password(login_data.password):
            return True
    raise ValueError('Invalid email address and or password.')


from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy import Request
# from slidesmodel.items import SlidesModelItem
from scrapy.loader import ItemLoader
from scrapy.utils.project import get_project_settings
import json


class SlidesGospider(Spider):
    name: str = "slides"
    
    def __init__(self, name: str | None = None, **kwargs: Any):
        super().__init__(name, **kwargs)
        # self.start_urls: list[str] = self.load_start_urls()
        self.start_urls: list[str] = [
            "https://slidesgo.com/food#rs=home"
        ]
    
    
    def parse(self, response: Response, **kwargs: Any) -> Any:
        self.logger.info("This is my first spider.")
        slide_links = response.css('div.theme_post a::attr(href)')
        for slide_link in slide_links:
            # title = problem_link.css('a::text')[0].get()
            link = slide_link.get()
            yield{
                "link": link,
            }
            # yield Request(link, callback=self.parse_problem)
        # for slide in slides:
        #     loader: ItemLoader = ItemLoader(item=SlidesModelItem(), selector=slide)
        #     loader.add_css("title", ".item a::text")
        #     loader.add_css("category", ".category::text")
        #     slide_item = loader.load_item()
        #     link = slide.css(".item a::attr(href)").get()
        #     self.logger.info("Parsing the slide")
        #     yield Request(link, callback=self.parse_slide, meta={"slide_item": slide_item})
        


def generate_users(count: int = 10) -> list[UserCreate]:
    """Generate ten random users."""
    first_names = (fake.name() for _ in range(count))
    last_names = (fake.name() for _ in range(count))
    emails = (fake.email() for i in range(count))
    profile_pictures = [f'profile-{i}.jpg' for i in range(21)]
    return [
        User(
            id='User_' + str(uuid4()),
            first_name=first_name,
            last_name=last_name,
            email_address=email,
            password=User.hash_password('password'),
            profile_picture_url=image
        ) 
        for first_name, last_name, email, image in zip(first_names, last_names, emails, profile_pictures)
    ]

"""This module declares the app configuration.

The classes include:

BaseConfig:
    Has all the configurations shared by all the environments.

"""
import os

from dotenv import load_dotenv

load_dotenv()


class BaseConfig:
    """Base configuration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ.get(
        "SECRET_KEY", "df0331cefc6c2b9a5d0208a726a5d1c0fd37324feba25506"
    )


class DevelopmentConfig(BaseConfig):
    """Development confuguration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ.get(
        "SECRET_KEY", "df0331cefc6c2b9a5d0208a726a5d1c0fd37324feba25506"
    )


class TestingConfig(BaseConfig):
    """Testing configuration."""

    TESTING = True
    SECRET_KEY = os.environ.get("SECRET_KEY", "secret-key")


        Shape.hvertex_fill_color = QtGui.QColor(
            *self._config["shape"]["hvertex_fill_color"]
        )

        # Set point size from config file
        Shape.point_size = self._config["shape"]["point_size"]

        super(MainWindow, self).__init__()
        self.setWindowTitle(__appname__)

        # Whether we need to save or not.
        self.dirty = False

        self._noSelectionSlot = False

        self._copied_shapes = None

        # Main widgets and related state.
        self.labelDialog = LabelDialog(
            parent=self,
            labels=self._config["labels"],
            sort_labels=self._config["sort_labels"],
            show_text_field=self._config["show_label_text_field"],
            completion=self._config["label_completion"],
            fit_to_content=self._config["fit_to_content"],
            flags=self._config["label_flags"],
        )

        self.labelList = LabelListWidget()
        self.lastOpenDir = None

        self.flag_dock = self.flag_widget = None
        self.flag_dock = QtWidgets.QDockWidget(self.tr("Flags"), self)
        self.flag_dock.setObjectName("Flags")
        self.flag_widget = QtWidgets.QListWidget()
        if config["flags"]:
            self.loadFlags({k: False for k in config["flags"]})
        self.flag_dock.setWidget(self.flag_widget)
        self.flag_widget.itemChanged.connect(self.setDirty)



        image_file = files[0]
        image_data = image_file.content # byte values of the image
        image = Image.open(io.BytesIO(image_data))
        model = load_model()
        predicted_label, predictions = evaluate_image(image, model)
        analysis_text: str = f"""
            After analyzing the image you uploaded, here is what I found:
            Maize Leaf Rust probability: {predictions['Maize Leaf Rust']}%
            Northern Leaf Blight probability: {predictions['Northern Leaf Blight']}%
            Healthy probability: {predictions['Healthy']}%
            Gray Leaf Spot probability: {predictions['Gray Leaf Spot']}%
            Your plant is most likely infected with {predicted_label}.
            """
        elements = [
            cl.Image(
                name="image2", display="inline", content=image_data
                ), 
            cl.Text(name="simple_text", content=analysis_text, display="inline", size='large')
        ]
        await cl.Message(content=f"Maize image with {predicted_label}!", elements=elements).send()
        msg = cl.Message(content="")
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Tell me some facts about the maize disease {predicted_label} especially in relation to kenya.')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'What fungicides or pesticides can be used to deal with the maize disease {predicted_label}?')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Get me aggrovets in {user_location}, Kenya')
        await msg.update()
        await cl.Message(content='Feel free to ask me more questions about maize plant diseases and how to deal with them.').send()
    else:
        await cl.Message(content='Currently cannot detect pests. Still working on that model.').send()
    

@cl.on_message
async def main(message: cl.Message):


@post.route("/load_more_comments", methods=["GET"])
def load_more_comments():
    """Get a single post."""
    offset: str = request.args.get('offset', 0)
    limit: str = request.args.get('limit', 10)
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post: Post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        comments: list[Comment] = list_post_comments(session=get_db, post_data=post_data, offset=offset, limit=limit)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    post_comments = []
    for comment in comments:
        comment_author: PostAuthor = PostAuthor(
            id=comment.author.id,
            profile_picture=url_for('static', filename=f'img/{comment.author.profile_picture_url}'),
            name=comment.author.first_name
        )
        comment_schema: CommentSchema = CommentSchema(
            author=comment_author,
            text=comment.comment_text
        )
        post_comments.append(comment_schema.model_dump())
    return post_comments

# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class LeetcodeItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass


from pydantic_settings import BaseSettings
from dotenv import load_dotenv

load_dotenv()


class PostgresSettings(BaseSettings):
    postgres_host: str
    postgres_port: int
    postgres_user: str
    postgres_password: str
    postgres_db: str
    
    @property
    def sqlalchemy_db_url(self) -> str:
        return f"postgresql://{self.postgres_user}:{self.postgres_password}@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"


class RedisSettings(BaseSettings):
    redis_host: str
    redis_port: int

    
class CeleryConfig(BaseSettings):
    celery_broker_url: str
    celery_result_backend: str


class Config(BaseSettings):
    expiration_time: int


def add_user(user: User) -> User:
    with get_db() as session:
        session.add(user)
        session.commit()
        session.refresh(user)
    return user

def add_post(post: Post) -> Post:
    with get_db() as session:
        session.add(post)
        session.commit()
        session.refresh(post)
    return post

def add_likes(likes: list[Like]) -> None:
    with get_db() as session:
        for like in likes:
            session.add(like)
        session.commit()
        
def add_bookmarks(bookmarks: list[Bookmark]) -> None:
    with get_db() as session:
        for bookmark in bookmarks:
            session.add(bookmark)
        session.commit()
    
def add_comments(comments: list[Comment]) -> None:
    with get_db() as session:
        for comment in comments:
            session.add(comment)
        session.commit()

# MIT License
# Copyright (c) Kentaro Wada

import math
import uuid

import numpy as np
import PIL.Image
import PIL.ImageDraw

from labelme.logger import logger


def polygons_to_mask(img_shape, polygons, shape_type=None):
    logger.warning(
        "The 'polygons_to_mask' function is deprecated, " "use 'shape_to_mask' instead."
    )
    return shape_to_mask(img_shape, points=polygons, shape_type=shape_type)


def shape_to_mask(img_shape, points, shape_type=None, line_width=10, point_size=5):
    mask = np.zeros(img_shape[:2], dtype=np.uint8)
    mask = PIL.Image.fromarray(mask)
    draw = PIL.ImageDraw.Draw(mask)
    xy = [tuple(point) for point in points]
    if shape_type == "circle":
        assert len(xy) == 2, "Shape of shape_type=circle must have 2 points"
        (cx, cy), (px, py) = xy
        d = math.sqrt((cx - px) ** 2 + (cy - py) ** 2)
        draw.ellipse([cx - d, cy - d, cx + d, cy + d], outline=1, fill=1)
    elif shape_type == "rectangle":
        assert len(xy) == 2, "Shape of shape_type=rectangle must have 2 points"
        draw.rectangle(xy, outline=1, fill=1)
    elif shape_type == "line":
        assert len(xy) == 2, "Shape of shape_type=line must have 2 points"
        draw.line(xy=xy, fill=1, width=line_width)
    elif shape_type == "linestrip":
        draw.line(xy=xy, fill=1, width=line_width)
    elif shape_type == "point":
        assert len(xy) == 1, "Shape of shape_type=point must have 1 points"


def generate_email(user_email_address: str, email_type: str) -> str:
    email_types: dict[str, Callable] = {
        'password_reset': send_password_reset_email,
        'account_activation': send_account_activation_email
    }
    return email_types[email_type]

def send_email(user_email_address: str, email_sender: str) -> None:
    email_senders: dict[str, Callable] = {
        'local': send_email_local,
        'aws_ses': send_email_aws_ses
    }
    return email_sender[email_sender]

from pydantic import BaseModel, Field
from schemas import DatasetMetadata
from datetime import datetime
from json import dump


class ExperimentConfig(BaseModel):
    data_dir: str
    models_directory: str
    features_dir: str
    dataset_metadata: DatasetMetadata
    label_columns: list[str] = Field(default_factory=list)
    feature_cols: list[str] = Field(default_factory=list)
    columns_to_drop: list[str] = Field(default_factory=list)
    numerical_features: list[str] = Field(default_factory=list)
    categorical_features: list[str] = Field(default_factory=list)
    
    def save_experiment_config(self, path: str,
            title: str = '', 
            description: str = '', 
            date: datetime=datetime.now()
        ) -> None:
        with open(path, 'w', encoding='utf-8') as f:
            dump(self.model_dump(), f, indent=4)
        
        


        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesn‚Äôt have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


class SlidesmodelDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called


#!/usr/bin/env python

import argparse
import collections
import datetime
import glob
import json
import os
import os.path as osp
import sys
import uuid

import imgviz
import numpy as np

import labelme

try:
    import pycocotools.mask
except ImportError:
    print("Please install pycocotools:\n\n    pip install pycocotools\n")
    sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("input_dir", help="input annotated directory")
    parser.add_argument("output_dir", help="output dataset directory")
    parser.add_argument("--labels", help="labels file", required=True)
    parser.add_argument("--noviz", help="no visualization", action="store_true")
    args = parser.parse_args()

    if osp.exists(args.output_dir):
        print("Output directory already exists:", args.output_dir)
        sys.exit(1)
    os.makedirs(args.output_dir)
    os.makedirs(osp.join(args.output_dir, "JPEGImages"))
    if not args.noviz:


from .youtube import YouTube
from .youtube.schemas import YouTubeListResponse, YouTubeResponse
from .youtube.models import Video, Search, Playlist
from os import path
from langchain_core.language_models.base import BaseLanguageModel
from langchain_openai import OpenAI
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
# from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser

from .oryks_google_oauth import (
    GoogleSlidesScope, GoogleOAuth, GoogleDirectories, GoogleDriveScopes
)
from typing import Optional, Any
import json


api_key: str = "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"
open_ai: BaseLanguageModel = OpenAI(temperature=0, api_key=api_key)

gemma_2b: BaseLanguageModel = Ollama(model="gemma:2b")
llama_2b: BaseLanguageModel = Ollama(model="llama2")

def create_gslide_client() -> Any:
    secrets_file: str = "/home/lyle/oryks/backend/api/libraries/slide.json"
    scopes: list[str] = [
        GoogleSlidesScope.slides.value,
        GoogleSlidesScope.drive.value
    ]
    api_service_name: str = "slides"
    api_version: str = "v1"
    credentials_dir: str = GoogleDirectories.slides.value
    credentials_file_name: Optional[str] = 'credentials.json'

    auth: GoogleOAuth = GoogleOAuth(
        secrets_file=secrets_file,
        scopes=scopes,
        api_service_name=api_service_name,


with open(file_path, "r", encoding="utf-8") as f:
    all_comments: list[str] = json.load(fp=f)
    cleaned_comments: list[str] = list(map(clean_text, all_comments))
    # comments: list[str] = choices(population=cleaned_comments, k=3)
    comments = cleaned_comments
    docs: list[Document] = [
        Document(page_content=comment)
        for comment in comments
        if is_acceptable_len(comment)
    ]
    comments: list[dict[str, str | int]] = [
        {"doc_id": i + 1, "comment": docs[i].page_content} for i in range(len(docs))
    ]

data_dir = "./agent_nelly/data_analysis/data"
features_dir = "features"
save_features_dir = path.join(data_dir, features_dir, "features.json")

with open(save_features_dir, 'r') as f:
    topics: list[str] = json.load(f)


class CustomerCommentData(BaseModel):
    doc_id: int = Field(description="The doc_id from the input")
    topics: list[str] = Field(
        description="List of the relevant topics for the customer review. Include only topics from the list provided.",
        default_factory=list,
    )
    sentiment: str = Field(
        description="Sentiment of the topic", enum=["positive", "neutral", "negative"]
    )
    

class CommentsParser(BaseModel):
    comment: list[CustomerCommentData] = Field(description="A list of the customer comment data", default_factory=list)


output_parser = PydanticOutputParser(pydantic_object=CommentsParser)
format_instructions = output_parser.get_format_instructions()



            mask = pycocotools.mask.encode(mask)
            area = float(pycocotools.mask.area(mask))
            bbox = pycocotools.mask.toBbox(mask).flatten().tolist()

            data["annotations"].append(
                dict(
                    id=len(data["annotations"]),
                    image_id=image_id,
                    category_id=cls_id,
                    segmentation=segmentations[instance],
                    area=area,
                    bbox=bbox,
                    iscrowd=0,
                )
            )

        if not args.noviz:
            viz = img
            if masks:
                labels, captions, masks = zip(
                    *[
                        (class_name_to_id[cnm], cnm, msk)
                        for (cnm, gid), msk in masks.items()
                        if cnm in class_name_to_id
                    ]
                )
                viz = imgviz.instances2rgb(
                    image=img,
                    labels=labels,
                    masks=masks,
                    captions=captions,
                    font_size=15,
                    line_width=2,
                )
            out_viz_file = osp.join(args.output_dir, "Visualization", base + ".jpg")
            imgviz.io.imsave(out_viz_file, viz)

    with open(out_ann_file, "w") as f:
        json.dump(data, f)



from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.vectorstores.chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate
from os import path
import json
from random import choices

data_dir = "data"
video_data_dir = "data"
transcribed_data = "transcriptions"
video_title = "iphone_15_marques_review"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")
persist_directory = path.join(data_dir, "vectore_store")

api_key: str = "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"
file_path: str = "comments.json"

with open(file_path, "r", encoding="utf-8") as f:
    all_comments: list[str] = json.load(fp=f)
    comments: list[str] = choices(population=all_comments, k=50)
    comments: list[Document] = [Document(page_content=comment) for comment in all_comments]

text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)
split_docs = text_splitter.split_documents(comments)

embeddings = OpenAIEmbeddings(api_key=api_key)
# vectordb = FAISS.from_texts(splits, embeddings)
# vectordb = FAISS.from_documents(documents=comments, embedding=embeddings)
# vectordb = Chroma.from_documents(
#     documents=split_docs,
#     embedding=embeddings,
#     persist_directory=persist_directory
# )

template_str: str = """


        if self.selectedVertex():  # A vertex is marked for selection.
            index, shape = self.hVertex, self.hShape
            shape.highlightVertex(index, shape.MOVE_VERTEX)
        else:
            for shape in reversed(self.shapes):
                if self.isVisible(shape) and shape.containsPoint(point):
                    self.setHiding()
                    if shape not in self.selectedShapes:
                        if multiple_selection_mode:
                            self.selectionChanged.emit(self.selectedShapes + [shape])
                        else:
                            self.selectionChanged.emit([shape])
                        self.hShapeIsSelected = False
                    else:
                        self.hShapeIsSelected = True
                    self.calculateOffsets(point)
                    return
        self.deSelectShape()

    def calculateOffsets(self, point):
        left = self.pixmap.width() - 1
        right = 0
        top = self.pixmap.height() - 1
        bottom = 0
        for s in self.selectedShapes:
            rect = s.boundingRect()
            if rect.left() < left:
                left = rect.left()
            if rect.right() > right:
                right = rect.right()
            if rect.top() < top:
                top = rect.top()
            if rect.bottom() > bottom:
                bottom = rect.bottom()

        x1 = left - point.x()
        y1 = top - point.y()
        x2 = right - point.x()
        y2 = bottom - point.y()
        self.offsets = QtCore.QPointF(x1, y1), QtCore.QPointF(x2, y2)



from random import randint    
def like(comment: dict) -> dict:
    comment['likes'] = randint(0,2)
    return comment

analyzed_comments: list[dict] = list(map(like, analyzed_comments))
            
            
from collections import deque
queue = deque(maxlen=10, iterable=analyzed_comments[:batch])
with Live(create_analyzed_comments_table(table_data=queue)) as live:
    for data in analyzed_comments[batch:]:
        queue.append(data)
        live.update(create_analyzed_comments_table(table_data=queue))
        sleep(0.5)

from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import Field, BaseModel
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI
import re
import json
from langchain.docstore.document import Document
from random import choices
from langchain.base_language import BaseLanguageModel

file_path: str = "comments.json"
api_key: str = "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"


def lower(text: str) -> str:
    return text.lower().strip()


def remove_urls(text: str) -> str:
    url_pattern = r"https?://\S+|www\.\S+"
    text = re.sub(url_pattern, "", text)
    return text


def remove_punctuations(text: str) -> str:
    punctuation_pattern = r"[^\w\s]"
    cleaned = re.sub(punctuation_pattern, "", text)
    return cleaned


def clean_text(text: str) -> str:
    text = lower(text)
    text = remove_urls(text)
    text = remove_punctuations(text)
    return text


def is_acceptable_len(text: str, l=15) -> bool:
    return len(text.split()) >= l



import logging
from logging import Handler
from typing import Optional


def create_logger(handlers: Optional[Handler] = []) -> None:
    logging_format: str = "%(asctime)s - %(levelname)s - %(message)s"
    date_format: str = "[%X]"
    logging.basicConfig(
        format=logging_format,
        datefmt=date_format,
        handlers=handlers,
        level=logging.INFO,
    )


        w = self.centralWidget().width() - 2.0
        return w / self.canvas.pixmap.width()

    def enableSaveImageWithData(self, enabled):
        self._config["store_data"] = enabled
        self.actions.saveWithImageData.setChecked(enabled)

    def closeEvent(self, event):
        if not self.mayContinue():
            event.ignore()
        self.settings.setValue("filename", self.filename if self.filename else "")
        self.settings.setValue("window/size", self.size())
        self.settings.setValue("window/position", self.pos())
        self.settings.setValue("window/state", self.saveState())
        self.settings.setValue("recentFiles", self.recentFiles)
        # ask the use for where to save the labels
        # self.settings.setValue('window/geometry', self.saveGeometry())

    def dragEnterEvent(self, event):
        extensions = [
            ".%s" % fmt.data().decode().lower()
            for fmt in QtGui.QImageReader.supportedImageFormats()
        ]
        if event.mimeData().hasUrls():
            items = [i.toLocalFile() for i in event.mimeData().urls()]
            if any([i.lower().endswith(tuple(extensions)) for i in items]):
                event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        if not self.mayContinue():
            event.ignore()
            return
        items = [i.toLocalFile() for i in event.mimeData().urls()]
        self.importDroppedImageFiles(items)

    # User Dialogs #

    def loadRecent(self, filename):


from ..schemas.activity import CreateActivity, GetRepeatableActivity
from sqlalchemy.orm import Session
from ..models.view import View
from ..models.user import User
from ..models.post import Post
from ..schemas.user import GetUser
from ..schemas.post import GetPost
from uuid import uuid4


def create_view(session: Session, activity: CreateActivity) -> View:
    with session() as db:
        view: View = View(
            author_id=activity.user_id,
            post_id=activity.post_id,
            id='View_' + str(uuid4())
        )
        db.add(view)
        db.commit()
        db.refresh(view)
    return view

        self.filename = None
        for file in imageFiles:
            if file in self.imageList or not file.lower().endswith(tuple(extensions)):
                continue
            label_file = osp.splitext(file)[0] + ".json"
            if self.output_dir:
                label_file_without_path = osp.basename(label_file)
                label_file = osp.join(self.output_dir, label_file_without_path)
            item = QtWidgets.QListWidgetItem(file)
            item.setFlags(Qt.ItemIsEnabled | Qt.ItemIsSelectable)
            if QtCore.QFile.exists(label_file) and LabelFile.is_label_file(label_file):
                item.setCheckState(Qt.Checked)
            else:
                item.setCheckState(Qt.Unchecked)
            self.fileListWidget.addItem(item)

        if len(self.imageList) > 1:
            self.actions.openNextImg.setEnabled(True)
            self.actions.openPrevImg.setEnabled(True)

        self.openNextImg()

    def importDirImages(self, dirpath, pattern=None, load=True):
        self.actions.openNextImg.setEnabled(True)
        self.actions.openPrevImg.setEnabled(True)

        if not self.mayContinue() or not dirpath:
            return

        self.lastOpenDir = dirpath
        self.filename = None
        self.fileListWidget.clear()

        filenames = self.scanAllImages(dirpath)
        if pattern:
            try:
                filenames = [f for f in filenames if re.search(pattern, f)]
            except re.error:
                pass
        for filename in filenames:


from .extensions import drive_client, gslide_client, youtube_client

#!/usr/bin/env python

from __future__ import print_function

import os.path as osp

import numpy as np
import PIL.Image

here = osp.dirname(osp.abspath(__file__))


def main():
    label_png = osp.join(here, "apc2016_obj3_json/label.png")
    print("Loading:", label_png)
    print()

    lbl = np.asarray(PIL.Image.open(label_png))
    labels = np.unique(lbl)

    label_names_txt = osp.join(here, "apc2016_obj3_json/label_names.txt")
    label_names = [name.strip() for name in open(label_names_txt)]
    print("# of labels:", len(labels))
    print("# of label_names:", len(label_names))
    if len(labels) != len(label_names):
        print("Number of unique labels and label_names must be same.")
        quit(1)
    print()

    print("label: label_name")
    for label, label_name in zip(labels, label_names):
        print("%d: %s" % (label, label_name))


if __name__ == "__main__":
    main()


def delete_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = db.query(Like).filter(Like.author_id==activity.user_id, Like.post_id==activity.post_id).first()
        db.delete(like)
        db.commit()
    return like

def has_liked(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = db.query(Like).filter(Like.author_id==activity.user_id, Like.post_id==activity.post_id).first()
        if like:
            return True
    return False

            record.message2 = colored(record.msg)

            asctime2 = datetime.datetime.fromtimestamp(record.created)
            record.asctime2 = termcolor.colored(asctime2, color="green")

            record.module2 = termcolor.colored(record.module, color="cyan")
            record.funcName2 = termcolor.colored(record.funcName, color="cyan")
            record.lineno2 = termcolor.colored(record.lineno, color="cyan")
        return logging.Formatter.format(self, record)


logger = logging.getLogger(__appname__)
logger.setLevel(logging.INFO)

stream_handler = logging.StreamHandler(sys.stderr)
handler_format = ColoredFormatter(
    "%(asctime)s [%(levelname2)s] %(module2)s:%(funcName2)s:%(lineno2)s"
    "- %(message2)s"
)
stream_handler.setFormatter(handler_format)

logger.addHandler(stream_handler)


from ..libraries.oryks_google_oauth import (
    GoogleSlidesScope, GoogleOAuth, GoogleDirectories, GoogleDriveScopes
)
from ..libraries.youtube import YouTube
from typing import Optional, Any


def create_gslide_client() -> Any:
    secrets_file: str = "/home/lyle/oryks/backend/api/libraries/slide.json"
    scopes: list[str] = [
        GoogleSlidesScope.slides.value,
        GoogleSlidesScope.drive.value
    ]
    api_service_name: str = "slides"
    api_version: str = "v1"
    credentials_dir: str = GoogleDirectories.slides.value
    credentials_file_name: Optional[str] = 'credentials.json'

    auth: GoogleOAuth = GoogleOAuth(
        secrets_file=secrets_file,
        scopes=scopes,
        api_service_name=api_service_name,
        api_version=api_version,
        credentials_dir=credentials_dir,
        credentials_file_name=credentials_file_name
    )

    gslides_client = auth.authenticate_google_server()
    return gslides_client


def create_drive_client() -> Any:
    secrets_file: str = "/home/lyle/oryks/backend/api/libraries/drive.json"
    scopes: list[str] = [
        GoogleDriveScopes.metadata.value,
        GoogleDriveScopes.drive.value,
        GoogleDriveScopes.files.value
    ]
    api_service_name: str = "drive"
    api_version: str = "v3"


# MIT License
# Copyright (c) Kentaro Wada

import os.path as osp

import numpy as np
import PIL.Image


def lblsave(filename, lbl):
    import imgviz

    if osp.splitext(filename)[1] != ".png":
        filename += ".png"
    # Assume label ranses [-1, 254] for int32,
    # and [0, 255] for uint8 as VOC.
    if lbl.min() >= -1 and lbl.max() < 255:
        lbl_pil = PIL.Image.fromarray(lbl.astype(np.uint8), mode="P")
        colormap = imgviz.label_colormap()
        lbl_pil.putpalette(colormap.flatten())
        lbl_pil.save(filename)
    else:
        raise ValueError(
            "[%s] Cannot save the pixel-wise class label as PNG. "
            "Please consider using the .npy format." % filename
        )


from google_drive import GoogleDrive


client_secrets_file = 'drive.json'
drive = GoogleDrive(client_secret_file=client_secrets_file)
drive.authenticate()


            self.line.paint(p)
        if self.selectedShapesCopy:
            for s in self.selectedShapesCopy:
                s.paint(p)

        if (
            self.fillDrawing()
            and self.createMode == "polygon"
            and self.current is not None
            and len(self.current.points) >= 2
        ):
            drawing_shape = self.current.copy()
            if drawing_shape.fill_color.getRgb()[3] == 0:
                logger.warning(
                    "fill_drawing=true, but fill_color is transparent,"
                    " so forcing to be opaque."
                )
                drawing_shape.fill_color.setAlpha(64)
            drawing_shape.addPoint(self.line[1])
            drawing_shape.fill = True
            drawing_shape.paint(p)
        elif self.createMode == "ai_polygon" and self.current is not None:
            drawing_shape = self.current.copy()
            drawing_shape.addPoint(
                point=self.line.points[1],
                label=self.line.point_labels[1],
            )
            points = self._ai_model.predict_polygon_from_points(
                points=[[point.x(), point.y()] for point in drawing_shape.points],
                point_labels=drawing_shape.point_labels,
            )
            if len(points) > 2:
                drawing_shape.setShapeRefined(
                    shape_type="polygon",
                    points=[QtCore.QPointF(point[0], point[1]) for point in points],
                    point_labels=[1] * len(points),
                )
                drawing_shape.fill = self.fillDrawing()
                drawing_shape.selected = True
                drawing_shape.paint(p)


from ..blueprints.database.schemas.user import UserCreate
from ..blueprints.database.schemas.post import CreatePost
from ..blueprints.database.models import User, Post, Like, Comment, Bookmark
from ..blueprints.database.database import get_db
from ..blueprints.database.crud.user import create_user
from ..blueprints.database.crud.post import create_post
from faker import Faker
from datetime import datetime, timedelta
import random
from uuid import uuid4


fake = Faker()

from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy.linkextractors import LinkExtractor 


class SlidesLinkExtractor(Spider):
    name: str = "links-extractor"
    
    start_urls: list[str] = [
        "https://slidemodel.com/templates/"
    ]
    
    def __init__(self, name=None, **kwargs): 
        super().__init__(name, **kwargs) 
  
        self.link_extractor = LinkExtractor(unique=True) 
  
    def parse(self, response: Response, **kwargs: Any) -> Any: 
        links = self.link_extractor.extract_links(response) 
  
        for link in links: 
            if "tag" in link.url:
                yield {
                        "url": link.url, 
                        "text": link.text
                    }

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


class SlidesgoPipeline:
    def process_item(self, item, spider):
        return item


from langchain.prompts import PromptTemplate

from .config import Config


def get_function_prompt_template(function_code: str, config: Config) -> str:
    function_prompt_template: str = """
    Generate python docstring for the given python function using the provided documentation style:
    Function code: {function_code}
    Documentation style: {documentation_style}
    """
    prompt = PromptTemplate.from_template(template=function_prompt_template)
    prompt_formatted_str: str = prompt.format(
        function_code=function_code, documentation_style=config.documentation_style
    )
    return prompt_formatted_str


def get_class_prompt_template(class_code: str, config: Config) -> str:
    function_prompt_template: str = """
    Generate python docstring for the given python class using the provided documentation style:
    Class code: {class_code}
    Documentation style: {documentation_style}
    """
    prompt = PromptTemplate.from_template(template=function_prompt_template)
    prompt_formatted_str: str = prompt.format(
        class_code=class_code, documentation_style=config.documentation_style
    )
    return prompt_formatted_str


def create_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = Like(
            author_id=activity.user_id,
            post_id=activity.post_id
        )
        db.add(like)
        db.commit()
        db.refresh(like)
    return like

        # divide by scale to allow more precision when zoomed in
        return labelme.utils.distance(p1 - p2) < (self.epsilon / self.scale)

    def intersectionPoint(self, p1, p2):
        # Cycle through each image edge in clockwise fashion,
        # and find the one intersecting the current line segment.
        # http://paulbourke.net/geometry/lineline2d/
        size = self.pixmap.size()
        points = [
            (0, 0),
            (size.width() - 1, 0),
            (size.width() - 1, size.height() - 1),
            (0, size.height() - 1),
        ]
        # x1, y1 should be in the pixmap, x2, y2 should be out of the pixmap
        x1 = min(max(p1.x(), 0), size.width() - 1)
        y1 = min(max(p1.y(), 0), size.height() - 1)
        x2, y2 = p2.x(), p2.y()
        d, i, (x, y) = min(self.intersectingEdges((x1, y1), (x2, y2), points))
        x3, y3 = points[i]
        x4, y4 = points[(i + 1) % 4]
        if (x, y) == (x1, y1):
            # Handle cases where previous point is on one of the edges.
            if x3 == x4:
                return QtCore.QPointF(x3, min(max(0, y2), max(y3, y4)))
            else:  # y3 == y4
                return QtCore.QPointF(min(max(0, x2), max(x3, x4)), y3)
        return QtCore.QPointF(x, y)

    def intersectingEdges(self, point1, point2, points):
        """Find intersecting edges.

        For each edge formed by `points', yield the intersection
        with the line segment `(x1,y1) - (x2,y2)`, if it exists.
        Also return the distance of `(x2,y2)' to the middle of the
        edge along with its index, so that the one closest can be chosen.
        """
        (x1, y1) = point1
        (x2, y2) = point2
        for i in range(4):


#!/usr/bin/env python

from __future__ import print_function

import argparse
import distutils.spawn
import json
import os
import os.path as osp
import platform
import shlex
import subprocess
import sys


def get_ip():
    dist = platform.platform().split("-")[0]
    if dist == "Linux":
        return ""
    elif dist == "Darwin":
        cmd = "ifconfig en0"
        output = subprocess.check_output(shlex.split(cmd))
        if str != bytes:  # Python3
            output = output.decode("utf-8")
        for row in output.splitlines():
            cols = row.strip().split(" ")
            if cols[0] == "inet":
                ip = cols[1]
                return ip
        else:
            raise RuntimeError("No ip is found.")
    else:
        raise RuntimeError("Unsupported platform.")


def labelme_on_docker(in_file, out_file):
    ip = get_ip()
    cmd = "xhost + %s" % ip
    subprocess.check_output(shlex.split(cmd))



def create_prod_logger():
    """Create the application logger."""
    config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
            },
            "json": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
                "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
            },
        },
        "handlers": {
            "standard": {
                "class": "logging.StreamHandler",
                "formatter": "json",
            },
        },
        "loggers": {"": {"handlers": ["standard"], "level": logging.INFO}},
    }

    logging.config.dictConfig(config)

    logger = logging.getLogger(__name__)

    return logger


        self.points.pop(i)
        self.point_labels.pop(i)

    def isClosed(self):
        return self._closed

    def setOpen(self):
        self._closed = False

    def getRectFromLine(self, pt1, pt2):
        x1, y1 = pt1.x(), pt1.y()
        x2, y2 = pt2.x(), pt2.y()
        return QtCore.QRectF(x1, y1, x2 - x1, y2 - y1)

    def paint(self, painter):
        if self.mask is None and not self.points:
            return

        color = self.select_line_color if self.selected else self.line_color
        pen = QtGui.QPen(color)
        # Try using integer sizes for smoother drawing(?)
        pen.setWidth(max(1, int(round(2.0 / self.scale))))
        painter.setPen(pen)

        if self.mask is not None:
            image_to_draw = np.zeros(self.mask.shape + (4,), dtype=np.uint8)
            fill_color = (
                self.select_fill_color.getRgb()
                if self.selected
                else self.fill_color.getRgb()
            )
            image_to_draw[self.mask] = fill_color
            qimage = QtGui.QImage.fromData(labelme.utils.img_arr_to_data(image_to_draw))
            painter.drawImage(
                int(round(self.points[0].x())),
                int(round(self.points[0].y())),
                qimage,
            )



def create_user(user_data: UserCreate, session: Session):
    hashed_password = User.hash_password(user_data.password)
    user = User(
        id='User_' + str(uuid4()),
        first_name=user_data.first_name,
        last_name=user_data.last_name,
        email_address=user_data.email_address,
        password=hashed_password
    )
    with session() as db:
        db.add(user)
        db.commit()
        db.refresh(user)
    return user

def get_user_by_email(session: Session, email: str):
    with session() as db:
        user = db.query(User).filter(User.email_address == email).first()
    return user

from os import path
import json
from random import choices, choice
from langchain.docstore.document import Document
import re
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI
from langchain.pydantic_v1 import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnableBranch
from langchain_core.output_parsers import StrOutputParser


api_key: str = "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"
llm = OpenAI(temperature=0, api_key=api_key)
file_path: str = "comments.json"


def lower(text: str) -> str:
    return text.lower().strip()


def remove_urls(text: str) -> str:
    url_pattern = r"https?://\S+|www\.\S+"
    text = re.sub(url_pattern, "", text)
    return text


def remove_punctuations(text: str) -> str:
    punctuation_pattern = r"[^\w\s]"
    cleaned = re.sub(punctuation_pattern, "", text)
    return cleaned


def clean_text(text: str) -> str:
    text = lower(text)
    text = remove_urls(text)
    text = remove_punctuations(text)
    return text



    def _saveFile(self, filename):
        if filename and self.saveLabels(filename):
            self.addRecentFile(filename)
            self.setClean()

    def closeFile(self, _value=False):
        if not self.mayContinue():
            return
        self.resetState()
        self.setClean()
        self.toggleActions(False)
        self.canvas.setEnabled(False)
        self.actions.saveAs.setEnabled(False)

    def getLabelFile(self):
        if self.filename.lower().endswith(".json"):
            label_file = self.filename
        else:
            label_file = osp.splitext(self.filename)[0] + ".json"

        return label_file

    def deleteFile(self):
        mb = QtWidgets.QMessageBox
        msg = self.tr(
            "You are about to permanently delete this label file, " "proceed anyway?"
        )
        answer = mb.warning(self, self.tr("Attention"), msg, mb.Yes | mb.No)
        if answer != mb.Yes:
            return

        label_file = self.getLabelFile()
        if osp.exists(label_file):
            os.remove(label_file)
            logger.info("Label file is removed: {}".format(label_file))

            item = self.fileListWidget.currentItem()
            item.setCheckState(Qt.Unchecked)

            self.resetState()


    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
]
models: list[Model] = [
    UntrainedModel(
        save_path=path.join(app_config.models_dir, 'trained', model_name),
        model=model,
        classifier_name=model_name,
        train_date=datetime.now(),
        owner='Lyle Okoth'
    ) 
    for model_name, model in zip(names, classifiers)
]


def create_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = Like(
            author_id=activity.user_id,
            post_id=activity.post_id
        )
        db.add(like)
        db.commit()
        db.refresh(like)
    return like


class DevelopmentConfig(BaseConfig):
    """Development confuguration."""

    DEBUG = True
    TESTING = False
    

class TestingConfig(BaseConfig):
    """Testing configuration."""

    TESTING = True
    SECRET_KEY = 'secret-key'
    SQLALCHEMY_DATABASE_URI = 'sqlite:///'

class ProductionConfig(BaseConfig):
    """Production configuration."""

    TESTING = False


Config = {
    "development": DevelopmentConfig,
    "test": TestingConfig,
    "production": ProductionConfig,
    "staging": ProductionConfig,
}

# Get the data
from utils import extract_dataset, load_data
from experiment import Experiment
from pandas import DataFrame
from schemas import DatasetMetadata
from uuid import uuid4
from os import path
from config.config import app_config
from experiment_config import ExperimentConfig
from experiment_models import models
from experiment_pipelines import create_experiment_pipeline
import logging


logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')
# Unzip and store the downloaded dataset with its metadata
archive_name: str = 'archive.zip'
file_name: str = 'Titanic-Dataset.csv'
data_path: str = extract_dataset(archive_name, file_name)
data_path: str = extract_dataset()
data: DataFrame = load_data(data_path)
dataset_metadata: DatasetMetadata = DatasetMetadata(
    source='https://www.kaggle.com/datasets/yasserh/titanic-dataset',
    cols=data.columns.values.tolist(),
    description='A dataset that shows the survivors of the titanic tragedy.',
    path=data_path,
    id=f'Dataset_{str(uuid4())}'
)
dataset_metadata_path: str = path.join(app_config.data_dir, 'titanic_metadata.json')
dataset_metadata.save(dataset_metadata_path)

# Create an experiment for training various models
label_cols: list[str] = ['Survived']
feature_cols: list[str] = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', "PassengerId", "Name", "Ticket", "Cabin"]
columns_to_drop: list[str] = ["PassengerId", "Name", "Ticket", "Cabin"]
numerical_features: list[str] = ["Age", "Fare"]
categorical_features: list[str] = ["Pclass", "Sex", "Embarked"]
experiment_config: ExperimentConfig = ExperimentConfig(
    data_dir=app_config.data_dir,
    models_directory=app_config.models_dir,


class GetPosts(BaseModel):
    offset: Optional[int] = 0
    limit: Optional[int] = 10
    
class PostAuthor(BaseModel):
    id: str
    profile_picture: str
    name: str
    
class PostLike(BaseModel):
    liked: bool
    liked_by: Optional[list[PostAuthor]] = Field(default_factory=list)
    key_like: Optional[PostAuthor] = None
    likes_count: Optional[int] = Field(default=0)
    
class KeyComment(BaseModel):
    author: PostAuthor
    text: str
    comments_count: int
    
class PostSchema(BaseModel):
    id: str
    text: str
    image: str
    author: PostAuthor
    date_published: str
    location: str
    like: PostLike
    bookmarked: bool
    key_comment: Optional[KeyComment] = None

def list_user_likes(session: Session, user_data: GetUser) -> list[Like]:
    with session() as db:
        user: User = db.query(User).filter(User.id == user_data.user_id).first()
        likes: list[Like] = user.likes
    return likes

def list_post_likes(session: Session, post_data: GetPost):
    with session() as db:
        post: Post = db.query(Post).filter(Post.id == post_data.post_id).first()
        likes: list[Like] = post.likes
        for like in likes:
            like.author
    return likes

def get_key_like(session: Session, post_data: GetPost):
    from random import choice
    with session() as db:
        post: Post = db.query(Post).filter(Post.id == post_data.post_id).first()
        likes: list[Like] = post.likes
        for like in likes:
            like.author
    return choice(likes).author if likes else None

# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class SlidesgoItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass


from argparse import Namespace

from .config import Config
from .docstring_generator import generate_project_docstrings
from .extensions import function_code_queue, source_code_queue
from .ui import create_application_config, parse_arguments


def main():
    args: Namespace = parse_arguments()
    config: Config = create_application_config(args)
    generate_project_docstrings(
        config=config,
        source_code_queue=source_code_queue,
        function_code_queue=function_code_queue,
    )


if __name__ == "__main__":
    main()


"""This script contains helper methods for use with the application."""
from flask import Flask

from .config import Config


def set_configuration(app: Flask, flask_env: str = "development") -> None:
    """Set the application configuration.

    The application configuration will depend on the
    environment i.e Test, Development, Staging or Production.

    Parameters
    ----------
    app: flask.Flask
        A flask app instance.
    """
    app.config.from_object(Config[flask_env])

def delete_user(session: Session, user_data: GetUser):
    with session() as db:
        user = db.query(User).filter(User.id == user_data.user_id).first()
        db.delete(user)
        db.commit()
        
    return user


def user_account_active(session: Session, user_data: GetUser):
    with session() as db:
        user: User = db.query(User).filter(User.id == user_data.user_id).first()
    return user.activated

import argparse
import base64
import json
import os
import os.path as osp

import imgviz
import PIL.Image

from labelme import utils
from labelme.logger import logger


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("json_file")
    parser.add_argument("-o", "--out", default=None)
    args = parser.parse_args()

    json_file = args.json_file

    if args.out is None:
        out_dir = osp.splitext(osp.basename(json_file))[0]
        out_dir = osp.join(osp.dirname(json_file), out_dir)
    else:
        out_dir = args.out
    if not osp.exists(out_dir):
        os.mkdir(out_dir)

    data = json.load(open(json_file))
    imageData = data.get("imageData")

    if not imageData:
        imagePath = os.path.join(os.path.dirname(json_file), data["imagePath"])
        with open(imagePath, "rb") as f:
            imageData = f.read()
            imageData = base64.b64encode(imageData).decode("utf-8")
    img = utils.img_b64_to_arr(imageData)

    label_name_to_value = {"_background_": 0}


                self.editDescription.toPlainText(),
            )
        else:
            return None, None, None, None


# utils.py

from playwright.sync_api import sync_playwright
import uuid
from PIL import Image
from PIL import Image
import io
from os import path
import json

index: int = 1

def take_screenshot_from_url(url, session_data):
    with sync_playwright() as playwright:
        webkit = playwright.webkit
        browser = webkit.launch()
        browser_context = browser.new_context(device_scale_factor=2)
        browser_context.add_cookies([session_data])
        page = browser_context.new_page()
        page.goto(url)
        screenshot_bytes = page.locator(".code").screenshot()
        browser.close()
        return screenshot_bytes
    
    
def save_data(image_bytes: bytes, code: str) -> None:
    file_name: str = str(uuid.uuid4())
    image: Image = Image.open(io.BytesIO(image_bytes))
    file_path: str = "data"
    image_path: str = path.join(file_path, f"{file_name}.png")
    image.save(image_path)
    code_path: str = path.join(file_path, "metadata.jsonl")
    metadata: dict = {
        "file_name": f"{file_name}.png",
        "code": code
    }
    with open(code_path, "a+", encoding="utf-8") as f:
        f.write(json.dumps(metadata) + "\n")

        if labels:
            self.labelList.addItems(labels)
        if self._sort_labels:
            self.labelList.sortItems()
        else:
            self.labelList.setDragDropMode(QtWidgets.QAbstractItemView.InternalMove)
        self.labelList.currentItemChanged.connect(self.labelSelected)
        self.labelList.itemDoubleClicked.connect(self.labelDoubleClicked)
        self.labelList.setFixedHeight(150)
        self.edit.setListWidget(self.labelList)
        layout.addWidget(self.labelList)
        # label_flags
        if flags is None:
            flags = {}
        self._flags = flags
        self.flagsLayout = QtWidgets.QVBoxLayout()
        self.resetFlags()
        layout.addItem(self.flagsLayout)
        self.edit.textChanged.connect(self.updateFlags)
        # text edit
        self.editDescription = QtWidgets.QTextEdit()
        self.editDescription.setPlaceholderText("Label description")
        self.editDescription.setFixedHeight(50)
        layout.addWidget(self.editDescription)
        self.setLayout(layout)
        # completion
        completer = QtWidgets.QCompleter()
        if not QT5 and completion != "startswith":
            logger.warn(
                "completion other than 'startswith' is only "
                "supported with Qt5. Using 'startswith'"
            )
            completion = "startswith"
        if completion == "startswith":
            completer.setCompletionMode(QtWidgets.QCompleter.InlineCompletion)
            # Default settings.
            # completer.setFilterMode(QtCore.Qt.MatchStartsWith)
        elif completion == "contains":
            completer.setCompletionMode(QtWidgets.QCompleter.PopupCompletion)
            completer.setFilterMode(QtCore.Qt.MatchContains)


        docstring: str = ast.get_docstring(node=node)
        if not docstring or self.config.overwrite_class_docstring:
            class_code: str = ast.get_source_segment(
                source=self.module_code, node=node, padded=True
            )
            class_and_docstring: str = generate_class_docstring(class_code, self.config)
            class_docstring: str = get_class_docstring(class_and_docstring)
            new_docstring_node = make_docstring_node(class_docstring)
            node.body.insert(0, new_docstring_node)
            methods_docstrings: dict[str, str] = get_class_methods_docstrings(
                class_and_docstring
            )
            for class_node in node.body:
                if isinstance(class_node, FunctionDef):
                    function_doc: str = ast.get_docstring(node=class_node)
                    if (
                        not function_doc
                        or self.config.overwrite_class_methods_docstring
                    ):
                        function_name: str = class_node.name
                        new_docstring_node = make_docstring_node(
                            methods_docstrings[function_name]
                        )
                        class_node.body.insert(0, new_docstring_node)
        return node


import json
import os.path as osp

import imgviz

import labelme.utils


def assert_labelfile_sanity(filename):
    assert osp.exists(filename)

    data = json.load(open(filename))

    assert "imagePath" in data
    imageData = data.get("imageData", None)
    if imageData is None:
        parent_dir = osp.dirname(filename)
        img_file = osp.join(parent_dir, data["imagePath"])
        assert osp.exists(img_file)
        img = imgviz.io.imread(img_file)
    else:
        img = labelme.utils.img_b64_to_arr(imageData)

    H, W = img.shape[:2]
    assert H == data["imageHeight"]
    assert W == data["imageWidth"]

    assert "shapes" in data
    for shape in data["shapes"]:
        assert "label" in shape
        assert "points" in shape
        for x, y in shape["points"]:
            assert 0 <= x <= W
            assert 0 <= y <= H


from flask import Flask
from .home import code


def register_blueprints(app: Flask) -> bool:
    """Register the application blueprints.

    Parameters
    ----------
    app: flask.Flask
        A flask app instance

    Returns
    -------
    bool:
        Whether all the blueprints were registered.
    """
    app.register_blueprint(code)
    return True


from pydantic import BaseModel, Field
from typing import Optional
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.exceptions import RefreshError
from typing import Any
from os import path, mkdir
from json import dump, load
from models import Scopes


class OAuth(BaseModel):
    secrets_file: str
    scopes: list[str] = [
        Scopes.drafts.value, Scopes.labels.value
    ]
    api_service_name: Optional[str] = 'gmail'
    api_version: Optional[str] = 'v1'
    credentials_file_name: Optional[str] = 'credentials.json' 
    credentials_dir: Optional[str] = '.gmail_credentials'
    
    def credentials_to_dict(self, credentials: Credentials) -> dict:
        """Convert credentials to a dict."""
        return {
            'token': credentials.token,
            'refresh_token': credentials.refresh_token,
            'token_uri': credentials.token_uri,
            'client_id': credentials.client_id,
            'client_secret': credentials.client_secret,
            'scopes': credentials.scopes,
        }
        
    def create_default_credentials_path(self) -> str:
        """Create the default credentials directory."""
        current_user_home_dir = path.expanduser('~')
        if not path.exists(path.join(current_user_home_dir, self.credentials_dir)):
            mkdir(path.join(current_user_home_dir, self.credentials_dir))
        return path.join(current_user_home_dir, self.credentials_dir)


            tip=self.tr("Toggle all polygons"),
            enabled=False,
        )

        help = action(
            self.tr("&Tutorial"),
            self.tutorial,
            icon="help",
            tip=self.tr("Show tutorial page"),
        )

        zoom = QtWidgets.QWidgetAction(self)
        zoomBoxLayout = QtWidgets.QVBoxLayout()
        zoomLabel = QtWidgets.QLabel("Zoom")
        zoomLabel.setAlignment(Qt.AlignCenter)
        zoomBoxLayout.addWidget(zoomLabel)
        zoomBoxLayout.addWidget(self.zoomWidget)
        zoom.setDefaultWidget(QtWidgets.QWidget())
        zoom.defaultWidget().setLayout(zoomBoxLayout)
        self.zoomWidget.setWhatsThis(
            str(
                self.tr(
                    "Zoom in or out of the image. Also accessible with "
                    "{} and {} from the canvas."
                )
            ).format(
                utils.fmtShortcut(
                    "{},{}".format(shortcuts["zoom_in"], shortcuts["zoom_out"])
                ),
                utils.fmtShortcut(self.tr("Ctrl+Wheel")),
            )
        )
        self.zoomWidget.setEnabled(False)

        zoomIn = action(
            self.tr("Zoom &In"),
            functools.partial(self.addZoom, 1.1),
            shortcuts["zoom_in"],
            "zoom-in",
            self.tr("Increase zoom level"),


import pytest
from qtpy import QtCore
from qtpy import QtWidgets

from labelme.widgets import LabelDialog
from labelme.widgets import LabelQLineEdit


@pytest.mark.gui
def test_LabelQLineEdit(qtbot):
    list_widget = QtWidgets.QListWidget()
    list_widget.addItems(["cat", "dog", "person"])
    widget = LabelQLineEdit()
    widget.setListWidget(list_widget)
    qtbot.addWidget(widget)

    # key press to navigate in label list
    item = widget.list_widget.findItems("cat", QtCore.Qt.MatchExactly)[0]
    widget.list_widget.setCurrentItem(item)
    assert widget.list_widget.currentItem().text() == "cat"
    qtbot.keyPress(widget, QtCore.Qt.Key_Down)
    assert widget.list_widget.currentItem().text() == "dog"

    # key press to enter label
    qtbot.keyPress(widget, QtCore.Qt.Key_P)
    qtbot.keyPress(widget, QtCore.Qt.Key_E)
    qtbot.keyPress(widget, QtCore.Qt.Key_R)
    qtbot.keyPress(widget, QtCore.Qt.Key_S)
    qtbot.keyPress(widget, QtCore.Qt.Key_O)
    qtbot.keyPress(widget, QtCore.Qt.Key_N)
    assert widget.text() == "person"


@pytest.mark.gui
def test_LabelDialog_addLabelHistory(qtbot):
    labels = ["cat", "dog", "person"]
    widget = LabelDialog(labels=labels, sort_labels=True)
    qtbot.addWidget(widget)

    widget.addLabelHistory("bicycle")



def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("in_file", help="Input file or directory.")
    parser.add_argument("-O", "--output")
    args = parser.parse_args()

    if not distutils.spawn.find_executable("docker"):
        print("Please install docker", file=sys.stderr)
        sys.exit(1)

    try:
        out_file = labelme_on_docker(args.in_file, args.output)
        if out_file:
            print("Saved to: %s" % out_file)
    except RuntimeError as e:
        sys.stderr.write(e.__str__() + "\n")
        sys.exit(1)


if __name__ == "__main__":
    main()


    agent = cl.user_session.get("agent")
    msg = cl.Message(content="")
    await msg.send()
    await cl.sleep(1)
    msg.content = agent.invoke({"input": message.content})["output"]
    await msg.update()

        self.visible = {}
        self._hideBackround = False
        self.hideBackround = False
        self.hShape = None
        self.prevhShape = None
        self.hVertex = None
        self.prevhVertex = None
        self.hEdge = None
        self.prevhEdge = None
        self.movingShape = False
        self.snapping = True
        self.hShapeIsSelected = False
        self._painter = QtGui.QPainter()
        self._cursor = CURSOR_DEFAULT
        # Menus:
        # 0: right-click without selection and dragging of shapes
        # 1: right-click with selection and dragging of shapes
        self.menus = (QtWidgets.QMenu(), QtWidgets.QMenu())
        # Set widget options.
        self.setMouseTracking(True)
        self.setFocusPolicy(QtCore.Qt.WheelFocus)

        self._ai_model = None

    def fillDrawing(self):
        return self._fill_drawing

    def setFillDrawing(self, value):
        self._fill_drawing = value

    @property
    def createMode(self):
        return self._createMode

    @createMode.setter
    def createMode(self, value):
        if value not in [
            "polygon",
            "rectangle",
            "circle",


        qlabel.setAlignment(Qt.AlignBottom)

        item.setSizeHint(qlabel.sizeHint())

        self.setItemWidget(item, qlabel)


def reset_password(session: Session, password_reset: PasswordReset):
    with session() as db:
        user: User = db.query(User).filter(User.email_address == password_reset.email_address).first()
        email_address = user.decode_password_token(password_reset.password_reset_token)
        if email_address == user.email_address:
            user.password = user.hash_password(password_reset.password)
            db.commit()
            return True

    ],
)


#     description='sample description',
#     defaultLanguage='en'
# )
# playlist = youtube.insert_playlist(playlist_schema=CreatePlaylist(snippet=playlist_snippet))
# playlist_snippet = CreatePlaylistSnippet(
#     title='sample title updated',
#     description='sample description',
#     defaultLanguage='en'
# )
# playlist = youtube.update_playlist(playlist_id='PL_26vmg8W_AejZY4OPSqdHrdIaRjoSvTW', 
#                                    playlist_schema=CreatePlaylist(snippet=playlist_snippet))
# youtube.delete_playlist(playlist_id='PL_26vmg8W_AejZY4OPSqdHrdIaRjoSvTW')
# my_playlists_iterator = youtube.get_my_playlists_iterator()
# print(next(my_playlists_iterator))
# print(next(my_playlists_iterator))
# playlist_items = youtube.find_playlist_items(playlist_id='PL_26vmg8W_AfD5tzNAbIGbtTLU6ivjorS')
# item_ids = [
#     'UExfMjZ2bWc4V19BZkQ1dHpOQWJJR2J0VExVNml2am9yUy5DMkM0MjQ3OTgwQzBCMEZB',
#     'UExfMjZ2bWc4V19BZkQ1dHpOQWJJR2J0VExVNml2am9yUy5BRjY4NjdBRjA5RTdCMUMx'
# ]
# playlist_items = youtube.find_playlist_items_by_ids(item_ids)
# resource_id = VideoResourceId(videoId='jbcjK0W6U0E')
# item_snippet = CreatePlaylistItemSnippet(playlistId='PL_26vmg8W_AfD5tzNAbIGbtTLU6ivjorS', resourceId=resource_id)
# item = youtube.insert_playlist_item(CreatePlaylistItem(snippet=item_snippet))
# youtube.delete_playlist_item(playlist_item_id='UExfMjZ2bWc4V19BZkQ1dHpOQWJJR2J0VExVNml2am9yUy5CNzAzQzRDMkI3QThEQzZB')
# print(item)
# channel = youtube.find_channel_by_name('Ark Invest')
# print(channel)
# query: str = 'Python programming videos'
# max_results: int = 10
# january_2023 = datetime(year=2023, month=1, day=1)
# january_2023 = str(january_2023.strftime("%Y-%m-%dT%H:%M:%S.%fZ"))
# part: SearchPart = SearchPart()
# optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
#     q=query,
#     maxResults=max_results,
#     type=['video'],
#     publishedAfter=january_2023
# )
# search_request: YouTubeRequest = YouTubeRequest(


from google_calendar import GoogleCalendar
import os

client_secret: str = os.environ['CLIENT_SECRET_FILE']
google_calendar: GoogleCalendar = GoogleCalendar(secret_file=client_secret)
google_calendar.authenticate()

from zipfile import ZipFile
from os import path, mkdir
import os


def download_data():
    pass


def extract_archive(archive_path: str, archive_name: str, extract_path: str = None) -> None:
    if not extract_path:
        extract_path: str = "raw-data"
        if not path.exists(extract_path):
            mkdir(extract_path)
    archive_path: str = path.join(archive_path, archive_name)
    with ZipFile(file=archive_path, mode="r") as z_object:
        z_object.extractall(path=extract_path)
        
 

from dotenv import load_dotenv

load_dotenv()
from langchain.agents import AgentExecutor, Tool
from langchain.agents.format_scratchpad import format_to_openai_function_messages
from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools.render import format_tool_to_openai_function
from typing import Optional
from langchain.llms.base import BaseLLM
from langchain_openai import OpenAI, ChatOpenAI
from langchain.callbacks.manager import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from langchain.tools import BaseTool, Tool
from langchain_community.utilities.google_search import GoogleSearchAPIWrapper


class GoogleSearchTool(BaseTool):
    name = "google_search"
    description = """
    useful when you need to to search for the latest information from the web
    """

    def _run(
        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        search = GoogleSearchAPIWrapper()
        return search.run(query=query)

    async def _arun(
        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()


api_key: str = "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"


from data_utils import extract_archive
from dotenv import load_dotenv
import os


load_dotenv()

DOWNLOAD_PATH: str = os.environ["DOWNLOAD_PATH"]
EXTRACT_PATH: str = os.environ["EXTRACT_PATH"]
ARCHIVE_NAME: str = "archive (2).zip"

extract_archive(
    archive_path=DOWNLOAD_PATH, 
    archive_name=ARCHIVE_NAME, 
    extract_path=EXTRACT_PATH
)



# print(video)

# with open('replies.json', 'w') as f:
#     json.dump(replies, f, indent=4)
# from youtube.resources.comment_thread.comment import CommentResource
# import json
# comment_res = CommentResource(youtube_client)
# with open('replies.json', 'r') as f:
#     comments = json.load(f)
# print(comment_res.parse_youtube_response(comments))
# print(len(comment_res.parse_youtube_response(comments).items))

# comment_text = 'Sample comment text'
# video_id = 'jl3b4eLKiP8'
# comment = youtube.insert_comment(video_id, comment_text)
# print(comment)

# comment_id = 'UgyblK9NGskUXp91WIt4AaABAg'
# reply = 'Sample comment reply'
# comment_reply = youtube.reply_to_comment(comment_id, reply)
# channel = youtube.find_my_channel()
# print(channel.items[0].snippet.custom_url)
# import json
# my_activities = youtube.list_my_activities()
# with open('my_activities.json', 'w') as f:
#     json.dump(my_activities, f, indent=4)
# print(my_activities)
# print(youtube.find_channel_by_id(channel_id="UCj3FYGvdAVNdD3M0dvKr3cA"))
it = youtube.get_comments_iterator(video_id='cBpGq-vDr2Y')
print(next(it))

        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


            line_path = QtGui.QPainterPath()
            contours = skimage.measure.find_contours(np.pad(self.mask, pad_width=1))
            for contour in contours:
                contour += [self.points[0].y(), self.points[0].x()]
                line_path.moveTo(contour[0, 1], contour[0, 0])
                for point in contour[1:]:
                    line_path.lineTo(point[1], point[0])
            painter.drawPath(line_path)

        if self.points:
            line_path = QtGui.QPainterPath()
            vrtx_path = QtGui.QPainterPath()
            negative_vrtx_path = QtGui.QPainterPath()

            if self.shape_type in ["rectangle", "mask"]:
                assert len(self.points) in [1, 2]
                if len(self.points) == 2:
                    rectangle = self.getRectFromLine(*self.points)
                    line_path.addRect(rectangle)
                if self.shape_type == "rectangle":
                    for i in range(len(self.points)):
                        self.drawVertex(vrtx_path, i)
            elif self.shape_type == "circle":
                assert len(self.points) in [1, 2]
                if len(self.points) == 2:
                    rectangle = self.getCircleRectFromLine(self.points)
                    line_path.addEllipse(rectangle)
                for i in range(len(self.points)):
                    self.drawVertex(vrtx_path, i)
            elif self.shape_type == "linestrip":
                line_path.moveTo(self.points[0])
                for i, p in enumerate(self.points):
                    line_path.lineTo(p)
                    self.drawVertex(vrtx_path, i)
            elif self.shape_type == "points":
                assert len(self.points) == len(self.point_labels)
                for i, point_label in enumerate(self.point_labels):
                    if point_label == 1:
                        self.drawVertex(vrtx_path, i)
                    else:


client_secrets_file = '/home/lyle/Downloads/client_secret.json'
credentials_path = '.'
youtube = YouTube(client_secrets_file)
youtube.authenticate(credentials_directory=credentials_path)
search_iterator = youtube.search_video('Python for beginners' ,max_results=2)
print(list(next(search_iterator)))
# print(list(next(search_iterator)))
# print(len(search_iterator.items))
# print_videos(next(search_iterator))
# print_videos(next(search_iterator))
# print_videos(next(search_iterator))
# print(next(search_iterator))
# print_videos(list(next(search_iterator)))
# print_videos(next(search_iterator))
# print_videos(next(search_iterator))
# videos = youtube.find_video_by_id('RFDK1rdJ_gg')
# print(videos)
# save_to_channels(video)
# ids = ['rfscVS0vtbw', 'TFa38ONq5PY']
# youtube.find_videos(ids)
# youtube.find_most_popular_video_by_region('us')
# search_iterator = youtube.search_channel('Python for beginners',max_results=2)
# channel = youtube.find_channel_by_id('UCu8luTDe_Xxd2ahAXsCWX5g')
# print(channel)
# print(channel.to_json())
# to_json([channel.to_dict()])
# search_iterator = youtube.search_channel('Python for beginners',max_results=2)
# print(next(search_iterator))
# channel = youtube.find_channel_by_name('GoogleDevelopers')
# channel = youtube.find_channel_by_name('@PROROBOTS')
# print(channel)
# search_iterator = youtube.find_video_comments('VSB2vjWa1LA', max_results=20)
# print(next(search_iterator))
# print(next(search_iterator))
# search_iterator = youtube.find_all_channel_comments('UCu8luTDe_Xxd2ahAXsCWX5g', max_results=20)
# print(next(search_iterator))
# print(next(search_iterator))
# search_iterator = youtube.search_playlist('Python for beginners',max_results=20)
# print(next(search_iterator))
# print(next(search_iterator))


        loc="rb",
    )

    plt.subplot(121)
    plt.imshow(img)
    plt.subplot(122)
    plt.imshow(lbl_viz)
    plt.show()


if __name__ == "__main__":
    main()




def get_channels() -> list[Channel]:
    logging.info('Getting the channel details from the database.')
    channels: list[Channel] = get_all_channels(get_db)
    logging.info('Fetched the channel details from the database.')
    return channels

def get_channel_names() -> list[str]:
    logging.info('Fetching the channel names.')
    channels: list[Channel] = get_all_channels(get_db)
    logging.info('Fetched the channel names.')
    return [channel.title for channel in channels]


def get_channel_id(name: str) -> str:
    channel: Channel = get_channel_by_title(name, get_db)
    return channel.id


def find_latest_video(channel_id: str, youtube_client: YouTube) -> Video:
    if redis.get(name=f'latest:{channel_id}'):
        logging.info('Found video in cache, retrieving it.')
        video_str: str = redis.get(name=f'latest:{channel_id}')
        video: Video = Video(**loads(video_str))
        logging.info('Successfully retrieved video from cache.')
    else:
        logging.info('Video missing from cache, retrieving from youtube.')
        part: SearchPart = SearchPart()
        optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
            q='',
            maxResults=1,
            type=['video'],
            channelId=channel_id,
            order='date'
        )
        search: YouTubeRequest = YouTubeRequest(part=part, optional_parameters=optional_parameters)
        response: YouTubeResponse = youtube_client.search(search)
        logging.info('Retrieved the latest video from youtube.')
        search_responses: list[Search] = response.items



topic_assign_msg: str = """
Below is a list of customer comments in JSON format with the following keys:
1. doc_id - identifier of the comment
2. comment - the user comment

Please analyze the provided comments and identify the main topics and sentiment. Include only the 
topics mentioned in the following text:
Text: {topics}

{format_instructions}

user comments: 
```{comments}```
"""

topic_assign_tmpl = PromptTemplate(
    template=topic_assign_msg,
    input_variables=["topics", "comments", "format_instructions"],
)
# topic_assign_tmpl = ChatPromptTemplate.from_messages(
#     [
#         ("system", "You are a helpful assistant. Your task is to analyze user comments."),
#         ("user", topic_assign_msg)
#     ]
# )

# messages = topic_assign_tmpl.format(
#     topics=topics,
#     format_instructions=format_instructions,
#     comments=json.dumps(comments)
# )

inputs = {
    "topics": topics,
    "format_instructions": format_instructions,
    "comments": json.dumps(comments),
}

chat = OpenAI(temperature=0, api_key=api_key)


            if osp.dirname(filename) and not osp.exists(osp.dirname(filename)):
                os.makedirs(osp.dirname(filename))
            lf.save(
                filename=filename,
                shapes=shapes,
                imagePath=imagePath,
                imageData=imageData,
                imageHeight=self.image.height(),
                imageWidth=self.image.width(),
                otherData=self.otherData,
                flags=flags,
            )
            self.labelFile = lf
            items = self.fileListWidget.findItems(self.imagePath, Qt.MatchExactly)
            if len(items) > 0:
                if len(items) != 1:
                    raise RuntimeError("There are duplicate files.")
                items[0].setCheckState(Qt.Checked)
            # disable allows next and previous image to proceed
            # self.filename = filename
            return True
        except LabelFileError as e:
            self.errorMessage(
                self.tr("Error saving label data"), self.tr("<b>%s</b>") % e
            )
            return False

    def duplicateSelectedShape(self):
        added_shapes = self.canvas.duplicateSelectedShapes()
        for shape in added_shapes:
            self.addLabel(shape)
        self.setDirty()

    def pasteSelectedShape(self):
        self.loadShapes(self._copied_shapes, replace=False)
        self.setDirty()

    def copySelectedShape(self):
        self._copied_shapes = [s.copy() for s in self.canvas.selectedShapes]
        self.actions.paste.setEnabled(len(self._copied_shapes) > 0)


        elif self.createMode == "ai_mask" and self.current is not None:
            drawing_shape = self.current.copy()
            drawing_shape.addPoint(
                point=self.line.points[1],
                label=self.line.point_labels[1],
            )
            mask = self._ai_model.predict_mask_from_points(
                points=[[point.x(), point.y()] for point in drawing_shape.points],
                point_labels=drawing_shape.point_labels,
            )
            y1, x1, y2, x2 = imgviz.instances.masks_to_bboxes([mask])[0].astype(int)
            drawing_shape.setShapeRefined(
                shape_type="mask",
                points=[QtCore.QPointF(x1, y1), QtCore.QPointF(x2, y2)],
                point_labels=[1, 1],
                mask=mask[y1 : y2 + 1, x1 : x2 + 1],
            )
            drawing_shape.selected = True
            drawing_shape.paint(p)

        p.end()

    def transformPos(self, point):
        """Convert from widget-logical coordinates to painter-logical ones."""
        return point / self.scale - self.offsetToCenter()

    def offsetToCenter(self):
        s = self.scale
        area = super(Canvas, self).size()
        w, h = self.pixmap.width() * s, self.pixmap.height() * s
        aw, ah = area.width(), area.height()
        x = (aw - w) / (2 * s) if aw > w else 0
        y = (ah - h) / (2 * s) if ah > h else 0
        return QtCore.QPointF(x, y)

    def outOfPixmap(self, p):
        w, h = self.pixmap.width(), self.pixmap.height()
        return not (0 <= p.x() <= w - 1 and 0 <= p.y() <= h - 1)

    def finalise(self):



        # Polygon drawing.
        if self.drawing():
            if self.createMode in ["ai_polygon", "ai_mask"]:
                self.line.shape_type = "points"
            else:
                self.line.shape_type = self.createMode

            self.overrideCursor(CURSOR_DRAW)
            if not self.current:
                self.repaint()  # draw crosshair
                return

            if self.outOfPixmap(pos):
                # Don't allow the user to draw outside the pixmap.
                # Project the point to the pixmap's edges.
                pos = self.intersectionPoint(self.current[-1], pos)
            elif (
                self.snapping
                and len(self.current) > 1
                and self.createMode == "polygon"
                and self.closeEnough(pos, self.current[0])
            ):
                # Attract line to starting point and
                # colorise to alert the user.
                pos = self.current[0]
                self.overrideCursor(CURSOR_POINT)
                self.current.highlightVertex(0, Shape.NEAR_VERTEX)
            if self.createMode in ["polygon", "linestrip"]:
                self.line.points = [self.current[-1], pos]
                self.line.point_labels = [1, 1]
            elif self.createMode in ["ai_polygon", "ai_mask"]:
                self.line.points = [self.current.points[-1], pos]
                self.line.point_labels = [
                    self.current.point_labels[-1],
                    0 if is_shift_pressed else 1,
                ]
            elif self.createMode == "rectangle":
                self.line.points = [self.current[0], pos]
                self.line.point_labels = [1, 1]


        slider = QtWidgets.QSlider(Qt.Horizontal)
        slider.setRange(0, 150)
        slider.setValue(50)
        slider.valueChanged.connect(self.onNewValue)
        return slider


        "requests": create_slides_request
    }
    response = (
        slide_client.presentations()
        .batchUpdate(presentationId=presentation_id, body=request_body)
        .execute()
    )
    create_slide_response = response.get("replies")[0].get("createSlide")
    print(f"Created slide with ID:{(create_slide_response.get('objectId'))}")
    slide_name: str = f"{page_id.casefold().strip().replace(' ', '_')}.json"
    with open(slide_name, "w") as f:
        json.dump(response, f)
    return response


def create_textbox_with_text(presentation_id: str, page_id: str, slide_client: Any) -> dict:
  try:
    # Create a new square textbox, using the supplied element ID.
    element_id = "MyTextBox_10"
    pt350 = {"magnitude": 350, "unit": "PT"}
    requests = [
        {
            "createShape": {
                "objectId": element_id,
                "shapeType": "TEXT_BOX",
                "elementProperties": {
                    "pageObjectId": page_id,
                    "size": {"height": pt350, "width": pt350},
                    "transform": {
                        "scaleX": 1,
                        "scaleY": 1,
                        "translateX": 350,
                        "translateY": 100,
                        "unit": "PT",
                    },
                },
            }
        },
        # Insert text into the box, using the supplied element ID.
        {


import ast
from ast import AST, NodeTransformer
from queue import Empty, Queue
from openai import RateLimitError

from .config import Config
from .helpers import format_file, parse_src, read_src, save_src
from .transformers import FunctionTransformer
from .walkers import FunctionVisitor


def process_file(source_code_queue: Queue, function_code_queue: Queue):
    while True:
        try:
            file_path: str = source_code_queue.get(timeout=1)
            print(f"processing the file: {file_path}")
            file_src: str = read_src(file_path=file_path)
            src_tree: AST = parse_src(file_src)
            visitor: FunctionVisitor = FunctionVisitor(
                function_code_queue=function_code_queue,
                file_path=file_path,
            )
            visitor.visit(src_tree)
            source_code_queue.task_done()
        except Empty:
            print("Terminating the file processing..")
            break


def process_function(config: Config, function_code_queue: Queue) -> None:
    while True:
        try:
            file_path, function_code = function_code_queue.get(timeout=1)
            # print(function_code)
            file_src: str = read_src(file_path=file_path)
            src_tree: AST = parse_src(file_src)
            # print_src(src_tree)
            transformer: NodeTransformer = FunctionTransformer(
                config=config, function_src=function_code
            )


from dotenv import load_dotenv
load_dotenv()
import chainlit as cl

from calendar_assistant.usecases.agent import agent_executor


@cl.on_message
async def main(message: cl.Message):
    msg = cl.Message(content="")
    await msg.send()
    await cl.sleep(1)
    msg.content = agent_executor.invoke({"input": message.content})['output']
    await msg.update()

        image_file = files[0]
        image_data = image_file.content # byte values of the image
        image = Image.open(io.BytesIO(image_data))
        model = load_model()
        predicted_label, predictions = evaluate_image(image, model)
        analysis_text: str = f"""
            After analyzing the image you uploaded, here is what I found:
            Maize Leaf Rust probability: {predictions['Maize Leaf Rust']}%
            Northern Leaf Blight probability: {predictions['Northern Leaf Blight']}%
            Healthy probability: {predictions['Healthy']}%
            Gray Leaf Spot probability: {predictions['Gray Leaf Spot']}%
            Your plant is most likely infected with {predicted_label}.
            """
        elements = [
            cl.Image(
                name="image2", display="inline", content=image_data
                ), 
            cl.Text(name="simple_text", content=analysis_text, display="inline", size='large')
        ]
        await cl.Message(content=f"Maize image with {predicted_label}!", elements=elements).send()
        msg = cl.Message(content="")
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Tell me some facts about the maize disease {predicted_label} especially in relation to kenya.')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'What fungicides or pesticides can be used to deal with the maize disease {predicted_label}?')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Get me aggrovets in {user_location}, Kenya')
        await msg.update()
        await cl.Message(content='Feel free to ask me more questions about maize plant diseases and how to deal with them.').send()
    else:
        await cl.Message(content='Currently cannot detect pests. Still working on that model.').send()
    

@cl.on_message
async def main(message: cl.Message):


import os.path as osp
import shutil

import yaml

from labelme.logger import logger

here = osp.dirname(osp.abspath(__file__))


def update_dict(target_dict, new_dict, validate_item=None):
    for key, value in new_dict.items():
        if validate_item:
            validate_item(key, value)
        if key not in target_dict:
            logger.warn("Skipping unexpected key in config: {}".format(key))
            continue
        if isinstance(target_dict[key], dict) and isinstance(value, dict):
            update_dict(target_dict[key], value, validate_item=validate_item)
        else:
            target_dict[key] = value


# -----------------------------------------------------------------------------


def get_default_config():
    config_file = osp.join(here, "default_config.yaml")
    with open(config_file) as f:
        config = yaml.safe_load(f)

    # save default config to ~/.labelmerc
    user_config_file = osp.join(osp.expanduser("~"), ".labelmerc")
    if not osp.exists(user_config_file):
        try:
            shutil.copy(config_file, user_config_file)
        except Exception:
            logger.warn("Failed to save config: {}".format(user_config_file))

    return config


import os
import pandas as pd
from spacy.lang.en import English
import torch
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from collections import defaultdict
from torchvision.transforms import Compose, ToTensor, Resize


nlp = English()
# Create a Tokenizer with the default settings for English
# including punctuation rules and exceptions
tokenizer = nlp.tokenizer

transforms = Compose([
    ToTensor()
])

class Vocabulary:
    def __init__(self, freq_threshold: int) -> None:
        self.freq_threshold: int = freq_threshold
        self.itos: dict[int, str] = {
            0: "<PAD>",
            1: "<SOS>",
            2: "<EOS>",
            3: "<UNK>"
        }
        self.stoi: dict[str, int] = self.invert_dict(self.itos)
        
    def __len__(self) -> int:
        return len(self.itos)
    
    @staticmethod
    def tokenize(text: str) -> list[str]:
        return [token.text.lower() for token in tokenizer(text)]
        
    @staticmethod
    def invert_dict(dct: dict) -> dict:



    def getCircleRectFromLine(self, line):
        """Computes parameters to draw with `QPainterPath::addEllipse`"""
        if len(line) != 2:
            return None
        (c, point) = line
        r = line[0] - line[1]
        d = math.sqrt(math.pow(r.x(), 2) + math.pow(r.y(), 2))
        rectangle = QtCore.QRectF(c.x() - d, c.y() - d, 2 * d, 2 * d)
        return rectangle

    def makePath(self):
        if self.shape_type in ["rectangle", "mask"]:
            path = QtGui.QPainterPath()
            if len(self.points) == 2:
                rectangle = self.getRectFromLine(*self.points)
                path.addRect(rectangle)
        elif self.shape_type == "circle":
            path = QtGui.QPainterPath()
            if len(self.points) == 2:
                rectangle = self.getCircleRectFromLine(self.points)
                path.addEllipse(rectangle)
        else:
            path = QtGui.QPainterPath(self.points[0])
            for p in self.points[1:]:
                path.lineTo(p)
        return path

    def boundingRect(self):
        return self.makePath().boundingRect()

    def moveBy(self, offset):
        self.points = [p + offset for p in self.points]

    def moveVertexBy(self, i, offset):
        self.points[i] = self.points[i] + offset

    def highlightVertex(self, i, action):
        """Highlight a vertex appropriately based on the current action



            ('classifier', untrained_model)
        ])
    if model_params:
        untrained_model.set_params(**model_params)
    train_start_time: float = perf_counter()
    pipeline.fit(train_config.train_features, train_config.train_labels.values.ravel())
    train_stop_time: float = perf_counter()
    predictions: list[int] = pipeline.predict(train_config.test_features).tolist()
    accuracy: float = accuracy_score(train_config.test_labels, predictions)
    precision: float = precision_score(train_config.test_labels, predictions)
    recall: float = recall_score(train_config.test_labels, predictions)
    f1: float = f1_score(train_config.test_labels, predictions)
    metrics: Metrics = Metrics(
        accuracy=round(accuracy,2),
        precision=round(precision,2),
        recall=round(recall,2),
        f1=round(f1,2)
    )
    model_train_time = train_stop_time - train_start_time
    if train:
        save_path: str = path.join(app_config.models_dir, 'trained', train_config.classifier_name)
    else:
        save_path: str = path.join(app_config.models_dir, 'tuned', train_config.classifier_name)
    Model.save_model(pipeline, save_path)
    return {
        'model_name': train_config.classifier_name,
        'metrics': metrics,
        'train_time': model_train_time
    }
    
    
@celery.task(name='train_tuned_model')
def train_tuned_model(tuned_model_data: dict, train_config: TrainConfig):
    train_results: dict = fit_pipeline(train_config=train_config, model_params=tuned_model_data['params'])
    tuned_model: TunedModel = TunedModel(
        classifier_name=train_results['model_name'],
        train_date=datetime.now(),
        save_path=path.join(app_config.models_dir, 'trained', train_results['model_name']),
        owner='Lyle Okoth',
        metrics=train_results['metrics'],


from langchain.prompts import PromptTemplate

function_prompt_template: str = """
Generate python docstring for the given python function using the provided documentation style.
Make sure to provide atleast two examples of the function usage only in the docstring as well
as the exceptions that may be raised when using the function. Make sure to return the
function and its docstring.
Function code: {function_code}
Documentation style: {documentation_style}
"""
function_prompt = PromptTemplate.from_template(template=function_prompt_template)


from flask import Flask
from .home import home
from .oauth import auth, google_blueprint


def register_blueprints(app: Flask) -> bool:
    """Register the application blueprints.

    Parameters
    ----------
    app: flask.Flask
        A flask app instance

    Returns
    -------
    bool:
        Whether all the blueprints were registered.
    """
    app.register_blueprint(home)
    app.register_blueprint(auth, url_prefix="/auth")
    app.register_blueprint(google_blueprint, url_prefix="/login")
    return True

        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesn‚Äôt have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


class LeetcodeDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called


import ast
from ast import AST, FunctionDef, NodeTransformer

from .config import Config
from .helpers import generate_doc_string, make_docstring_node
from .model_parsers import parse_function_docstr


class FunctionTransformer(NodeTransformer):
    def __init__(self, config: Config, function_src: str) -> None:
        super().__init__()
        self._config: Config = config
        self._function_src = function_src

    def visit_FunctionDef(self, node: FunctionDef) -> None:
        ast_tree: AST = ast.parse(self._function_src)
        function_node: AST = ast_tree.body[0]
        docstring: str = ast.get_docstring(node=node)
        if function_node.name == node.name:
            if not docstring:
                src_code: str = ast.unparse(node)
                func_docstr: str = generate_doc_string(
                    src_code=src_code, config=self._config
                )
                doc_str: str = parse_function_docstr(func_docstr.strip())
                if doc_str:
                    dcstr_node: AST = make_docstring_node(doc_str)
                    node.body.insert(0, dcstr_node)
            elif self._config.overwrite_function_docstring:
                src_code: str = ast.unparse(node)
                func_docstr: str = generate_doc_string(
                    src_code=src_code, config=self._config
                )
                doc_str: str = parse_function_docstr(func_docstr.strip())
                if doc_str:
                    dcstr_node: AST = make_docstring_node(doc_str)
                    node.body[0] = dcstr_node
        return node


        p = self._painter
        p.begin(self)
        p.setRenderHint(QtGui.QPainter.Antialiasing)
        p.setRenderHint(QtGui.QPainter.HighQualityAntialiasing)
        p.setRenderHint(QtGui.QPainter.SmoothPixmapTransform)

        p.scale(self.scale, self.scale)
        p.translate(self.offsetToCenter())

        p.drawPixmap(0, 0, self.pixmap)

        # draw crosshair
        if (
            self._crosshair[self._createMode]
            and self.drawing()
            and self.prevMovePoint
            and not self.outOfPixmap(self.prevMovePoint)
        ):
            p.setPen(QtGui.QColor(0, 0, 0))
            p.drawLine(
                0,
                int(self.prevMovePoint.y()),
                self.width() - 1,
                int(self.prevMovePoint.y()),
            )
            p.drawLine(
                int(self.prevMovePoint.x()),
                0,
                int(self.prevMovePoint.x()),
                self.height() - 1,
            )

        Shape.scale = self.scale
        for shape in self.shapes:
            if (shape.selected or not self._hideBackround) and self.isVisible(shape):
                shape.fill = shape.selected or shape == self.hShape
                shape.paint(p)
        if self.current:
            self.current.paint(p)
            assert len(self.line.points) == len(self.line.point_labels)


            x3, y3 = points[i]
            x4, y4 = points[(i + 1) % 4]
            denom = (y4 - y3) * (x2 - x1) - (x4 - x3) * (y2 - y1)
            nua = (x4 - x3) * (y1 - y3) - (y4 - y3) * (x1 - x3)
            nub = (x2 - x1) * (y1 - y3) - (y2 - y1) * (x1 - x3)
            if denom == 0:
                # This covers two cases:
                #   nua == nub == 0: Coincident
                #   otherwise: Parallel
                continue
            ua, ub = nua / denom, nub / denom
            if 0 <= ua <= 1 and 0 <= ub <= 1:
                x = x1 + ua * (x2 - x1)
                y = y1 + ua * (y2 - y1)
                m = QtCore.QPointF((x3 + x4) / 2, (y3 + y4) / 2)
                d = labelme.utils.distance(m - QtCore.QPointF(x2, y2))
                yield d, i, (x, y)

    # These two, along with a call to adjustSize are required for the
    # scroll area.
    def sizeHint(self):
        return self.minimumSizeHint()

    def minimumSizeHint(self):
        if self.pixmap:
            return self.scale * self.pixmap.size()
        return super(Canvas, self).minimumSizeHint()

    def wheelEvent(self, ev):
        if QT5:
            mods = ev.modifiers()
            delta = ev.angleDelta()
            if QtCore.Qt.ControlModifier == int(mods):
                # with Ctrl/Command key
                # zoom
                self.zoomRequest.emit(delta.y(), ev.pos())
            else:
                # scroll
                self.scrollRequest.emit(delta.x(), QtCore.Qt.Horizontal)
                self.scrollRequest.emit(delta.y(), QtCore.Qt.Vertical)


    cleaned_comments: list[str] = list(map(clean_text, all_comments))
    comments: list[str] = choices(population=cleaned_comments, k=10)
    comments: list[Document] = [Document(page_content=comment) for comment in comments if is_acceptable_len(comment)]
    
data_dir = "./agent_nelly/data_analysis/data"
features_dir = "features"
save_features_dir = path.join(data_dir, features_dir, "features.json")

with open(save_features_dir, 'r') as f:
    topics: list[str] = json.load(f)

topic_assignment_msg: str = """
You are provided with a detailed summary for the review of the product {product}. You will also be 
given a comment, which is a reaction to the product review. Please analyze the  

List of topics: {topics}
Comments: {comments}
"""



    image_paths = Column(ScalarListType())
    image_urls = Column(ScalarListType())
    

class Tag(Base):
    __tablename__ = "tag"

    id = Column(String(), primary_key=True)
    name = Column('name', String(30), unique=True)
    slides = relationship('Slide', secondary='slide_tag',
        lazy='dynamic', backref="tag")  # M-to-M for quote and tag
    
    
class Category(Base):
    __tablename__ = "category"

    id = Column(String(), primary_key=True)
    name = Column('name', String(50), unique=True)
    slides = relationship('Slide', backref='category')

def load_model(model_path: str) -> Pipeline:
    """Load a saved model."""
    try:
        model: Pipeline = load(model_path)
    except FileNotFoundError:
        logging.error(f'There is no such model "{model_path}".')
    return model


def send_email():
    logging.info('Sending email.')
    secrets_path = app_config.secret_file
    gmail_client = get_gmail_client(secrets_path)
    message = create_message()
    message = send_message(gmail_client, message)
    logging.info('Email Sent.')
    logging.info(message)


        os.makedirs(osp.join(args.output_dir, "Visualization"))
    print("Creating dataset:", args.output_dir)

    now = datetime.datetime.now()

    data = dict(
        info=dict(
            description=None,
            url=None,
            version=None,
            year=now.year,
            contributor=None,
            date_created=now.strftime("%Y-%m-%d %H:%M:%S.%f"),
        ),
        licenses=[
            dict(
                url=None,
                id=0,
                name=None,
            )
        ],
        images=[
            # license, url, file_name, height, width, date_captured, id
        ],
        type="instances",
        annotations=[
            # segmentation, area, iscrowd, image_id, bbox, category_id, id
        ],
        categories=[
            # supercategory, id, name
        ],
    )

    class_name_to_id = {}
    for i, line in enumerate(open(args.labels).readlines()):
        class_id = i - 1  # starts with -1
        class_name = line.strip()
        if class_id == -1:
            assert class_name == "__ignore__"
            continue


from celery_app import celery_app
import logging
from play_list import get_youtube_client, get_channel_names, workflow
from youtube import YouTube


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')


@celery_app.task
def create_daily_playlist() -> None:
    client_secret_file: str = 'client_secret.json'
    logging.info('Getting the youtube client.')
    youtube: YouTube = get_youtube_client(client_secret_file)
    logging.info('Getting the channel names.')
    channel_names: list[str] = get_channel_names()[:2]
    logging.info('Fetched the channel names: ')
    logging.info(channel_names)
    playlist_name: str = 'Daily Videos'
    logging.info('Fetching and adding latest videos to the playlist "%s".', playlist_name)
    playlist_items: list[str] = workflow(youtube, channel_names, playlist_name)
    if playlist_items:
        logging.info('Added the following playlist items: ')
        logging.info(playlist_items)
        logging.info('Notifying via email.')
    return playlist_items


@celery_app.task
def clear_daily_playlist() -> None:
    logging.info('Clearing daily playlist.')

from datetime import datetime
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy import ForeignKey
from ..database import Base


class Post(Base):
    __tablename__ = 'posts'
    
    id: Mapped[str] = mapped_column(primary_key=True)
    author_id: Mapped[str] = mapped_column(ForeignKey('users.id'))
    location: Mapped[str]
    text: Mapped[str]
    image_url: Mapped[str]
    date_published: Mapped[datetime] = mapped_column(default_factory=datetime.utcnow)
    date_updated: Mapped[datetime] = mapped_column(onupdate=datetime.utcnow, default_factory=datetime.utcnow)
    
    author = relationship('User', back_populates='posts')
    bookmarks = relationship('Bookmark', back_populates='post')
    likes = relationship('Like', back_populates='post')
    comments = relationship('Comment', back_populates='post')
    views = relationship('View', back_populates='post')

"""This module resgisters the application extensions.

Example:
    To register the extensions:
        register_extensions(app)

@Author: Lyle Okoth
@Date: 28/06/2023
@Portfolio: https://lyleokoth.oryks-sytem.com
"""
from flask import Flask

from .extensions import bcrypt, cors


def register_extensions(app: Flask) -> None:
    """Register the application extensions.

    Parameters
    ----------
    app: flask.Flask
        The Flask app instance.
    """
    bcrypt.init_app(app)
    cors.init_app(app)

    credentials_dir: str = GoogleDirectories.drive.value
    credentials_file_name: Optional[str] = 'credentials.json'

    auth: GoogleOAuth = GoogleOAuth(
        secrets_file=secrets_file,
        scopes=scopes,
        api_service_name=api_service_name,
        api_version=api_version,
        credentials_dir=credentials_dir,
        credentials_file_name=credentials_file_name
    )

    drive_client = auth.authenticate_google_server()
    return drive_client


def get_youtube_client() -> YouTube:
    client_secrets_file: str = "/home/lyle/oryks/backend/api/libraries/youtube.json"
    youtube: YouTube = YouTube(client_secret_file=client_secrets_file)
    return youtube

    # channel_playlists = get_channel_playlists('UC5WVOSvL9bc6kwCMXXeFLLw')
    # playlist_items = get_playlist_items('PLouh1K1d9jkYZo8h1zPH3P1ScAWA8gxbu')
    # playlist_video_ids = list(map(get_playlist_item_video_id, playlist_items))
    # playlist_videos = get_videos(playlist_video_ids)
    video_comments = get_video_comments('pIzyo4cCGxU')
    print(video_comments)

if __name__ == '__main__':
    main()

def create_post(post_data: CreatePost, post_image: dict, session: Session):
    post_image_url: str = save_post_photo(post_image)
    post: Post = Post(
        id='Post_' + str(uuid4()),
        author_id=post_data.author_id,
        location=post_data.location,
        text=post_data.text,
        image_url=post_image_url
    )
    with session() as db:
        db.add(post)
        db.commit()
        db.refresh(post)
    return post

import imgviz
from qtpy import QtCore
from qtpy import QtGui
from qtpy import QtWidgets

import labelme.ai
import labelme.utils
from labelme import QT5
from labelme.logger import logger
from labelme.shape import Shape

# TODO(unknown):
# - [maybe] Find optimal epsilon value.


CURSOR_DEFAULT = QtCore.Qt.ArrowCursor
CURSOR_POINT = QtCore.Qt.PointingHandCursor
CURSOR_DRAW = QtCore.Qt.CrossCursor
CURSOR_MOVE = QtCore.Qt.ClosedHandCursor
CURSOR_GRAB = QtCore.Qt.OpenHandCursor

MOVE_SPEED = 5.0


class Canvas(QtWidgets.QWidget):
    zoomRequest = QtCore.Signal(int, QtCore.QPoint)
    scrollRequest = QtCore.Signal(int, int)
    newShape = QtCore.Signal()
    selectionChanged = QtCore.Signal(list)
    shapeMoved = QtCore.Signal()
    drawingPolygon = QtCore.Signal(bool)
    vertexSelected = QtCore.Signal(bool)

    CREATE, EDIT = 0, 1

    # polygon, rectangle, line, or point
    _createMode = "polygon"

    _fill_drawing = False



class User(Base):
    __tablename__ = 'users'
    
    id: Mapped[str] = mapped_column(primary_key=True)
    first_name: Mapped[str]
    last_name: Mapped[str]
    email_address: Mapped[str] = mapped_column(unique=True)
    password: Mapped[str]
    registration_date: Mapped[datetime] = mapped_column(default_factory=datetime.utcnow)
    role: Mapped[str] = mapped_column(default='user')
    activated: Mapped[bool] = mapped_column(default=False)
    profile_picture_url: Mapped[str] = mapped_column(default='default.jpeg')
    
    posts = relationship('Post', back_populates='author')
    bookmarks = relationship('Bookmark', back_populates='author')
    likes = relationship('Like', back_populates='author')
    comments = relationship('Comment', back_populates='author')
    views = relationship('View', back_populates='author')
    
    @staticmethod
    def hash_password(password: str) -> str:
        return bcrypt.generate_password_hash(password).decode("utf-8")

    def check_password(self, password: str) -> bool:
        return bcrypt.check_password_hash(self.password, password)

from ..schemas.user import UserCreate
from sqlalchemy.orm import Session
from ....extensions.extensions import bcrypt
from ..models.user import User
from ..schemas.user import User as Userschemas
from ..schemas.user import (
    GetUser, GetUsers, ActivateUser, LoginUser, RequestPasswordReset, PasswordReset
)
from typing import Optional
from jwt import ExpiredSignatureError, InvalidTokenError
from uuid import uuid4
from typing import Callable

@post.route("/update", methods=["PUT"])
def update_one_post():
    """Update a post."""
    if request.method == 'GET':
        return {'success': 'post creation form'}, HTTPStatus.OK
    elif request.method == 'PUT':
        try:
            post_data = UpdatePost(
                **request.form
            )
        except ValidationError:
            return {'Error': 'The data provided is invalid or incomplete!'}, HTTPStatus.BAD_REQUEST
        try:
            user_data = GetUser(user_id=post_data.author_id)
            user: User = get_user(session=get_db, user_data=user_data)
            if not user:
                return {'Error': f'User with id {user_data.user_id} does not exists'}, HTTPStatus.NOT_FOUND 
            post: Post = get_post(session=get_db, post_data=post_data)
            if not post:
                return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
            if user.id != post.author_id:
                return {'Error': 'You can only update your own post!'}, HTTPStatus.FORBIDDEN
            post: Post = update_post(post_data=post_data, post_image=request.files, session=get_db) 
        except (OperationalError, IntegrityError) as e:
            print(e)
            # Send email to
            return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
        resp = CreatedPost(
            id=post.id,
            location=post.location,
            text=post.text,
            image_url=post.image_url,
            author_id=post.author_id,
            date_published=post.date_published
        )
        return resp.model_dump_json(indent=4), HTTPStatus.CREATED

import torch
from tqdm import tqdm
from torch.nn import CrossEntropyLoss
from torch import optim
from torchvision.transforms import Compose, Resize, RandomCrop, ToTensor, Normalize
from torch.utils.tensorboard import SummaryWriter
from utils import save_checkpoint, load_checkpoint, print_examples
from create_dataset import get_loader
from model import CNNToRNN


def train():
    transforms = Compose(
        [
            Resize((356, 356)),
            RandomCrop((299, 299)),
            ToTensor(),
            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ]
    )

    train_loader, dataset = get_loader(
        images_dir="raw-data/Images",
        captions_file="raw-data/captions.txt",
        transforms=transforms,
        num_workers=2,
    )

    torch.backends.cudnn.benchmark = True
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    load_model = False
    save_model = False
    train_CNN = False

    # Hyperparameters
    embed_size = 256
    hidden_size = 256
    vocab_size = len(dataset.vocabulary)
    num_layers = 1
    learning_rate = 3e-4



    # Message Dialogs. #
    def hasLabels(self):
        if self.noShapes():
            self.errorMessage(
                "No objects labeled",
                "You must label at least one object to save the file.",
            )
            return False
        return True

    def hasLabelFile(self):
        if self.filename is None:
            return False

        label_file = self.getLabelFile()
        return osp.exists(label_file)

    def mayContinue(self):
        if not self.dirty:
            return True
        mb = QtWidgets.QMessageBox
        msg = self.tr('Save annotations to "{}" before closing?').format(self.filename)
        answer = mb.question(
            self,
            self.tr("Save annotations?"),
            msg,
            mb.Save | mb.Discard | mb.Cancel,
            mb.Save,
        )
        if answer == mb.Discard:
            return True
        elif answer == mb.Save:
            self.saveFile()
            return True
        else:  # answer == mb.Cancel
            return False

    def errorMessage(self, title, message):
        return QtWidgets.QMessageBox.critical(


        self.labelPreview.setHidden(True)

        box = QtWidgets.QVBoxLayout()
        box.addWidget(self.labelPreview)
        box.addStretch()

        self.setFixedSize(self.width() + 300, self.height())
        self.layout().addLayout(box, 1, 3, 1, 1)
        self.currentChanged.connect(self.onChange)

    def onChange(self, path):
        if path.lower().endswith(".json"):
            with open(path, "r") as f:
                data = json.load(f)
                self.labelPreview.setText(json.dumps(data, indent=4, sort_keys=False))
            self.labelPreview.label.setAlignment(
                QtCore.Qt.AlignLeft | QtCore.Qt.AlignTop
            )
            self.labelPreview.setHidden(False)
        else:
            pixmap = QtGui.QPixmap(path)
            if pixmap.isNull():
                self.labelPreview.clear()
                self.labelPreview.setHidden(True)
            else:
                self.labelPreview.setPixmap(
                    pixmap.scaled(
                        self.labelPreview.width() - 30,
                        self.labelPreview.height() - 30,
                        QtCore.Qt.KeepAspectRatio,
                        QtCore.Qt.SmoothTransformation,
                    )
                )
                self.labelPreview.label.setAlignment(QtCore.Qt.AlignCenter)
                self.labelPreview.setHidden(False)


"""This module declares the app configuration.

The classes include:

BaseConfig:
    Has all the configurations shared by all the environments.

"""
import os

from dotenv import load_dotenv

load_dotenv()


class BaseConfig:
    """Base configuration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ["SECRET_KEY"]
    db_conn_string = 'sqlite:///./butterfly.db'
    SQLALCHEMY_DATABASE_URI = db_conn_string
    SQLALCHEMY_TRACK_MODIFICATIONS = False

from helpers import redis


def analyzed_quotes():
    sub = redis.pubsub()
    sub.subscribe('analyzed_quotes')
    for message in sub.listen():
        if message and isinstance(message['data'], str):
            quote: dict = message['data']
            print(quote)
            
if __name__ == '__main__':
    while True:
        analyzed_quotes()

def get_posts(session: Session, post_data: GetPosts):
    with session() as db:
        posts: list[Post] = db.query(Post).offset(post_data.offset).limit(post_data.limit).all()
        for post in posts:
            post.author
        return posts

def delete_post(session: Session, post_data: GetPost):
    with session() as db:
        post = db.query(Post).filter(Post.id == post_data.post_id).first()
        db.delete(post)
        db.commit()
        
    return post

        else:
            raise ValueError("Unsupported completion: {}".format(completion))
        completer.setModel(self.labelList.model())
        self.edit.setCompleter(completer)

    def addLabelHistory(self, label):
        if self.labelList.findItems(label, QtCore.Qt.MatchExactly):
            return
        self.labelList.addItem(label)
        if self._sort_labels:
            self.labelList.sortItems()

    def labelSelected(self, item):
        self.edit.setText(item.text())

    def validate(self):
        text = self.edit.text()
        if hasattr(text, "strip"):
            text = text.strip()
        else:
            text = text.trimmed()
        if text:
            self.accept()

    def labelDoubleClicked(self, item):
        self.validate()

    def postProcess(self):
        text = self.edit.text()
        if hasattr(text, "strip"):
            text = text.strip()
        else:
            text = text.trimmed()
        self.edit.setText(text)

    def updateFlags(self, label_new):
        # keep state of shared flags
        flags_old = self.getFlags()

        flags_new = {}


from http import HTTPStatus

from flask import Flask, Response, jsonify, make_response


def handle_resource_not_found(exeption: Exception) -> Response:
    """Handle all resource not found errors.

    Called when an requested resource is not found on the server.

    Parameters
    ----------
    exception: Exception
        The exception that was raised. This is a subclass of Exception.

    Returns
    -------
    Response:
        A string consiting of json data and response code.
    """
    return make_response(jsonify({"error": str(exeption)}), HTTPStatus.NOT_FOUND)

    except HttpError as error:
        print(F'An error occurred: {error}')
        create_message = None
    return create_message

def send_message(gmail_client, message):
    try:
        send_message = (gmail_client.users().messages().send
                        (userId="me", body=message).execute())
        print(F'Message Id: {send_message["id"]}')
    except HttpError as error:
        print(F'An error occurred: {error}')
        send_message = None
    return send_message

def get_gmail_client(secrets_file: str):
    oauth: OAuth = OAuth(secrets_file=secrets_file)
    gmail_client = oauth.authenticate()
    return gmail_client

        self.captions: pd.Series = self.df["caption"]
        
        # Build vocabulary
        self.vocabulary: Vocabulary = Vocabulary(freq_threshold=freq_threshold)
        self.vocabulary.build_vocab(self.captions.tolist())
        
    def __len__(self) -> int:
        return len(self.df)
    
    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:
        caption: str = self.captions[index]
        img_id: str = self.images[index]
        
        image: Image = Image.open(os.path.join(self.images_dir, img_id)).convert("RGB")
        if self.transforms:
            image = self.transforms(image)
            
        numericalized_caption: list[int] = [self.vocabulary.stoi["<SOS>"]]
        numericalized_caption += self.vocabulary.numericalize(caption)
        numericalized_caption.append(self.vocabulary.stoi["<EOS>"])
        
        return image, torch.tensor(numericalized_caption)
            

class MyCollate:
    def __init__(self, pad_idx: int) -> None:
        self.pad_idx: int = pad_idx
        
    def __call__(self, batch: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:
        imgs = [item[0].unsqueeze(dim=0) for item in batch]
        imgs = torch.cat(tensors=imgs, dim=0)
        targets = [item[1] for item in batch]
        targets = pad_sequence(sequences=targets, batch_first=False, padding_value=self.pad_idx)
        
        return imgs, targets
    
    
def get_loader(
    images_dir: str,
    captions_file: str,


You are given the comments by various users on the review of {product}. Use the comments to answer 
the questions that follow. When answering questions, try to list out your answer, with each answer 
on its own separate line. If you do not know the answer, just say that you do not know. DO NOT MAKE 
STUFF UP.
---------
{context}
Question: {question}
Helpful answer: 
"""

product: str = "iphone 15 pro max"
template = PromptTemplate.from_template(template_str)
template = template.partial(product=product)

vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embeddings
)

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=api_key),
    chain_type="stuff",
    return_source_documents=True,
    retriever=vectordb.as_retriever(search_kwargs={"k": 10}),
    chain_type_kwargs={"prompt": template}
)

while True:
    query = input("User: ")
    res = qa_chain.invoke(query)
    print(res["result"])

    chat = OpenAI(temperature=0, api_key=api_key)
    try:
        chain = topic_assign_tmpl | chat | output_parser
        res = chain.invoke(inputs)
    except Exception:
        pass
    else:
        print(res)
        res = [
            {
                "comment_id": c.doc_id,
                "comment": x[c.doc_id],
                "sentiment": c.sentiment,
                "features": c.topics
            }
            for c in res.comment
        ]
        print(res)
        for y in res:
            data.append(y)
        with open('analysis.json', 'w') as f:
            json.dump(data, fp=f, indent=4)
        sleep(5)



if __name__ == "__main__":
    main()


    version=VERSION,
    description=DESCRIPTION,
    long_description_content_type='text/markdown',
    long_description=LONG_DESCRIPTION,
    url='https://youtube-wrapper.readthedocs.io/en/latest/index.html',
    author='Lyle Okoth',
    author_email='lyceokoth@gmail.com',
    license='MIT',
    install_requires=install_requires,
    keywords=key_words,
    classifiers=[
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.11',
        'Programming Language :: Python :: 3.12',
        'Operating System :: OS Independent'
    ],
)



# @app.before_request
    # def rate_limit_request():
    #     if request_is_rate_limited(r, 'admin', 10, timedelta(seconds=60)):
    #         return {'Error': 'You have exceeded the allowed requests'}, HTTPStatus.TOO_MANY_REQUESTS



        for i in range(self.uniqLabelList.count()):
            label_i = self.uniqLabelList.item(i).data(Qt.UserRole)
            if self._config["validate_label"] in ["exact"]:
                if label_i == label:
                    return True
        return False

    def editLabel(self, item=None):
        if item and not isinstance(item, LabelListWidgetItem):
            raise TypeError("item must be LabelListWidgetItem type")

        if not self.canvas.editing():
            return
        if not item:
            item = self.currentItem()
        if item is None:
            return
        shape = item.shape()
        if shape is None:
            return
        text, flags, group_id, description = self.labelDialog.popUp(
            text=shape.label,
            flags=shape.flags,
            group_id=shape.group_id,
            description=shape.description,
        )
        if text is None:
            return
        if not self.validateLabel(text):
            self.errorMessage(
                self.tr("Invalid label"),
                self.tr("Invalid label '{}' with validation type '{}'").format(
                    text, self._config["validate_label"]
                ),
            )
            return
        shape.label = text
        shape.flags = flags
        shape.group_id = group_id


#    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
#    "Accept-Language": "en",
#}

# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    "slidesmodel.middlewares.SlidesmodelSpiderMiddleware": 543,
#}

# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    "slidesmodel.middlewares.SlidesmodelDownloaderMiddleware": 543,
#}

# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    "scrapy.extensions.telnet.TelnetConsole": None,
#}

# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
   "slidesmodel.pipelines.SlidesmodelPipeline": 300,
   "slidesmodel.pipelines.MyImagesPipeline": 1,
   "slidesmodel.pipelines.SaveSlidesPipeline": 200,
   "slidesmodel.pipelines.DuplicatesPipeline": 100,
}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server


def create_logger(env="development"):
    app_logger = create_dev_logger()
    if env == "production":
        app_logger = create_prod_logger()
    return app_logger


app_logger = create_logger()
app_logger.addHandler(logstash.TCPLogstashHandler(host='localhost', port=5959, version=1))

        class_name_to_id[class_name] = class_id
        data["categories"].append(
            dict(
                supercategory=None,
                id=class_id,
                name=class_name,
            )
        )

    out_ann_file = osp.join(args.output_dir, "annotations.json")
    label_files = glob.glob(osp.join(args.input_dir, "*.json"))
    for image_id, filename in enumerate(label_files):
        print("Generating dataset from:", filename)

        label_file = labelme.LabelFile(filename=filename)

        base = osp.splitext(osp.basename(filename))[0]
        out_img_file = osp.join(args.output_dir, "JPEGImages", base + ".jpg")

        img = labelme.utils.img_data_to_arr(label_file.imageData)
        imgviz.io.imsave(out_img_file, img)
        data["images"].append(
            dict(
                license=0,
                url=None,
                file_name=osp.relpath(out_img_file, osp.dirname(out_ann_file)),
                height=img.shape[0],
                width=img.shape[1],
                date_captured=None,
                id=image_id,
            )
        )

        masks = {}  # for area
        segmentations = collections.defaultdict(list)  # for segmentation
        for shape in label_file.shapes:
            points = shape["points"]
            label = shape["label"]
            group_id = shape.get("group_id")
            shape_type = shape.get("shape_type", "polygon")


            continue
        else:
            save_processed_file(
                file_path=module_path, processed_module_code=new_module_code
            )
            format_file(module_path)
            class_source_queue.task_done()


from __future__ import annotations

import ast
from abc import ABC, abstractmethod
from ast import AST, FunctionDef

from .helpers import parse_src


class Parser(ABC):
    @abstractmethod
    def set_next(self, parser: Parser) -> Parser:
        pass

    @abstractmethod
    def parse(self, docstring: str) -> str:
        pass


class AbstractParser(Parser):
    _next_parser: Parser = None

    def set_next(self, parser: Parser) -> Parser:
        self._next_parser = parser
        return parser

    @abstractmethod
    def parse(self, docstring: str) -> str:
        if self._next_parser:
            return self._next_parser.parse(docstring)
        print("Unable to parse the docstring generated.")
        print("################################")
        print(docstring)
        print("################################")
        return None


class DefaultParser(AbstractParser):
    def parse(self, docstring: str) -> str:
        try:


from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import OpenAIWhisperParser
from langchain_community.document_loaders.blob_loaders.youtube_audio import (
    YoutubeAudioLoader,
)
from os import path

# Two Karpathy lecture videos
urls = ["https://www.youtube.com/watch?v=cBpGq-vDr2Y"]

# Directory to save audio files
data_dir = "data"
video_data_dir = "data"
transcribed_data = "transcriptions"
video_title = "iphone_15_marques_review"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")

api_key: str = "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"

loader = GenericLoader(
    YoutubeAudioLoader(urls, save_video_dir), OpenAIWhisperParser(api_key=api_key)
)
docs = loader.load()

full_transcript = ""
for doc in docs:
    full_transcript += doc.page_content

with open(save_transcript_dir, "w", encoding="utf-8") as f:
    f.write(full_transcript)

print(full_transcript)


from dotenv import load_dotenv
load_dotenv()
# from assistant.utils.channel_utils import get_channel_latest_video, get_favorite_channels_latest_videos
# from assistant.utils.playlist_utils import add_video_to_youtube_playlist
from assistant.agent import get_agent_executor
# from assistant.tools.playlist.helpers import list_playlist_videos
# from assistant.tools.comment.helpers import list_video_comments
from assistant.agent import get_tools
from assistant.tools.comment.helpers import (
    list_video_comments, find_my_comments, find_author_comments, list_comment_replies
)
from assistant.tools.channel.helpers import find_my_youtube_username

title: str = 'Real Engineering'
# print(get_favorite_channels_latest_videos())
# title: str = 'How Nebula Works from Real Engineering'
# playlist: str = 'Daily Videos'
# add_video_to_youtube_playlist(title, playlist)
# query = 'When was the youtube channel Ark Invest created?'
# print(agent_executor.invoke({"input": query})['output'])
# print(list_playlist_videos(title, title))
#PLx7ERghZ6LoOKkmL4oeLoqWousfkKpdM_
# print(list_video_comments('How Nebula Works', max_results=10))
# query = 'When was my youtube channel created?'
# agent_executor = get_agent_executor()
# print(agent_executor.invoke({"input": query})['output'])
# tools = get_tools(query)
# print(len(tools))
# t = [tool.description for tool in tools]
# print(t)
# print(find_author_comments('Trapping Rain Water - Google Interview Question - Leetcode 42', '@NeetCode'))

query = "List all the replies to the comments by neetcode on the video titled 'Trapping Rain Water - Google Interview Question - Leetcode 42'"
agent_executor = get_agent_executor()
print(agent_executor.invoke({"input": query})['output'])
# print(list_comment_replies('neetcode', 'Trapping Rain Water - Google Interview Question - Leetcode 42'))

        if fit_to_content is None:
            fit_to_content = {"row": False, "column": True}
        self._fit_to_content = fit_to_content

        super(LabelDialog, self).__init__(parent)
        self.edit = LabelQLineEdit()
        self.edit.setPlaceholderText(text)
        self.edit.setValidator(labelme.utils.labelValidator())
        self.edit.editingFinished.connect(self.postProcess)
        if flags:
            self.edit.textChanged.connect(self.updateFlags)
        self.edit_group_id = QtWidgets.QLineEdit()
        self.edit_group_id.setPlaceholderText("Group ID")
        self.edit_group_id.setValidator(
            QtGui.QRegExpValidator(QtCore.QRegExp(r"\d*"), None)
        )
        layout = QtWidgets.QVBoxLayout()
        if show_text_field:
            layout_edit = QtWidgets.QHBoxLayout()
            layout_edit.addWidget(self.edit, 6)
            layout_edit.addWidget(self.edit_group_id, 2)
            layout.addLayout(layout_edit)
        # buttons
        self.buttonBox = bb = QtWidgets.QDialogButtonBox(
            QtWidgets.QDialogButtonBox.Ok | QtWidgets.QDialogButtonBox.Cancel,
            QtCore.Qt.Horizontal,
            self,
        )
        bb.button(bb.Ok).setIcon(labelme.utils.newIcon("done"))
        bb.button(bb.Cancel).setIcon(labelme.utils.newIcon("undo"))
        bb.accepted.connect(self.validate)
        bb.rejected.connect(self.reject)
        layout.addWidget(bb)
        # label_list
        self.labelList = QtWidgets.QListWidget()
        if self._fit_to_content["row"]:
            self.labelList.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        if self._fit_to_content["column"]:
            self.labelList.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        self._sort_labels = sort_labels


            if not self.selectedShapes or (
                self.hShape is not None and self.hShape not in self.selectedShapes
            ):
                self.selectShapePoint(pos, multiple_selection_mode=group_mode)
                self.repaint()
            self.prevPoint = pos

    def mouseReleaseEvent(self, ev):
        if ev.button() == QtCore.Qt.RightButton:
            menu = self.menus[len(self.selectedShapesCopy) > 0]
            self.restoreCursor()
            if not menu.exec_(self.mapToGlobal(ev.pos())) and self.selectedShapesCopy:
                # Cancel the move by deleting the shadow copy.
                self.selectedShapesCopy = []
                self.repaint()
        elif ev.button() == QtCore.Qt.LeftButton:
            if self.editing():
                if (
                    self.hShape is not None
                    and self.hShapeIsSelected
                    and not self.movingShape
                ):
                    self.selectionChanged.emit(
                        [x for x in self.selectedShapes if x != self.hShape]
                    )

        if self.movingShape and self.hShape:
            index = self.shapes.index(self.hShape)
            if self.shapesBackups[-1][index].points != self.shapes[index].points:
                self.storeShapes()
                self.shapeMoved.emit()

            self.movingShape = False

    def endMove(self, copy):
        assert self.selectedShapes and self.selectedShapesCopy
        assert len(self.selectedShapesCopy) == len(self.selectedShapes)
        if copy:
            for i, shape in enumerate(self.selectedShapesCopy):
                self.shapes.append(shape)


      You an agent who converts detailed travel plans into a simple list of locations.

      The itinerary will be denoted by four hashtags. Convert it into
      list of places that they should visit. Try to include the specific address of each location.

      Your output should always contain the start and end point of the trip, and may also include a list
      of waypoints. It should also include a mode of transit. The number of waypoints cannot exceed 20.
      If you can't infer the mode of transit, make a best guess given the trip location.

      For example:

      ####
      Itinerary for a 2-day driving trip within London:
      - Day 1:
        - Start at Buckingham Palace (The Mall, London SW1A 1AA)
        - Visit the Tower of London (Tower Hill, London EC3N 4AB)
        - Explore the British Museum (Great Russell St, Bloomsbury, London WC1B 3DG)
        - Enjoy shopping at Oxford Street (Oxford St, London W1C 1JN)
        - End the day at Covent Garden (Covent Garden, London WC2E 8RF)
      - Day 2:
        - Start at Westminster Abbey (20 Deans Yd, Westminster, London SW1P 3PA)
        - Visit the Churchill War Rooms (Clive Steps, King Charles St, London SW1A 2AQ)
        - Explore the Natural History Museum (Cromwell Rd, Kensington, London SW7 5BD)
        - End the trip at the Tower Bridge (Tower Bridge Rd, London SE1 2UP)
      #####

      Output:
      Start: Buckingham Palace, The Mall, London SW1A 1AA
      End: Tower Bridge, Tower Bridge Rd, London SE1 2UP
      Waypoints: ["Tower of London, Tower Hill, London EC3N 4AB", "British Museum, Great Russell St, Bloomsbury, London WC1B 3DG", "Oxford St, London W1C 1JN", "Covent Garden, London WC2E 8RF","Westminster, London SW1A 0AA", "St. James's Park, London", "Natural History Museum, Cromwell Rd, Kensington, London SW7 5BD"]
      Transit: driving

      Transit can be only one of the following options: "driving", "train", "bus" or "flight".

      {format_instructions}
    """

        self.human_template = """
      ####{agent_suggestion}####
    """


from itemadapter import ItemAdapter
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem
from os import path, mkdir
from scrapy.http import Response
from scrapy import Request, Spider
from scrapy import Item
from pathlib import PurePosixPath
from urllib.parse import urlparse
from slidesmodel.models import db_connect, Tag, Category, Slide, create_table, create_engine
from sqlalchemy.orm import sessionmaker
import uuid
import logging


class SlidesmodelPipeline:
    def process_item(self, item: Item, spider: Spider):
        return item
    
class MyImagesPipeline(ImagesPipeline):
    def file_path(self, request: Request, response: Response = None, info=None, *, item=None):
        slide_name: str = request.meta['title']
        return f"{slide_name}/" + PurePosixPath(urlparse(request.url).path).name
    
    def get_media_requests(self, item: Item, info):
        for image_url in item["image_urls"]:
            yield Request(image_url, meta={"title": item["title"]})
            

class SaveSlidesPipeline(object):
    def __init__(self):
        """
        Initializes database connection and sessionmaker
        Creates tables
        """
        engine = db_connect()
        create_table(engine)
        self.Session = sessionmaker(bind=engine)




@post.route("/load_more_comments", methods=["GET"])
def load_more_comments():
    """Get a single post."""
    offset: str = request.args.get('offset', 0)
    limit: str = request.args.get('limit', 10)
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post: Post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        comments: list[Comment] = list_post_comments(session=get_db, post_data=post_data, offset=offset, limit=limit)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    post_comments = []
    for comment in comments:
        comment_author: PostAuthor = PostAuthor(
            id=comment.author.id,
            profile_picture=url_for('static', filename=f'img/{comment.author.profile_picture_url}'),
            name=comment.author.first_name
        )
        comment_schema: CommentSchema = CommentSchema(
            author=comment_author,
            text=comment.comment_text
        )
        post_comments.append(comment_schema.model_dump())
    return post_comments

#!/usr/bin/env python

from __future__ import print_function

import argparse
import glob
import os
import os.path as osp
import sys

import imgviz

import labelme

try:
    import lxml.builder
    import lxml.etree
except ImportError:
    print("Please install lxml:\n\n    pip install lxml\n")
    sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("input_dir", help="input annotated directory")
    parser.add_argument("output_dir", help="output dataset directory")
    parser.add_argument("--labels", help="labels file", required=True)
    parser.add_argument("--noviz", help="no visualization", action="store_true")
    args = parser.parse_args()

    if osp.exists(args.output_dir):
        print("Output directory already exists:", args.output_dir)
        sys.exit(1)
    os.makedirs(args.output_dir)
    os.makedirs(osp.join(args.output_dir, "JPEGImages"))
    os.makedirs(osp.join(args.output_dir, "Annotations"))
    if not args.noviz:
        os.makedirs(osp.join(args.output_dir, "AnnotationsVisualization"))


from langchain.agents import AgentType, initialize_agent
from langchain.chat_models import ChatOpenAI
from langchain.tools import BaseTool, StructuredTool, Tool, tool
from dotenv import load_dotenv
from pydantic.v1 import BaseModel, Field
from typing import Optional, Type
from pydantic_settings import BaseSettings
from langchain.callbacks.manager import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from youtube import YouTube
from youtube.models import Search
from youtube.resources.schemas import (
    CreatePlaylistSchema, CreatePlaylistSnippet, CreateStatus, CreatePlaylistItem, CreatePlaylistItemSnippet,
    VideoResourceId, YouTubeRequest, SearchPart, SearchOptionalParameters, SearchFilter, 
    YouTubeResponse
)


load_dotenv()

class Config(BaseSettings):
    open_ai_token: str

config: Config = Config()
client_secret_file: str = 'client_secret.json'
youtube: YouTube = YouTube(client_secret_file=client_secret_file)
youtube.authenticate()

class YouTubeChannelTitleSearch(BaseModel):
    title: str


class YouTubeChannelTitleSearchTool(BaseTool):
    name = "youtube_title_channel_search"
    description = """
    useful when you need to find information about a channel when provided with the title. 
    To use this tool you must provide the channel title.
    """


from scrapy import Item, Field
from itemloaders.processors import TakeFirst, MapCompose, Join
import re


def remove_html_tags(description: str) -> str:
    html_pattern = "<(?:\"[^\"]*\"['\"]*|'[^']*'['\"]*|[^'\">])+>" 
    return re.sub(html_pattern, '', description)

def remove_unicode_chars(text: str) -> str:
    return text.replace(u"\xa0", "")

def num_of_slides(text: str) -> int:
    vals = [val for val in list(text) if val.isdigit()]
    return "".join(vals)


class SlidesModelItem(Item):
    title = Field(output_processor=TakeFirst())
    category = Field(output_processor=TakeFirst())
    description = Field(
        input_processor=MapCompose(remove_html_tags, remove_unicode_chars),
        output_processor=Join()
    )
    tags = Field()
    slides_count = Field(
        input_processor=MapCompose(num_of_slides),
        output_processor=TakeFirst()
    )
    colors = Field()
    image_urls = Field()
    images = Field()


def create_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = Like(
            author_id=activity.user_id,
            post_id=activity.post_id
        )
        db.add(like)
        db.commit()
        db.refresh(like)
    return like

    return timestamps

def partition_video_segments(video_id: str) -> TimeStamps:
    video_segments: TimeStamps = get_timestamps(video_id=video_id)
    return video_segments
        
def get_description(video_id: str) -> str:
    video_path: str = path.join(descriptions_dir, f"{video_id}.txt")
    if not path.exists(video_path):
        description: str = download_description(video_id=video_id)
        save_description(description=description, video_id=video_id)
    else:
        with open(video_path, "r", encoding="utf-8") as f:
            description: str = f.read()
    return description


def download_description(video_id: str) -> None:
    response: YouTubeListResponse = youtube_client.find_video_by_id(video_id=video_id)
    video: Video = response.items[0]
    description: str = video.snippet.description
    return description
    
    
parser = PydanticOutputParser(pydantic_object=TimeStamps)

segment_str: str = ("""Extract the time stamps and their titles from the following text. Only""" 
                    """ include valid time stamps.\n{format_instructions}\ntext: ```{text}```"""
)

def get_video_segments(video_description: str, segment_str: str = segment_str, 
                       llm: BaseLanguageModel = gemma_2b):
    template: PromptTemplate = PromptTemplate(template=segment_str, 
                    input_variables=["text"],
                    partial_variables={"format_instructions": parser.get_format_instructions()}
                                              )
    chain = template | llm | parser
    inputs: dict[str, str] = {
        "text": video_description
    }


import logging.config
import logstash

from dotenv import load_dotenv

load_dotenv()


def create_dev_logger():
    """Create the application logger."""
    config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
            },
            "json": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
                "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
            },
        },
        "handlers": {
            "standard": {
                "class": "logging.StreamHandler",
                "formatter": "json",
            },
        },
        "loggers": {"": {"handlers": ["standard"], "level": logging.INFO}},
    }

    logging.config.dictConfig(config)

    logger = logging.getLogger(__name__)

    return logger

import collections
import threading

import imgviz
import numpy as np
import onnxruntime
import skimage

from ..logger import logger
from . import _utils


class EfficientSam:
    def __init__(self, encoder_path, decoder_path):
        self._encoder_session = onnxruntime.InferenceSession(encoder_path)
        self._decoder_session = onnxruntime.InferenceSession(decoder_path)

        self._lock = threading.Lock()
        self._image_embedding_cache = collections.OrderedDict()

        self._thread = None

    def set_image(self, image: np.ndarray):
        with self._lock:
            self._image = image
            self._image_embedding = self._image_embedding_cache.get(
                self._image.tobytes()
            )

        if self._image_embedding is None:
            self._thread = threading.Thread(
                target=self._compute_and_cache_image_embedding
            )
            self._thread.start()

    def _compute_and_cache_image_embedding(self):
        with self._lock:
            logger.debug("Computing image embedding...")
            image = imgviz.rgba2rgb(self._image)
            batched_images = image.transpose(2, 0, 1)[None].astype(np.float32) / 255.0


#!/usr/bin/env python

import argparse
import sys

import imgviz
import matplotlib.pyplot as plt

from labelme import utils
from labelme.label_file import LabelFile

PY2 = sys.version_info[0] == 2


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("json_file")
    args = parser.parse_args()

    label_file = LabelFile(args.json_file)
    img = utils.img_data_to_arr(label_file.imageData)

    label_name_to_value = {"_background_": 0}
    for shape in sorted(label_file.shapes, key=lambda x: x["label"]):
        label_name = shape["label"]
        if label_name in label_name_to_value:
            label_value = label_name_to_value[label_name]
        else:
            label_value = len(label_name_to_value)
            label_name_to_value[label_name] = label_value
    lbl, _ = utils.shapes_to_label(img.shape, label_file.shapes, label_name_to_value)

    label_names = [None] * (max(label_name_to_value.values()) + 1)
    for name, value in label_name_to_value.items():
        label_names[value] = name
    lbl_viz = imgviz.label2rgb(
        lbl,
        imgviz.asgray(img),
        label_names=label_names,
        font_size=30,


            enabled=False,
        )
        createRectangleMode = action(
            self.tr("Create Rectangle"),
            lambda: self.toggleDrawMode(False, createMode="rectangle"),
            shortcuts["create_rectangle"],
            "objects",
            self.tr("Start drawing rectangles"),
            enabled=False,
        )
        createCircleMode = action(
            self.tr("Create Circle"),
            lambda: self.toggleDrawMode(False, createMode="circle"),
            shortcuts["create_circle"],
            "objects",
            self.tr("Start drawing circles"),
            enabled=False,
        )
        createLineMode = action(
            self.tr("Create Line"),
            lambda: self.toggleDrawMode(False, createMode="line"),
            shortcuts["create_line"],
            "objects",
            self.tr("Start drawing lines"),
            enabled=False,
        )
        createPointMode = action(
            self.tr("Create Point"),
            lambda: self.toggleDrawMode(False, createMode="point"),
            shortcuts["create_point"],
            "objects",
            self.tr("Start drawing points"),
            enabled=False,
        )
        createLineStripMode = action(
            self.tr("Create LineStrip"),
            lambda: self.toggleDrawMode(False, createMode="linestrip"),
            shortcuts["create_linestrip"],
            "objects",
            self.tr("Start drawing linestrip. Ctrl+LeftClick ends creation."),


import ast
from _ast import ClassDef, FunctionDef
from ast import NodeTransformer
from typing import Any

from pydantic import BaseModel, Field

from .config import Config
from .helpers import (
    generate_class_docstring,
    generate_function_docstring,
    get_class_docstring,
    get_class_methods_docstrings,
    get_function_docstring,
    get_module_source_code,
    make_docstring_node,
)


class FunctionDocStringWriter(NodeTransformer, BaseModel):
    module_path: str = Field(description='The path to this module')
    function_name: str = Field(
        description='The name of the function to generate docstrings'
    )
    function_code: str = Field(description='The source code for this function')
    config: Config = Field(description='The application configurations.')

    @property
    def module_code(self) -> str:
        return get_module_source_code(self.module_path)

    def visit_FunctionDef(self, node: FunctionDef) -> Any:
        docstring: str = ast.get_docstring(node=node)
        if node.name == self.function_name and (
            self.config.overwrite_function_docstring or not docstring
        ):
            function_code: str = ast.get_source_segment(
                source=self.module_code, node=node, padded=True
            )
            function_and_docstring: str = generate_function_docstring(


def add_user(user: User) -> User:
    with get_db() as session:
        session.add(user)
        session.commit()
        session.refresh(user)
    return user

def add_post(post: Post) -> Post:
    with get_db() as session:
        session.add(post)
        session.commit()
        session.refresh(post)
    return post

def add_likes(likes: list[Like]) -> None:
    with get_db() as session:
        for like in likes:
            session.add(like)
        session.commit()
        
def add_bookmarks(bookmarks: list[Bookmark]) -> None:
    with get_db() as session:
        for bookmark in bookmarks:
            session.add(bookmark)
        session.commit()
    
def add_comments(comments: list[Comment]) -> None:
    with get_db() as session:
        for comment in comments:
            session.add(comment)
        session.commit()

            'youtube.resources.playlist',
            'youtube.resources.playlist_item',
            'youtube.resources.comment_thread',
            'youtube.resources.channel',
            'youtube.resources.activity',
            'youtube.resources.subscription',
            'youtube.exceptions',
        ]
    ),
    version=VERSION,
    description=DESCRIPTION,
    long_description_content_type='text/markdown',
    long_description=LONG_DESCRIPTION,
    url='https://youtube-wrapper.readthedocs.io/en/latest/index.html',
    author='Lyle Okoth',
    author_email='lyceokoth@gmail.com',
    license='MIT',
    install_requires=install_requires,
    keywords=key_words,
    classifiers=[
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.11',
        'Programming Language :: Python :: 3.12',
        'Operating System :: OS Independent'
    ],
)


# MIT License
# Copyright (c) Kentaro Wada

import base64
import io

import numpy as np
import PIL.ExifTags
import PIL.Image
import PIL.ImageOps


def img_data_to_pil(img_data):
    f = io.BytesIO()
    f.write(img_data)
    img_pil = PIL.Image.open(f)
    return img_pil


def img_data_to_arr(img_data):
    img_pil = img_data_to_pil(img_data)
    img_arr = np.array(img_pil)
    return img_arr


def img_b64_to_arr(img_b64):
    img_data = base64.b64decode(img_b64)
    img_arr = img_data_to_arr(img_data)
    return img_arr


def img_pil_to_data(img_pil):
    f = io.BytesIO()
    img_pil.save(f, format="PNG")
    img_data = f.getvalue()
    return img_data


def img_arr_to_b64(img_arr):
    img_data = img_arr_to_data(img_arr)


        self.addDockWidget(Qt.RightDockWidgetArea, self.label_dock)
        self.addDockWidget(Qt.RightDockWidgetArea, self.shape_dock)
        self.addDockWidget(Qt.RightDockWidgetArea, self.file_dock)

        # Actions
        action = functools.partial(utils.newAction, self)
        shortcuts = self._config["shortcuts"]
        quit = action(
            self.tr("&Quit"),
            self.close,
            shortcuts["quit"],
            "quit",
            self.tr("Quit application"),
        )
        open_ = action(
            self.tr("&Open\n"),
            self.openFile,
            shortcuts["open"],
            "open",
            self.tr("Open image or label file"),
        )
        opendir = action(
            self.tr("Open Dir"),
            self.openDirDialog,
            shortcuts["open_dir"],
            "open",
            self.tr("Open Dir"),
        )
        openNextImg = action(
            self.tr("&Next Image"),
            self.openNextImg,
            shortcuts["open_next"],
            "next",
            self.tr("Open next (hold Ctl+Shift to copy labels)"),
            enabled=False,
        )
        openPrevImg = action(
            self.tr("&Prev Image"),
            self.openPrevImg,
            shortcuts["open_prev"],


from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import Field, BaseModel
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI
import re
import json
from langchain.docstore.document import Document
from random import choices
from langchain.base_language import BaseLanguageModel

file_path: str = "comments.json"
api_key: str = "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"


def lower(text: str) -> str:
    return text.lower().strip()


def remove_urls(text: str) -> str:
    url_pattern = r"https?://\S+|www\.\S+"
    text = re.sub(url_pattern, "", text)
    return text


def remove_punctuations(text: str) -> str:
    punctuation_pattern = r"[^\w\s]"
    cleaned = re.sub(punctuation_pattern, "", text)
    return cleaned


def clean_text(text: str) -> str:
    text = lower(text)
    text = remove_urls(text)
    text = remove_punctuations(text)
    return text


def is_acceptable_len(text: str, l=15) -> bool:
    return len(text.split()) >= l



from datetime import datetime
from sqlalchemy.orm import Mapped, mapped_column, relationship
from ..database import Base
from sqlalchemy import ForeignKey


class Comment(Base):
    __tablename__ = 'comments'
    
    id: Mapped[str] = mapped_column(primary_key=True)
    author_id: Mapped[str] = mapped_column(ForeignKey('users.id'))
    post_id: Mapped[str] = mapped_column(ForeignKey('posts.id'))
    comment_text: Mapped[str]
    comment_date: Mapped[datetime] = mapped_column(default_factory=datetime.utcnow)
    
    author = relationship('User', back_populates='comments')
    post = relationship('Post', back_populates='comments')

#         outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))
#         # outputs shape: (1, N, hidden_size)

#         predictions = self.fc(outputs)

#         # predictions shape: (1, N, length_target_vocabulary) to send it to
#         # loss function we want it to be (N, length_target_vocabulary) so we're
#         # just gonna remove the first dim
#         predictions = predictions.squeeze(0)

#         return predictions, hidden, cell


# class Seq2Seq(nn.Module):
#     def __init__(self, encoder, decoder):
#         super(Seq2Seq, self).__init__()
#         self.encoder = encoder
#         self.decoder = decoder

#     def forward(self, source, target, teacher_force_ratio=0.5):
#         batch_size = source.shape[1]
#         target_len = target.shape[0]
#         target_vocab_size = len(english.vocab)

#         outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)

#         hidden, cell = self.encoder(source)

#         # Grab the first input to the Decoder which will be <SOS> token
#         x = target[0]

#         for t in range(1, target_len):
#             # Use previous hidden, cell as context from encoder at start
#             output, hidden, cell = self.decoder(x, hidden, cell)

#             # Store next output prediction
#             outputs[t] = output

#             # Get the best word the Decoder predicted (index in the vocabulary)
#             best_guess = output.argmax(1)


import logging.config
import logstash

from dotenv import load_dotenv

load_dotenv()


def create_dev_logger():
    """Create the application logger."""
    config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
            },
            "json": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
                "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
            },
        },
        "handlers": {
            "standard": {
                "class": "logging.StreamHandler",
                "formatter": "json",
            },
        },
        "loggers": {"": {"handlers": ["standard"], "level": logging.INFO}},
    }

    logging.config.dictConfig(config)

    logger = logging.getLogger(__name__)

    return logger

    os.makedirs(osp.join(args.output_dir, "JPEGImages"))
    os.makedirs(osp.join(args.output_dir, "SegmentationClass"))
    if not args.nonpy:
        os.makedirs(osp.join(args.output_dir, "SegmentationClassNpy"))
    if not args.noviz:
        os.makedirs(osp.join(args.output_dir, "SegmentationClassVisualization"))
    if not args.noobject:
        os.makedirs(osp.join(args.output_dir, "SegmentationObject"))
        if not args.nonpy:
            os.makedirs(osp.join(args.output_dir, "SegmentationObjectNpy"))
        if not args.noviz:
            os.makedirs(osp.join(args.output_dir, "SegmentationObjectVisualization"))
    print("Creating dataset:", args.output_dir)

    if osp.exists(args.labels):
        with open(args.labels) as f:
            labels = [label.strip() for label in f if label]
    else:
        labels = [label.strip() for label in args.labels.split(",")]

    class_names = []
    class_name_to_id = {}
    for i, label in enumerate(labels):
        class_id = i - 1  # starts with -1
        class_name = label.strip()
        class_name_to_id[class_name] = class_id
        if class_id == -1:
            assert class_name == "__ignore__"
            continue
        elif class_id == 0:
            assert class_name == "_background_"
        class_names.append(class_name)
    class_names = tuple(class_names)
    print("class_names:", class_names)
    out_class_names_file = osp.join(args.output_dir, "class_names.txt")
    with open(out_class_names_file, "w") as f:
        f.writelines("\n".join(class_names))
    print("Saved class_names:", out_class_names_file)

    for filename in sorted(glob.glob(osp.join(args.input_dir, "*.json"))):


from youtube.models import (
    Playlist, PlaylistItem, Search
)
from api.database.models import Video
from youtube import YouTube
from youtube.resources.schemas import (
    SearchPart, SearchFilter, SearchOptionalParameters
)
from youtube.resources.schemas import (
    YouTubeResponse, CreateStatus, CreatePlaylistSnippet, CreatePlaylistSchema,
    VideoResourceId, CreatePlaylistItemSnippet, CreatePlaylistItem, YouTubeRequest
)
from redis import Redis
import logging
from dotenv import load_dotenv
from config import RedisSettings, Config
from api.database.models import Channel
from api.database.crud import get_all_channels, get_channel_by_title
from api.database.database import get_db
from json import loads, dumps
from datetime import timedelta


load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')

redis_config: RedisSettings = RedisSettings()
redis: Redis = Redis(
    host=redis_config.redis_host,
    port=redis_config.redis_port,
    decode_responses=True
)
config: Config = Config()


def get_youtube_client(client_secret_file: str) -> YouTube:
    logging.info('Creating the YouTube client.')
    youtube: YouTube = YouTube(client_secret_file=client_secret_file)
    logging.info('Authenticating the user.')
    youtube.authenticate()


        print("Generating dataset from:", filename)

        label_file = labelme.LabelFile(filename=filename)

        base = osp.splitext(osp.basename(filename))[0]
        out_img_file = osp.join(args.output_dir, "JPEGImages", base + ".jpg")
        out_clsp_file = osp.join(args.output_dir, "SegmentationClass", base + ".png")
        if not args.nonpy:
            out_cls_file = osp.join(
                args.output_dir, "SegmentationClassNpy", base + ".npy"
            )
        if not args.noviz:
            out_clsv_file = osp.join(
                args.output_dir,
                "SegmentationClassVisualization",
                base + ".jpg",
            )
        if not args.noobject:
            out_insp_file = osp.join(
                args.output_dir, "SegmentationObject", base + ".png"
            )
            if not args.nonpy:
                out_ins_file = osp.join(
                    args.output_dir, "SegmentationObjectNpy", base + ".npy"
                )
            if not args.noviz:
                out_insv_file = osp.join(
                    args.output_dir,
                    "SegmentationObjectVisualization",
                    base + ".jpg",
                )

        img = labelme.utils.img_data_to_arr(label_file.imageData)
        imgviz.io.imsave(out_img_file, img)

        cls, ins = labelme.utils.shapes_to_label(
            img_shape=img.shape,
            shapes=label_file.shapes,
            label_name_to_value=class_name_to_id,
        )


from itemadapter import ItemAdapter
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem
from os import path, mkdir
from scrapy.http import Response
from scrapy import Request, Spider
from scrapy import Item
from pathlib import PurePosixPath
from urllib.parse import urlparse
from slidesmodel.models import db_connect, Tag, Category, Slide, create_table, create_engine
from sqlalchemy.orm import sessionmaker
import uuid
import logging


class SlidesmodelPipeline:
    def process_item(self, item: Item, spider: Spider):
        return item
    
class MyImagesPipeline(ImagesPipeline):
    def file_path(self, request: Request, response: Response = None, info=None, *, item=None):
        slide_name: str = request.meta['title']
        return f"{slide_name}/" + PurePosixPath(urlparse(request.url).path).name
    
    def get_media_requests(self, item: Item, info):
        for image_url in item["image_urls"]:
            yield Request(image_url, meta={"title": item["title"]})
            

class SaveSlidesPipeline(object):
    def __init__(self):
        """
        Initializes database connection and sessionmaker
        Creates tables
        """
        engine = db_connect()
        create_table(engine)
        self.Session = sessionmaker(bind=engine)




from collections.abc import Iterator
from typing import Optional
from youtube import YouTube
from youtube.models import Search, Video
from youtube.schemas import (
    SearchOptionalParameters,
    SearchPart,
    YouTubeListResponse,
    YouTubeRequest,
    YouTubeResponse,
)
from youtube.models import Comment
from youtube.schemas import (
    CommentThreadFilter,
    CommentThreadOptionalParameters,
    CommentThreadPart,
    YouTubeRequest,
)
import json


def get_video_id(video_title: str) -> str:
    """Get video id given the title."""
    part: SearchPart = SearchPart()
    optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
        q=video_title, maxResults=1, type=["video"]
    )
    search_request: YouTubeRequest = YouTubeRequest(
        part=part, optional_parameters=optional_parameters
    )
    search_results: YouTubeResponse = youtube_client.search(search_request)
    search_result: Search = search_results.items[0]
    return search_result.resource_id


def list_video_comments(video_id: str, max_results: Optional[int] = 2500) -> None:
    """List a given videos comments"""
    # video_id: str = get_video_id(video_title)
    part: CommentThreadPart = CommentThreadPart()
    filter: CommentThreadFilter = CommentThreadFilter(videoId=video_id)


from typing import Any
from scrapy import Spider
from scrapy.http import Response
from scrapy.linkextractors import LinkExtractor 


class SlidesLinkExtractor(Spider):
    name: str = "links-extractor"
    
    start_urls: list[str] = [
        "https://slidesgo.com/"
    ]
    
    def __init__(self, name=None, **kwargs): 
        super().__init__(name, **kwargs) 
  
        self.link_extractor = LinkExtractor(unique=True) 
  
    def parse(self, response: Response, **kwargs: Any) -> Any: 
        self.logger.info("Links spider")
        links = response.css('li.w-1\/2 a::attr(href)') 
  
        for link in links: 
            yield {
                    "url": link.get(), 
                }

#         eventType='live',
#         type=['video']
#     )
#     search = YouTubeRequest(part=part, optional_parameters=optional_parameters)
#     results: list[Search] = youtube.search(search).items
#     results: dict[str, Any] = [result.model_dump() for result in results]
#     with open('live-news.json', 'w', encoding='utf-8') as f:
#         dump(results, f, indent=4, default=str)
#     print(results)
    
# search_save()
# part: Part = CommentThreadPart()
# filter: Filter = CommentThreadFilter(videoId='crYum29M-VE')
# optional: OptionalParameters = CommentThreadOptionalParameters(
#     maxResults=10
# )
# req: YouTubeRequest = YouTubeRequest(
#     part=part,
#     optional_parameters=optional,
#     filter=filter
# )
# res: YouTubeResponse = youtube.find_video_comments(request=req)
# print(res)
print(youtube.list_activities())

        QtCore.QLocale.system().name(),
        osp.dirname(osp.abspath(__file__)) + "/translate",
    )
    app = QtWidgets.QApplication(sys.argv)
    app.setApplicationName(__appname__)
    app.setWindowIcon(newIcon("icon"))
    app.installTranslator(translator)
    win = MainWindow(
        config=config,
        filename=filename,
        output_file=output_file,
        output_dir=output_dir,
    )

    if reset_config:
        logger.info("Resetting Qt config: %s" % win.settings.fileName())
        win.settings.clear()
        sys.exit(0)

    win.show()
    win.raise_()
    sys.exit(app.exec_())


# this main block is required to generate executable by pyinstaller
if __name__ == "__main__":
    main()


from dotenv import load_dotenv
load_dotenv()
from flask.cli import FlaskGroup
from api import create_app

app = create_app()
cli = FlaskGroup(create_app=create_app)



if __name__ == "__main__":
    cli()

print(parse_events(events.items))

        # Since loading the file may take some time,
        # make sure it runs in the background.
        if self.filename is not None:
            self.queueEvent(functools.partial(self.loadFile, self.filename))

        # Callbacks:
        self.zoomWidget.valueChanged.connect(self.paintCanvas)

        self.populateModeActions()

        # self.firstStart = True
        # if self.firstStart:
        #    QWhatsThis.enterWhatsThisMode()

    def menu(self, title, actions=None):
        menu = self.menuBar().addMenu(title)
        if actions:
            utils.addActions(menu, actions)
        return menu

    def toolbar(self, title, actions=None):
        toolbar = ToolBar(title)
        toolbar.setObjectName("%sToolBar" % title)
        # toolbar.setOrientation(Qt.Vertical)
        toolbar.setToolButtonStyle(Qt.ToolButtonTextUnderIcon)
        if actions:
            utils.addActions(toolbar, actions)
        self.addToolBar(Qt.TopToolBarArea, toolbar)
        return toolbar

    # Support Functions

    def noShapes(self):
        return not len(self.labelList)

    def populateModeActions(self):
        tool, menu = self.actions.tool, self.actions.menu
        self.tools.clear()
        utils.addActions(self.tools, tool)
        self.canvas.menus[0].clear()


class LoggedInUser(BaseModel):
    email_address: str
    access_token: str
    refresh_token: str
    
class RequestPasswordReset(BaseModel):
    user_id: str
    email_address: str
    
class RequestPasswordResetToken(RequestPasswordReset):
    password_reset_token: str
    
class PasswordReset(BaseModel):
    email_address: str
    password_reset_token: str
    password: str
    confirm_password: str

from sklearn.ensemble import RandomForestClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.ensemble import AdaBoostClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from schemas import Model, UntrainedModel
from os import path
from config.config import app_config
from datetime import datetime


names = [
    "Nearest Neighbors",
    "Linear SVM",
    "RBF SVM",
    "Gaussian Process",
    "Decision Tree",
    "Random Forest",
    "Neural Net",
    "AdaBoost",
    "Naive Bayes",
    "QDA",
]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel="linear", C=0.025, random_state=42),
    SVC(gamma=2, C=1, random_state=42),
    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),
    DecisionTreeClassifier(random_state=42),
    RandomForestClassifier(
        max_depth=5, n_estimators=10, max_features=1, random_state=42
    ),
    MLPClassifier(alpha=1, max_iter=1000, random_state=42),
    AdaBoostClassifier(random_state=42),


import json

from qtpy import QtCore
from qtpy import QtGui
from qtpy import QtWidgets


class ScrollAreaPreview(QtWidgets.QScrollArea):
    def __init__(self, *args, **kwargs):
        super(ScrollAreaPreview, self).__init__(*args, **kwargs)

        self.setWidgetResizable(True)

        content = QtWidgets.QWidget(self)
        self.setWidget(content)

        lay = QtWidgets.QVBoxLayout(content)

        self.label = QtWidgets.QLabel(content)
        self.label.setWordWrap(True)

        lay.addWidget(self.label)

    def setText(self, text):
        self.label.setText(text)

    def setPixmap(self, pixmap):
        self.label.setPixmap(pixmap)

    def clear(self):
        self.label.clear()


class FileDialogPreview(QtWidgets.QFileDialog):
    def __init__(self, *args, **kwargs):
        super(FileDialogPreview, self).__init__(*args, **kwargs)
        self.setOption(self.DontUseNativeDialog, True)

        self.labelPreview = ScrollAreaPreview(self)
        self.labelPreview.setFixedSize(300, 300)


from api import create_app
from api.helpers import generate_data, post_data
import os


app = create_app()

def seed_data():
    data = generate_data(count=1)
    url: str = os.environ.get('url', 'http://127.0.0.1:8000/predict')
    post_data(data=data, url=url)
    
if __name__ == '__main__':
    seed_data()

#!/usr/bin/env python

from __future__ import print_function

import argparse
import glob
import os
import os.path as osp
import sys

import imgviz
import numpy as np

import labelme


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("input_dir", help="Input annotated directory")
    parser.add_argument("output_dir", help="Output dataset directory")
    parser.add_argument(
        "--labels", help="Labels file or comma separated text", required=True
    )
    parser.add_argument(
        "--noobject", help="Flag not to generate object label", action="store_true"
    )
    parser.add_argument(
        "--nonpy", help="Flag not to generate .npy files", action="store_true"
    )
    parser.add_argument(
        "--noviz", help="Flag to disable visualization", action="store_true"
    )
    args = parser.parse_args()

    if osp.exists(args.output_dir):
        print("Output directory already exists:", args.output_dir)
        sys.exit(1)
    os.makedirs(args.output_dir)


    n_classes = 4
    maizenet = MaizeNet(n_classes)
    maizenet.load_state_dict(torch.load(model_path, map_location=torch.device('cpu') ))
    return maizenet

def preprocess_image(image):
    mean = np.array([0.5, 0.5, 0.5])
    std = np.array([0.25, 0.25, 0.25])
    data_transform = transforms.Compose([
            transforms.RandomResizedCrop(224), # resize and crop image to 224 x 224 pixels
            transforms.RandomHorizontalFlip(), # flip the images horizontally
            transforms.ToTensor(), # convert to pytorch tensor data type
            transforms.Normalize(mean, std) # normalize the input image dataset.
        ])
    transformed_image = data_transform(image).to('cpu')
    transformed_image = torch.unsqueeze(transformed_image, 0)
    return transformed_image

def evaluate_image(image, model):
    transformed_image = preprocess_image(image)
    labels = ['Maize Leaf Rust', 'Northern Leaf Blight', 'Healthy', 'Gray Leaf Spot']
    model.eval()
    prediction = F.softmax(model(transformed_image), dim = 1)
    data = {
        'Maize Leaf Rust': round(float(prediction[0][0]), 4) * 100,
        'Northern Leaf Blight': round(float(prediction[0][1]) * 100, 4),
        'Healthy': round(float(prediction[0][2]), 4) * 100,
        'Gray Leaf Spot': round(float(prediction[0][3]) * 100, 4)
    }
    prediction = prediction.argmax()
    return labels[prediction], data



        delete = action(
            self.tr("Delete Polygons"),
            self.deleteSelectedShape,
            shortcuts["delete_polygon"],
            "cancel",
            self.tr("Delete the selected polygons"),
            enabled=False,
        )
        duplicate = action(
            self.tr("Duplicate Polygons"),
            self.duplicateSelectedShape,
            shortcuts["duplicate_polygon"],
            "copy",
            self.tr("Create a duplicate of the selected polygons"),
            enabled=False,
        )
        copy = action(
            self.tr("Copy Polygons"),
            self.copySelectedShape,
            shortcuts["copy_polygon"],
            "copy_clipboard",
            self.tr("Copy selected polygons to clipboard"),
            enabled=False,
        )
        paste = action(
            self.tr("Paste Polygons"),
            self.pasteSelectedShape,
            shortcuts["paste_polygon"],
            "paste",
            self.tr("Paste copied polygons"),
            enabled=False,
        )
        undoLastPoint = action(
            self.tr("Undo last point"),
            self.canvas.undoLastPoint,
            shortcuts["undo_last_point"],
            "undo",
            self.tr("Undo last drawn point"),
            enabled=False,


from .oryks_google_oauth import GoogleSlidesScope, GoogleOAuth, GoogleDirectories
from typing import Optional
from .ml import AnalyzedVideo, analayze_video, create_presentation
from .ml.slide_requests import create_slide

secrets_file: str = "/home/lyle/oryks/backend/api/libraries/slide.json"
scopes: list[str] = [
    GoogleSlidesScope.slides.value,
    GoogleSlidesScope.drive.value
]
api_service_name: str = "slides"
api_version: str = "v1"
credentials_dir: str = GoogleDirectories.slides.value
credentials_file_name: Optional[str] = 'credentials.json'

auth: GoogleOAuth = GoogleOAuth(
    secrets_file=secrets_file,
    scopes=scopes,
    api_service_name=api_service_name,
    api_version=api_version,
    credentials_dir=credentials_dir
)

# slide_client = auth.authenticate_google_server()

# video_transcript: str = ""
# analyzed_video: AnalyzedVideo = analayze_video(video_transcript=video_transcript)
# presentation_name: str = "YouTube Video"
# response: dict = create_presentation(presentation_name=presentation_name, 
#                                      slide_client=slide_client, analyzed_video=analyzed_video)
# presentation_id: str = "1UutpJTI9VOp7u_5iBGCHnKV-YwljkV61HYrrvJMyVAg"
# response: dict = create_slide(presentation_id=presentation_id, slide_client=slide_client)

from .youtube_helper import main

main()


import os
from .config import Config
from flask import Flask


def set_configuration(app: Flask):
    """Set the application configuration.

    The application configuration will depend on the
    environment i.e Test, Development, Staging or Production.

    Parameters
    ----------
    app: flask.Flask
        A flask app instance

    Returns
    -------
    bool:
        Whether the config was set up successfully.
    """
    config_name = os.environ.get("FLASK_ENV")
    app.config.from_object(Config[config_name])

    return True

        train_time=train_results['train_time']
    )
    logging.info('Posting the model metrics.')
    tuned_model.post_model_metrics(app_config, redis)
    return {
        'Model Name': train_results['model_name'],
        'Train Time': train_results['train_time'],
        'Metrics': train_results['metrics']
    }
    

            
    def parse_link(self, response: Response, **kwargs: Any) -> Any:
        # slide_item = response.meta["slide_item"]
        # loader = ItemLoader(item=slide_item, response=response)
        # loader.add_css(field_name="tags", css=".Sm-tags a.mr-2::text")
        # loader.add_css(field_name="description", css=".product-text p")
        # loader.add_css(field_name="slides_count", css='h4 small::text')
        # loader.add_css(field_name="colors", css='li.color a::text')
        # loader.add_css(field_name="image_urls", css='a.preview-link img::attr(src)')
        # add slide link
        # yield loader.load_item()
        categories: list[dict] = []
        cats = response.css('span.cat-links a')
        for cat in cats:
            category = cat.css('::text').get()
            category_link = cat.css('::attr(href)').get()
            categories.append({
                "category": category,
                "link": category_link
            })
        
        yield {
            "categories": categories,
            "title": response.css('h1::text').get(),
            "problem": response.css('.post-content p').getall(),
            "io": response.css('.io').get(),
            "solutions": response.css('h2::text').getall(), 
            "link": response.url,
            "code": response.css('.c-line').getall()
        }

from datetime import datetime
from uuid import uuid4
from redis_om import Migrator
from redis_om.model import NotFoundError
from api.database.models import Video
from youtube.models import Search
from youtube import YouTube
from youtube.resources.schemas import(
    SearchFilter, SearchPart, SearchOptionalParameters, YouTubeRequest, YouTubeResponse
)


client_secret_file: str = 'client_secret.json'
youtube: YouTube = YouTube(client_secret_file=client_secret_file)
youtube.authenticate()

Migrator().run()

# part: SearchPart = SearchPart()
# optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
#     q='Python programming',
#     maxResults=2,
#     type=['video']
# )
# search: YouTubeRequest = YouTubeRequest(part=part, optional_parameters=optional_parameters)
# response: YouTubeResponse = youtube.search(search)
# search_resp: Search = response.items[0]

# video: Video = Video(**search_resp.model_dump(exclude={'thumbnails'}))
# print(video)
videos: list[Video] = Video.find().all()
# for video in videos:
#     video.expire(num_seconds=1)
print(videos)

import torch
import os
from torch import nn
from torchvision import transforms
import numpy as np
import os
from PIL import Image
import torch
import os
from torch import nn
import torch.nn.functional as F
import random


class MaizeNet(nn.Module):
  def __init__(self, K) -> None:
      super(MaizeNet, self).__init__()

      self.conv_layers = nn.Sequential(
          # convolution 1
          nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(32),
          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(32),
          nn.MaxPool2d(2),
          # Convolution 2
          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(64),
          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(64),
          nn.MaxPool2d(2),
          # Convolution 3
          nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(128),
          nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),


        self.tuned_model_ids = []
        for model_path, _ in best_models:
            logging.info(model_path)
            model_name: str = model_path.split('/')[-1]
            for model in models:
                if model_name == model.classifier_name:
                    train_config: TrainConfig = self.create_train_config(model.model, model.classifier_name, model.save_path)
                    res = tune_model.apply_async((train_config,), 
                        link=train_tuned_model.s(train_config,)
                    )
                    self.tuned_model_ids.append(res.id)
                    logging.info('%s queued for tuning and retraining.', model_name)
                        
    def get_tuned_models(self):
        best_model_names = [name.split('/')[-1] for name, _ in self.get_best_models(start=-3, end=-1)]
        while self.tuned_model_ids:
            for index, id in enumerate(self.tuned_model_ids):
                res: AsyncResult = AsyncResult(id)
                if res.ready():
                    logging.info('Tuned Model result for %s is ready.', res.result['name'])
                    logging.info(res.result)
                    self.tuned_model_ids.pop(index)
                    best_model_names.remove(res.result['name'])
                names = ', '.join(best_model_names)
                if names:
                    logging.info('Tuned Model result for %s are not ready.', names)
                sleep(3)
                
    def create_train_config(self, model: BaseEstimator, name: str, save_path: str) -> TrainConfig:
        (train_features, train_labels), (test_features, test_labels) = self.get_train_test_data()
        train_config: TrainConfig = TrainConfig(
            preprocessor=self.preprocessor,
            model=model,
            classifier_name=name,
            save_path=save_path,
            train_features=train_features,
            train_labels=train_labels,
            test_features=test_features,
            test_labels=test_labels
        )


from dotenv import load_dotenv
load_dotenv()
import chainlit as cl

from calendar_assistant.usecases.agent import agent_executor


@cl.on_message
async def main(message: cl.Message):
    msg = cl.Message(content="")
    await msg.send()
    await cl.sleep(1)
    msg.content = agent_executor.invoke({"input": message.content})['output']
    await msg.update()

"""This module declares the extensions used by the application."""
from flask_bcrypt import Bcrypt
from flask_cors import CORS
from flask_login import LoginManager
from flask_mail import Mail
from flask_marshmallow import Marshmallow
from flask_migrate import Migrate
from flask_sqlalchemy import SQLAlchemy

cors = CORS()
db = SQLAlchemy()
migrate = Migrate()
ma = Marshmallow()
bcrypt = Bcrypt()
login_manager = LoginManager()
mail = Mail()


    YouTubeChannelDetailsTool(),
    DeleteYoutubePlaylistsTool(),
    InsertVideoIntoPlaylistTool(),
    CreatePlaylistTool(),
    ListChannelPlaylistsTool(),
    MyYouTubeChannelDetailsTool(),
    FindUserCommentsTool(),
    FindMyCommentsTool(),
    ListVideoCommentRepliesTool(),
    ReplyCommentTool(),
]


def get_tools(query: str, tools: list[Tool] = tools) -> str:
    """Get the agent tools."""
    documents: list[Document] = [
        Document(page_content=tool.description, metadata={"index": i})
        for i, tool in enumerate(tools)
    ]
    vectore_store = FAISS.from_documents(documents, OpenAIEmbeddings())
    retriver = vectore_store.as_retriever()
    retrieved = retriver.get_relevant_documents(query)
    return [tools[document.metadata["index"]] for document in retrieved]


def get_agent_executor():
    """Get the agent"""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a funny and friendly youtube assistant. Your task is to help the user with tasks related to youtube..",
            ),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )

    functions = [format_tool_to_openai_function(t) for t in tools]



# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


class LeetcodePipeline:
    def process_item(self, item, spider):
        return item


"""This module declares application exceptions."""


class DatabaseNotConnectedException(Exception):
    """Raised when the database is not connected."""


    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main()


def create_post(post_data: CreatePost, post_image: dict, session: Session):
    post_image_url: str = save_post_photo(post_image)
    post: Post = Post(
        id='Post_' + str(uuid4()),
        author_id=post_data.author_id,
        location=post_data.location,
        text=post_data.text,
        image_url=post_image_url
    )
    with session() as db:
        db.add(post)
        db.commit()
        db.refresh(post)
    return post

import distutils.spawn
import os
import re
import shlex
import subprocess
import sys

from setuptools import find_packages
from setuptools import setup


def get_version():
    filename = "labelme/__init__.py"
    with open(filename) as f:
        match = re.search(r"""^__version__ = ['"]([^'"]*)['"]""", f.read(), re.M)
    if not match:
        raise RuntimeError("{} doesn't contain __version__".format(filename))
    version = match.groups()[0]
    return version


def get_install_requires():
    install_requires = [
        "gdown",
        "imgviz>=1.7.5",
        "matplotlib",
        "natsort>=7.1.0",
        "numpy",
        "onnxruntime>=1.14.1,!=1.16.0",
        "Pillow>=2.8",
        "PyYAML",
        "qtpy!=1.11.2",
        "scikit-image",
        "termcolor",
    ]

    # Find python binding for qt with priority:
    # PyQt5 -> PySide2
    # and PyQt5 is automatically installed on Python3.
    QT_BINDING = None


from typing import List, Optional

from pydantic import BaseModel, Field


class Config(BaseModel):
    path: set[str] = Field(description='The path to the source code directory')
    overwrite_function_docstring: Optional[bool] = Field(
        description='Whether or not to overwrite the existing function docstring',
        default=False,
    )
    overwrite_class_docstring: Optional[bool] = Field(
        description='Whether or not to overwrite the existing class docstring',
        default=False,
    )
    overwrite_class_methods_docstring: Optional[bool] = Field(
        description='Whether or not to overwrite the existing class methods docstring',
        default=False,
    )
    documentation_style: Optional[str] = Field(
        description='The format of documentation to use',
        default='Numpy-Style',
        enum=['Numpy-Style', 'Google-Style', 'Sphinx-Style'],
    )
    directories_ignore: set[str] = Field(
        description='Directories to ignore',
        default={'venv', '.venv', '__pycache__', '.git', 'build', 'dist', 'docs'},
    )
    files_ignore: set[str] = Field(
        description='Files to ignore',
        default_factory=set,
    )


chain = topic_assign_tmpl | chat | output_parser
res = chain.invoke(inputs)
print(res)



query: str = """
Find the id of the youtube channel "Ark Invest", then using the id, find the number of subcribers 
for the channel.
"""

res = agent.run(query)
print(res)

import logging

from .agent_state import AgentState
from .states import State
from .ui import BaseUI


class AgentNelly:
    def __init__(self, ui: BaseUI, initial_state: State) -> None:
        self._ui: BaseUI = ui
        self._ui.agent = self
        self._ui.launch()
        self._agent_state = AgentState()
        self.transition_to(initial_state)

    @property
    def ui(self) -> BaseUI:
        return self._ui

    @property
    def agent_state(self) -> AgentState:
        return self._agent_state

    def transition_to(self, state: State) -> None:
        if state:
            logging.info("Transitioning to the state: %s", type(state).__name__)
            self._state = state
            self._state.agent = self
        else:
            self._state = None

    def analyze_product_review(self) -> None:
        while self._state:
            try:
                self._state.execute()
            except KeyboardInterrupt:
                from .ui.utils import (agent_confirm_prompt,
                                       agent_question_prompt)

                end_chat: bool = agent_confirm_prompt(


            'youtube.resources.mixins'
        ]
        ),
    version=VERSION,
    description=DESCRIPTION,
    long_description_content_type='text/markdown',
    long_description=LONG_DESCRIPTION,
    url='https://youtube-wrapper.readthedocs.io/en/latest/index.html',
    author='Lyle Okoth',
    author_email='lyceokoth@gmail.com',
    license='MIT',
    install_requires=install_requires,
    keywords=key_words,
    classifiers=[
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.10',
        'Programming Language :: Python :: 3.11',
        'Operating System :: OS Independent'
    ],
)


from .register_blueprints import register_blueprints

from sqlalchemy import create_engine
from sqlalchemy.orm import DeclarativeBase, MappedAsDataclass
from sqlalchemy.orm import sessionmaker
from ...config.config import BaseConfig
from contextlib import contextmanager
from flask import current_app


class Base(MappedAsDataclass, DeclarativeBase):
    pass

SQLALCHEMY_DATABASE_URI = BaseConfig().db_conn_string
engine = create_engine(SQLALCHEMY_DATABASE_URI)
Session = sessionmaker(bind=engine, autocommit=False, autoflush=False)

def create_all():
    Base.metadata.create_all(bind=engine)
    
def drop_all():
    Base.metadata.drop_all(bind=engine)

@contextmanager
def get_db():
    try:
        db = Session()
        yield db
    finally:
        db.close()

# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

from scrapy.item import Item, Field
from itemloaders.processors import MapCompose, TakeFirst
from datetime import datetime


def remove_quotes(text):
    # strip the unicode quotes
    text = text.strip(u'\u201c'u'\u201d')
    return text


def convert_date(text):
    # convert string March 14, 1879 to Python date
    return datetime.strptime(text, '%B %d, %Y')


def parse_location(text):
    # parse location "in Ulm, Germany"
    # this simply remove "in ", you can further parse city, state, country, etc.
    return text[3:]

def parse_url(url: str) -> str:
    url: str = [url.split('/')[-2]]
    try:
        int(url)
    except:
        return '1'
    return url
    
class TagItem(Item):
    name = Field()

class QuoteItemSchema(Item):


from dotenv import load_dotenv
load_dotenv()
from crewai import Crew
from textwrap import dedent

from product_review_agents import ProductReviewAgents
from product_review_tasks import ProductReviewTasks


class ProductReviewCrew:
  def __init__(self, product):
    self.product = product

  def run(self):
    agents = ProductReviewAgents()
    tasks = ProductReviewTasks()

    research_analyst_agent = agents.research_analyst()

    research_task = tasks.research(research_analyst_agent, self.product)

    crew = Crew(
      agents=[
        research_analyst_agent,
      ],
      tasks=[
        research_task,
      ],
      verbose=True
    )

    result = crew.kickoff()
    return result

if __name__ == "__main__":
  print("## Welcome to Product Analysis Crew")
  print('-------------------------------')
  company = input(
    dedent("""
      What is the product you want to analyze?


from .find_product_reviews_tool import FindProductReviewTools
from .find_product_tool import FindProductVideoTools

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from experiment_config import ExperimentConfig


def create_numeric_pipeline() -> Pipeline:
    num_pipeline: Pipeline = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    return num_pipeline

def create_categorical_pipeline() -> Pipeline:
    cat_pipeline: Pipeline = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder())
    ])
    return cat_pipeline

def create_experiment_pipeline(experiment_config: ExperimentConfig) -> ColumnTransformer:
    preprocessor: ColumnTransformer = ColumnTransformer(
        transformers=[
            ('drop_columns', 'drop', experiment_config.columns_to_drop),
            ('num', create_numeric_pipeline(), experiment_config.numerical_features),
            ('cat', create_categorical_pipeline(), experiment_config.categorical_features)
        ],
        remainder='passthrough'
    )
    return preprocessor

import ast
from queue import Empty, Queue

from .config import Config
from .docstring_writer import ClassDocStringWriter, FunctionDocStringWriter
from .helpers import (
    format_file,
    get_class_source,
    get_functions_source,
    get_module_source_code,
    save_processed_file,
)


def queue_unprocessed_functions_methods(
    functions_source_queue: Queue, classes_source_queue: Queue, module_path_queue: Queue
) -> None:
    while True:
        try:
            module_path: str = module_path_queue.get()
            functions: list[str] = get_functions_source(module_path)
            for function_name, function_code in functions:
                functions_source_queue.put((module_path, function_name, function_code))
            classes: list[str] = get_class_source(module_path)
            for class_name, class_code in classes:
                classes_source_queue.put((module_path, class_name, class_code))
        except Empty:
            continue
        else:
            module_path_queue.task_done()


def generate_function_docstrings(functions_source_queue: Queue, config: Config) -> None:
    """Generate docstrings for this file."""
    while True:
        try:
            module_path, function_name, function_code = functions_source_queue.get()
            module_tree = ast.parse(get_module_source_code(module_path))
            transformer = FunctionDocStringWriter(
                module_path=module_path,


    def process_item(self, item: Item, spider: Spider):
        """Save quotes in the database
        This method is called for every item pipeline component
        """
        session = self.Session()
        
        category = Category()
        slide = Slide()
        tag = Tag()
        
        slide.id = str(uuid.uuid4())
        slide.description = item["description"]
        slide.title = item["title"]
        slide.image_urls = item["image_urls"]
        slide.image_paths = [image["path"] for image in item["images"]]
        slide.colors = item["colors"]
        
        category.id = str(uuid.uuid4())
        if item.get("category"):
            category.name = item["category"]
        else:
            category.name = ""

        # check whether the category exists
        exist_category = session.query(Category).filter_by(name=category.name).first()
        if exist_category is not None:  # the current category exists
            slide.category = exist_category
        else:
            slide.category = category

        # check whether the current quote has tags or not
        for tag_name in item["tags"]:
            tag = Tag(name=tag_name, id=str(uuid.uuid4()))
            # check whether the current tag already exists in the database
            exist_tag = session.query(Tag).filter_by(name=tag.name).first()
            if exist_tag is not None:  # the current tag exists
                tag = exist_tag
            slide.tags.append(tag)

        try:


from qtpy import QtCore
from qtpy import QtGui
from qtpy import QtWidgets


class ZoomWidget(QtWidgets.QSpinBox):
    def __init__(self, value=100):
        super(ZoomWidget, self).__init__()
        self.setButtonSymbols(QtWidgets.QAbstractSpinBox.NoButtons)
        self.setRange(1, 1000)
        self.setSuffix(" %")
        self.setValue(value)
        self.setToolTip("Zoom Level")
        self.setStatusTip(self.toolTip())
        self.setAlignment(QtCore.Qt.AlignCenter)

    def minimumSizeHint(self):
        height = super(ZoomWidget, self).minimumSizeHint().height()
        fm = QtGui.QFontMetrics(self.font())
        width = fm.width(str(self.maximum()))
        return QtCore.QSize(width, height)



    def moveShape(self):
        self.canvas.endMove(copy=False)
        self.setDirty()

    def openDirDialog(self, _value=False, dirpath=None):
        if not self.mayContinue():
            return

        defaultOpenDirPath = dirpath if dirpath else "."
        if self.lastOpenDir and osp.exists(self.lastOpenDir):
            defaultOpenDirPath = self.lastOpenDir
        else:
            defaultOpenDirPath = osp.dirname(self.filename) if self.filename else "."

        targetDirPath = str(
            QtWidgets.QFileDialog.getExistingDirectory(
                self,
                self.tr("%s - Open Directory") % __appname__,
                defaultOpenDirPath,
                QtWidgets.QFileDialog.ShowDirsOnly
                | QtWidgets.QFileDialog.DontResolveSymlinks,
            )
        )
        self.importDirImages(targetDirPath)

    @property
    def imageList(self):
        lst = []
        for i in range(self.fileListWidget.count()):
            item = self.fileListWidget.item(i)
            lst.append(item.text())
        return lst

    def importDroppedImageFiles(self, imageFiles):
        extensions = [
            ".%s" % fmt.data().decode().lower()
            for fmt in QtGui.QImageReader.supportedImageFormats()
        ]




def _compute_mask_from_points(
    image_size, decoder_session, image, image_embedding, points, point_labels
):
    input_point = np.array(points, dtype=np.float32)
    input_label = np.array(point_labels, dtype=np.int32)

    onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[
        None, :, :
    ]
    onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(
        np.float32
    )

    scale, new_height, new_width = _compute_scale_to_resize_image(
        image_size=image_size, image=image
    )
    onnx_coord = (
        onnx_coord.astype(float)
        * (new_width / image.shape[1], new_height / image.shape[0])
    ).astype(np.float32)

    onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)
    onnx_has_mask_input = np.array([-1], dtype=np.float32)

    decoder_inputs = {
        "image_embeddings": image_embedding,
        "point_coords": onnx_coord,
        "point_labels": onnx_label,
        "mask_input": onnx_mask_input,
        "has_mask_input": onnx_has_mask_input,
        "orig_im_size": np.array(image.shape[:2], dtype=np.float32),
    }

    masks, _, _ = decoder_session.run(None, decoder_inputs)
    mask = masks[0, 0]  # (1, 1, H, W) -> (H, W)
    mask = mask > 0.0

    MIN_SIZE_RATIO = 0.05
    skimage.morphology.remove_small_objects(



#             # With probability of teacher_force_ratio we take the actual next word
#             # otherwise we take the word that the Decoder predicted it to be.
#             # Teacher Forcing is used so that the model gets used to seeing
#             # similar inputs at training and testing time, if teacher forcing is 1
#             # then inputs at test time might be completely different than what the
#             # network is used to. This was a long comment.
#             x = target[t] if random.random() < teacher_force_ratio else best_guess

#         return outputs


# ### We're ready to define everything we need for training our Seq2Seq model ###

# # Training hyperparameters
# num_epochs = 100
# learning_rate = 0.001
# batch_size = 64

# # Model hyperparameters
# load_model = False
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# input_size_encoder = len(german.vocab)
# input_size_decoder = len(english.vocab)
# output_size = len(english.vocab)
# encoder_embedding_size = 300
# decoder_embedding_size = 300
# hidden_size = 1024  # Needs to be the same for both RNN's
# num_layers = 2
# enc_dropout = 0.5
# dec_dropout = 0.5

# # Tensorboard to get nice loss plot
# writer = SummaryWriter(f"runs/loss_plot")
# step = 0

# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
#     (train_data, valid_data, test_data),
#     batch_size=batch_size,
#     sort_within_batch=True,


from langchain.chains import StuffDocumentsChain
from langchain.chains.llm import LLMChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.text import TextLoader
from os import path
from langchain.prompts import PromptTemplate
from langchain.llms.base import BaseLLM
from langchain_openai import ChatOpenAI, OpenAI


data_dir = "./agent_nelly/data_analysis/data"
summary_dir = "summary"
save_transcript_dir = path.join(data_dir, summary_dir, "summary.txt")

api_key: str = "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"

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)


template_str: str = """
You are provided with the transcript for a youtube video. The video is a review of the product {product}. 
Return a JSON object with a single key ```features``` which is a list of all the {product} features mentioned.
Transcript: {text} 
"""

product: str = "iphone 15 pro max"
template = PromptTemplate.from_template(template_str)
template = template.partial(product=product)

api_key: str = "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"
chat: BaseLLM = ChatOpenAI(temperature=0, api_key=api_key)
llm: BaseLLM = OpenAI(temperature=0, api_key=api_key)

with open(save_transcript_dir, 'r') as f:
    summry: str = f.read()
    
chain = template | llm
res = chain.invoke({"text": summry})
print(res)

from youtube import YouTube
from youtube.models import Search
from youtube.resources.schemas import (
    CreatePlaylistSchema, CreatePlaylistSnippet, CreateStatus, CreatePlaylistItem, CreatePlaylistItemSnippet,
    VideoResourceId, YouTubeRequest, SearchPart, SearchOptionalParameters, SearchFilter,
    CommentThreadPart, CommentThreadFilter, CommentThreadOptionalParameters, YouTubeResponse,
    Part, Filter, OptionalParameters
)
from typing import Any


client_secret_file: str = 'client_secret.json'
youtube: YouTube = YouTube(client_secret_file=client_secret_file)
youtube.authenticate()
# part = Part()
# optional_parameters: OptionalParameters = OptionalParameters(
#     q='news',
#     maxResults=2,
#     eventType='live',
#     type=['video']
# )
# optional_parameters: OptionalParameters = OptionalParameters(
#     q='Python programming',
#     maxResults=2,
#     type=['video', 'channel', 'playlist'],
# )
# search = SearchSchema(part=part, optional_parameters=optional_parameters)

# print(youtube.search(search))
# search_iterator = youtube.get_search_iterator(search)
# print(next(search_iterator))
# print(next(search_iterator))
# print(next(search_iterator))
# print(youtube.find_video_by_id('rfscVS0vtbw'))
# print(youtube.find_channel_by_name('East Meets West'))
# print(youtube.get_video_ratings(['s7AvT7cGdSo']))
# print(youtube.find_most_popular_video_by_regionn(region_code='KE'))
# print(youtube.rate_video('s7AvT7cGdSo', 'like'))
# print(youtube.update_video())
# print(youtube.upload_video())



    def boundedMoveVertex(self, pos):
        index, shape = self.hVertex, self.hShape
        point = shape[index]
        if self.outOfPixmap(pos):
            pos = self.intersectionPoint(point, pos)
        shape.moveVertexBy(index, pos - point)

    def boundedMoveShapes(self, shapes, pos):
        if self.outOfPixmap(pos):
            return False  # No need to move
        o1 = pos + self.offsets[0]
        if self.outOfPixmap(o1):
            pos -= QtCore.QPointF(min(0, o1.x()), min(0, o1.y()))
        o2 = pos + self.offsets[1]
        if self.outOfPixmap(o2):
            pos += QtCore.QPointF(
                min(0, self.pixmap.width() - o2.x()),
                min(0, self.pixmap.height() - o2.y()),
            )
        # XXX: The next line tracks the new position of the cursor
        # relative to the shape, but also results in making it
        # a bit "shaky" when nearing the border and allows it to
        # go outside of the shape's area for some reason.
        # self.calculateOffsets(self.selectedShapes, pos)
        dp = pos - self.prevPoint
        if dp:
            for shape in shapes:
                shape.moveBy(dp)
            self.prevPoint = pos
            return True
        return False

    def deSelectShape(self):
        if self.selectedShapes:
            self.setHiding(False)
            self.selectionChanged.emit([])
            self.hShapeIsSelected = False
            self.update()



def create_post(post_data: CreatePost, post_image: dict, session: Session):
    post_image_url: str = save_post_photo(post_image)
    post: Post = Post(
        id='Post_' + str(uuid4()),
        author_id=post_data.author_id,
        location=post_data.location,
        text=post_data.text,
        image_url=post_image_url
    )
    with session() as db:
        db.add(post)
        db.commit()
        db.refresh(post)
    return post

#!/usr/bin/env python

from __future__ import print_function

import argparse
import glob
import os
import os.path as osp
import sys

import imgviz
import numpy as np

import labelme


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("input_dir", help="Input annotated directory")
    parser.add_argument("output_dir", help="Output dataset directory")
    parser.add_argument(
        "--labels", help="Labels file or comma separated text", required=True
    )
    parser.add_argument(
        "--noobject", help="Flag not to generate object label", action="store_true"
    )
    parser.add_argument(
        "--nonpy", help="Flag not to generate .npy files", action="store_true"
    )
    parser.add_argument(
        "--noviz", help="Flag to disable visualization", action="store_true"
    )
    args = parser.parse_args()

    if osp.exists(args.output_dir):
        print("Output directory already exists:", args.output_dir)
        sys.exit(1)
    os.makedirs(args.output_dir)


import os
from argparse import ArgumentParser, Namespace
from os import path

from .config import Config


def parse_arguments() -> Namespace:
    parser = ArgumentParser(
        prog="docstring-generator",
        description="Generate docstrings for your python projects",
        epilog="Thanks for using %(prog)s! :)",
    )
    parser.add_argument("--path", nargs="*", default=["."], type=str)
    parser.add_argument("--config-file", nargs="?", default="", type=str)
    parser.add_argument("--OPENAI_API_KEY", nargs="?", default="", type=str)
    parser.add_argument(
        "--overwrite-function-docstring", nargs="?", default=False, type=bool
    )
    parser.add_argument("--directories-ignore", nargs="*", default=[], type=str)
    parser.add_argument("--files-ignore", nargs="*", default=[], type=str)
    parser.add_argument(
        "--documentation-style",
        nargs="?",
        default="Numpy-Style",
        choices=["Numpy-Style", "Google-Style", "Sphinx-Style"],
        type=str,
    )
    args = parser.parse_args()
    paths: list[str] = args.path
    for entry in paths:
        if not path.exists(entry):
            print(f"The target path '{entry}' doesn't exist")
            raise SystemExit(1)
    if args.OPENAI_API_KEY:
        os.environ["OPENAI_API_KEY"] = args.OPENAI_API_KEY
    if not os.environ.get("OPENAI_API_KEY", None):
        print("You have not provided the open ai api key.")
        raise SystemExit(1)
    return args


# -*- encoding: utf-8 -*-

import html

from qtpy import QtWidgets
from qtpy.QtCore import Qt

from .escapable_qlist_widget import EscapableQListWidget


class UniqueLabelQListWidget(EscapableQListWidget):
    def mousePressEvent(self, event):
        super(UniqueLabelQListWidget, self).mousePressEvent(event)
        if not self.indexAt(event.pos()).isValid():
            self.clearSelection()

    def findItemByLabel(self, label):
        for row in range(self.count()):
            item = self.item(row)
            if item.data(Qt.UserRole) == label:
                return item

    def createItemFromLabel(self, label):
        if self.findItemByLabel(label):
            raise ValueError("Item for label '{}' already exists".format(label))

        item = QtWidgets.QListWidgetItem()
        item.setData(Qt.UserRole, label)
        return item

    def setItemLabel(self, item, label, color=None):
        qlabel = QtWidgets.QLabel()
        if color is None:
            qlabel.setText("{}".format(label))
        else:
            qlabel.setText(
                '{} <font color="#{:02x}{:02x}{:02x}">‚óè</font>'.format(
                    html.escape(label), *color
                )
            )


    if hasattr(args, "labels"):
        if os.path.isfile(args.labels):
            with codecs.open(args.labels, "r", encoding="utf-8") as f:
                args.labels = [line.strip() for line in f if line.strip()]
        else:
            args.labels = [line for line in args.labels.split(",") if line]

    if hasattr(args, "label_flags"):
        if os.path.isfile(args.label_flags):
            with codecs.open(args.label_flags, "r", encoding="utf-8") as f:
                args.label_flags = yaml.safe_load(f)
        else:
            args.label_flags = yaml.safe_load(args.label_flags)

    config_from_args = args.__dict__
    config_from_args.pop("version")
    reset_config = config_from_args.pop("reset_config")
    filename = config_from_args.pop("filename")
    output = config_from_args.pop("output")
    config_file_or_yaml = config_from_args.pop("config")
    config = get_config(config_file_or_yaml, config_from_args)

    if not config["labels"] and config["validate_label"]:
        logger.error(
            "--labels must be specified with --validatelabel or "
            "validate_label: true in the config file "
            "(ex. ~/.labelmerc)."
        )
        sys.exit(1)

    output_file = None
    output_dir = None
    if output is not None:
        if output.endswith(".json"):
            output_file = output
        else:
            output_dir = output

    translator = QtCore.QTranslator()
    translator.load(


import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator
import numpy as np
import spacy
import random
from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard
from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint

spacy_ger = spacy.load("de")
spacy_eng = spacy.load("en")


def tokenize_ger(text):
    return [tok.text for tok in spacy_ger.tokenizer(text)]


def tokenize_eng(text):
    return [tok.text for tok in spacy_eng.tokenizer(text)]


german = Field(tokenize=tokenize_ger, lower=True, init_token="<sos>", eos_token="<eos>")

english = Field(
    tokenize=tokenize_eng, lower=True, init_token="<sos>", eos_token="<eos>"
)

train_data, valid_data, test_data = Multi30k.splits(
    exts=(".de", ".en"), fields=(german, english)
)

german.build_vocab(train_data, max_size=10000, min_freq=2)
english.build_vocab(train_data, max_size=10000, min_freq=2)


# class Encoder(nn.Module):
#     def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):
#         super(Encoder, self).__init__()


from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from os import path

data_dir = "data"
video_data_dir = "data"
transcribed_data = "transcriptions"
video_title = "iphone_15_marques_review"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")

api_key: str = "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"
with open(save_transcript_dir, "r") as f:
    video_transcript = f.read()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=150)
splits = text_splitter.split_text(video_transcript)

embeddings = OpenAIEmbeddings(api_key=api_key)
vectordb = FAISS.from_texts(splits, embeddings)

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=api_key),
    chain_type="stuff",
    retriever=vectordb.as_retriever(),
)

features: list[str] = [
    "5G connectivity",
    "A15 Bionic chip",
    "ProMotion display",
    "Ceramic Shield front cover",
    "Triple-camera system",
    "LiDAR scanner",
    "Night mode",
    "Cinematic mode",
    "Dolby Vision HDR recording",
    "MagSafe charging",


# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class SlidesmodelSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method


"""This module contains routes for the app."""
from flask import Blueprint, flash, jsonify, redirect, render_template, request, url_for
from http import HTTPStatus
from ..database.schemas.post import (
    CreatePost, CreatedPost, GetPost, GetPosts, UpdatePost, PostSchema, PostAuthor
)
from ..database.crud.post import (
    create_post, get_post, get_posts, delete_post, update_post
)
from ..database.models.post import Post
from ..database.database import get_db
from pydantic import ValidationError
from sqlalchemy.exc import OperationalError, IntegrityError
from ..database.models.user import User
from ..database.crud.user import get_user
from ..database.schemas.user import GetUser
from ..database.models.bookmark import Bookmark
from ..database.crud.bookmark import list_post_bookmarks
from ..database.schemas.activity import ActivityCreated, RepeatableActivityCreated, CommentCreated
from ..database.crud.like import list_post_likes
from ..database.models.like import Like
from ..database.models.comment import Comment
from ..database.models.view import View
from ..database.schemas.comment import CommentSchema
from ..database.crud.view import (
    list_post_views
)
from ..database.crud.comment import list_post_comments
from ..home.helpers import load_posts

post = Blueprint("post", __name__)

                    question="Are you sure you want to end this chat?"
                )
                if end_chat:
                    agent_question_prompt("Goodbye")
                    break


from email.message import EmailMessage
from base64 import urlsafe_b64encode
from googleapiclient.errors import HttpError
from oauth import OAuth


def create_draft(gmail_client) -> dict:
    try:
        message = EmailMessage()
        message.set_content('This is automated draft mail')
        message['To'] = 'lyleokothdev@gmail.com'
        message['From'] = 'lyceokoth@gmail.com'
        message['Subject'] = 'Automated draft'
        # encoded message
        encoded_message = urlsafe_b64encode(message.as_bytes()).decode()
        create_message = {
            'message': {
                'raw': encoded_message
            }
        }
        draft = gmail_client.users().drafts().create(userId="me",body=create_message).execute()
        print(f'Draft id: {draft["id"]}\nDraft message: {draft["message"]}')
    except HttpError as error:
        print(f'An error occurred: {error}')
        draft = None

    return draft

def create_message():
    try:
        message = EmailMessage()
        message.set_content('This is automated draft mail')
        message['To'] = 'lyleokothdev@gmail.com'
        message['From'] = 'lyceokoth@gmail.com'
        message['Subject'] = 'Automated draft'
        # encoded message
        encoded_message = urlsafe_b64encode(message.as_bytes()).decode()
        create_message = {
            'raw': encoded_message
        }


        Returns
        -------

        """
        validation_agent = LLMChain(
            llm=self.chat_model,
            prompt=self.validation_prompt.chat_prompt,
            output_parser=self.validation_prompt.parser,
            output_key="validation_output",
            verbose=debug,
        )

        overall_chain = SequentialChain(
            chains=[validation_agent],
            input_variables=["query", "format_instructions"],
            output_variables=["validation_output"],
            verbose=debug,
        )

        return overall_chain

    def _set_up_agent_chain(self, debug=True):
        """

        Parameters
        ----------
        debug

        Returns
        -------

        """
        travel_agent = LLMChain(
            llm=self.chat_model,
            prompt=self.itinerary_prompt.chat_prompt,
            verbose=debug,
            output_key="agent_suggestion",
        )

        parser = LLMChain(


# flake8: noqa

import logging
import sys

from qtpy import QT_VERSION


__appname__ = "labelme"

# Semantic Versioning 2.0.0: https://semver.org/
# 1. MAJOR version when you make incompatible API changes;
# 2. MINOR version when you add functionality in a backwards-compatible manner;
# 3. PATCH version when you make backwards-compatible bug fixes.
# e.g., 1.0.0a0, 1.0.0a1, 1.0.0b0, 1.0.0rc0, 1.0.0, 1.0.0.post0
__version__ = "5.4.1"

QT4 = QT_VERSION[0] == "4"
QT5 = QT_VERSION[0] == "5"
del QT_VERSION

PY2 = sys.version[0] == "2"
PY3 = sys.version[0] == "3"
del sys

from labelme.label_file import LabelFile
from labelme import testing
from labelme import utils


                self.labelFile.imagePath,
            )
            self.otherData = self.labelFile.otherData
        else:
            self.imageData = LabelFile.load_image_file(filename)
            if self.imageData:
                self.imagePath = filename
            self.labelFile = None
        image = QtGui.QImage.fromData(self.imageData)

        if image.isNull():
            formats = [
                "*.{}".format(fmt.data().decode())
                for fmt in QtGui.QImageReader.supportedImageFormats()
            ]
            self.errorMessage(
                self.tr("Error opening file"),
                self.tr(
                    "<p>Make sure <i>{0}</i> is a valid image file.<br/>"
                    "Supported image formats: {1}</p>"
                ).format(filename, ",".join(formats)),
            )
            self.status(self.tr("Error reading %s") % filename)
            return False
        self.image = image
        self.filename = filename
        if self._config["keep_prev"]:
            prev_shapes = self.canvas.shapes
        self.canvas.loadPixmap(QtGui.QPixmap.fromImage(image))
        flags = {k: False for k in self._config["flags"] or []}
        if self.labelFile:
            self.loadLabels(self.labelFile.shapes)
            if self.labelFile.flags is not None:
                flags.update(self.labelFile.flags)
        self.loadFlags(flags)
        if self._config["keep_prev"] and self.noShapes():
            self.loadShapes(prev_shapes, replace=False)
            self.setDirty()
        else:
            self.setClean()


import chainlit as cl
from assistant.utils.assistant_utils import welcome_user
from assistant.agent import get_agent_executor


@cl.on_chat_start
async def start():
    res = await cl.AskUserMessage(content="What is your name?", timeout=30).send()
    if res:
        msg = cl.Message(content="")
        await msg.send()
        msg.content = welcome_user(user_name=res['content'])
        await msg.update()
        

@cl.on_message
async def main(message: cl.Message):
    msg = cl.Message(content="")
    await msg.send()
    query: str = message.content
    agent_executor = get_agent_executor(query)
    msg.content = agent_executor.invoke({"input": query})['output']
    await msg.update()

def create_post(post_data: CreatePost, post_image: dict, session: Session):
    post_image_url: str = save_post_photo(post_image)
    post: Post = Post(
        id='Post_' + str(uuid4()),
        author_id=post_data.author_id,
        location=post_data.location,
        text=post_data.text,
        image_url=post_image_url
    )
    with session() as db:
        db.add(post)
        db.commit()
        db.refresh(post)
    return post

                option.palette.color(QPalette.Active, QPalette.Text),
            )

        textRect = style.subElementRect(QStyle.SE_ItemViewItemText, options)

        if index.column() != 0:
            textRect.adjust(5, 0, 0, 0)

        thefuckyourshitup_constant = 4
        margin = (option.rect.height() - options.fontMetrics.height()) // 2
        margin = margin - thefuckyourshitup_constant
        textRect.setTop(textRect.top() + margin)

        painter.translate(textRect.topLeft())
        painter.setClipRect(textRect.translated(-textRect.topLeft()))
        self.doc.documentLayout().draw(painter, ctx)

        painter.restore()

    def sizeHint(self, option, index):
        thefuckyourshitup_constant = 4
        return QtCore.QSize(
            int(self.doc.idealWidth()),
            int(self.doc.size().height() - thefuckyourshitup_constant),
        )


class LabelListWidgetItem(QtGui.QStandardItem):
    def __init__(self, text=None, shape=None):
        super(LabelListWidgetItem, self).__init__()
        self.setText(text or "")
        self.setShape(shape)

        self.setCheckable(True)
        self.setCheckState(Qt.Checked)
        self.setEditable(False)
        self.setTextAlignment(Qt.AlignBottom)

    def clone(self):
        return LabelListWidgetItem(self.text(), self.shape())


    os.makedirs(osp.join(args.output_dir, "JPEGImages"))
    os.makedirs(osp.join(args.output_dir, "SegmentationClass"))
    if not args.nonpy:
        os.makedirs(osp.join(args.output_dir, "SegmentationClassNpy"))
    if not args.noviz:
        os.makedirs(osp.join(args.output_dir, "SegmentationClassVisualization"))
    if not args.noobject:
        os.makedirs(osp.join(args.output_dir, "SegmentationObject"))
        if not args.nonpy:
            os.makedirs(osp.join(args.output_dir, "SegmentationObjectNpy"))
        if not args.noviz:
            os.makedirs(osp.join(args.output_dir, "SegmentationObjectVisualization"))
    print("Creating dataset:", args.output_dir)

    if osp.exists(args.labels):
        with open(args.labels) as f:
            labels = [label.strip() for label in f if label]
    else:
        labels = [label.strip() for label in args.labels.split(",")]

    class_names = []
    class_name_to_id = {}
    for i, label in enumerate(labels):
        class_id = i - 1  # starts with -1
        class_name = label.strip()
        class_name_to_id[class_name] = class_id
        if class_id == -1:
            assert class_name == "__ignore__"
            continue
        elif class_id == 0:
            assert class_name == "_background_"
        class_names.append(class_name)
    class_names = tuple(class_names)
    print("class_names:", class_names)
    out_class_names_file = osp.join(args.output_dir, "class_names.txt")
    with open(out_class_names_file, "w") as f:
        f.writelines("\n".join(class_names))
    print("Saved class_names:", out_class_names_file)

    for filename in sorted(glob.glob(osp.join(args.input_dir, "*.json"))):


        self.toggleActions(True)
        self.canvas.setFocus()
        self.status(str(self.tr("Loaded %s")) % osp.basename(str(filename)))
        return True

    def resizeEvent(self, event):
        if (
            self.canvas
            and not self.image.isNull()
            and self.zoomMode != self.MANUAL_ZOOM
        ):
            self.adjustScale()
        super(MainWindow, self).resizeEvent(event)

    def paintCanvas(self):
        assert not self.image.isNull(), "cannot paint null image"
        self.canvas.scale = 0.01 * self.zoomWidget.value()
        self.canvas.adjustSize()
        self.canvas.update()

    def adjustScale(self, initial=False):
        value = self.scalers[self.FIT_WINDOW if initial else self.zoomMode]()
        value = int(100 * value)
        self.zoomWidget.setValue(value)
        self.zoom_values[self.filename] = (self.zoomMode, value)

    def scaleFitWindow(self):
        """Figure out the size of the pixmap to fit the main widget."""
        e = 2.0  # So that no scrollbars are generated.
        w1 = self.centralWidget().width() - e
        h1 = self.centralWidget().height() - e
        a1 = w1 / h1
        # Calculate a new scale value based on the pixmap's aspect ratio.
        w2 = self.canvas.pixmap.width() - 0.0
        h2 = self.canvas.pixmap.height() - 0.0
        a2 = w2 / h2
        return w1 / w2 if a2 >= a1 else h1 / h2

    def scaleFitWidth(self):
        # The epsilon does not seem to work too well here.


import PIL.Image
import PIL.ImageEnhance
from qtpy import QtGui
from qtpy import QtWidgets
from qtpy.QtCore import Qt

from .. import utils


class BrightnessContrastDialog(QtWidgets.QDialog):
    def __init__(self, img, callback, parent=None):
        super(BrightnessContrastDialog, self).__init__(parent)
        self.setModal(True)
        self.setWindowTitle("Brightness/Contrast")

        self.slider_brightness = self._create_slider()
        self.slider_contrast = self._create_slider()

        formLayout = QtWidgets.QFormLayout()
        formLayout.addRow(self.tr("Brightness"), self.slider_brightness)
        formLayout.addRow(self.tr("Contrast"), self.slider_contrast)
        self.setLayout(formLayout)

        assert isinstance(img, PIL.Image.Image)
        self.img = img
        self.callback = callback

    def onNewValue(self, value):
        brightness = self.slider_brightness.value() / 50.0
        contrast = self.slider_contrast.value() / 50.0

        img = self.img
        img = PIL.ImageEnhance.Brightness(img).enhance(brightness)
        img = PIL.ImageEnhance.Contrast(img).enhance(contrast)

        img_data = utils.img_pil_to_data(img)
        qimage = QtGui.QImage.fromData(img_data)
        self.callback(qimage)

    def _create_slider(self):


from typing import Optional
from youtube.schemas import (
    SearchPart, SearchOptionalParameters, YouTubeResponse, YouTubeRequest
)
from youtube.schemas import (
    CommentThreadFilter, CommentThreadOptionalParameters, CommentThreadPart
)
from youtube.models import Search, Comment
from .extensions import youtube_client
from collections.abc import Iterator


def advanced_video_search( 
    query: str,
    channel_id: Optional[str] = None,
    max_results: Optional[int] = 10,
    order: Optional[str] = None,
    published_after: Optional[str] = None,
    published_before: Optional[str] = None,
    region_code: Optional[str] = None,
    relevance_language: Optional[str] = 'en',
    video_caption: Optional[str] = None,
    video_category_id: Optional[str] = None,
    video_definition: Optional[str] = None,
    video_dimension: Optional[str] = None,
    video_duration: Optional[str] = None,
    video_paid_product_placement: Optional[str] = None,
    video_syndicated: Optional[str] = None,
    video_type: Optional[str] = 'any'
    ) -> list[Search]:
    """Search the given channel for the given videos."""
    search_part: SearchPart = SearchPart()
    optional_params: SearchOptionalParameters = SearchOptionalParameters(
        channelId=channel_id,
        q=query,
        maxResults=max_results,
        order=order,
        publishedAfter=published_after,
        publishedBefore=published_before,
        regionCode=region_code,


        QtWidgets.QApplication.setOverrideCursor(cursor)

    def restoreCursor(self):
        QtWidgets.QApplication.restoreOverrideCursor()

    def resetState(self):
        self.restoreCursor()
        self.pixmap = None
        self.shapesBackups = []
        self.update()


    print("Creating dataset:", args.output_dir)

    class_names = []
    class_name_to_id = {}
    for i, line in enumerate(open(args.labels).readlines()):
        class_id = i - 1  # starts with -1
        class_name = line.strip()
        class_name_to_id[class_name] = class_id
        if class_id == -1:
            assert class_name == "__ignore__"
            continue
        elif class_id == 0:
            assert class_name == "_background_"
        class_names.append(class_name)
    class_names = tuple(class_names)
    print("class_names:", class_names)
    out_class_names_file = osp.join(args.output_dir, "class_names.txt")
    with open(out_class_names_file, "w") as f:
        f.writelines("\n".join(class_names))
    print("Saved class_names:", out_class_names_file)

    for filename in glob.glob(osp.join(args.input_dir, "*.json")):
        print("Generating dataset from:", filename)

        label_file = labelme.LabelFile(filename=filename)

        base = osp.splitext(osp.basename(filename))[0]
        out_img_file = osp.join(args.output_dir, "JPEGImages", base + ".jpg")
        out_xml_file = osp.join(args.output_dir, "Annotations", base + ".xml")
        if not args.noviz:
            out_viz_file = osp.join(
                args.output_dir, "AnnotationsVisualization", base + ".jpg"
            )

        img = labelme.utils.img_data_to_arr(label_file.imageData)
        imgviz.io.imsave(out_img_file, img)

        maker = lxml.builder.ElementMaker()
        xml = maker.annotation(
            maker.folder(),


from sqlalchemy import create_engine
from sqlalchemy.orm import DeclarativeBase, MappedAsDataclass
from sqlalchemy.orm import sessionmaker
from ...config.config import BaseConfig
from contextlib import contextmanager
from flask import current_app


class Base(MappedAsDataclass, DeclarativeBase):
    pass

SQLALCHEMY_DATABASE_URI = BaseConfig().db_conn_string
engine = create_engine(SQLALCHEMY_DATABASE_URI)
Session = sessionmaker(bind=engine, autocommit=False, autoflush=False)

def create_all():
    Base.metadata.create_all(bind=engine)
    
def drop_all():
    Base.metadata.drop_all(bind=engine)

@contextmanager
def get_db():
    try:
        db = Session()
        yield db
    finally:
        db.close()

query = """
        I want to do a three day trip across Kenya's Rift valley.
        """
travel_agent = Agent(
   open_ai_api_key=secrets['OPENAI_API_KEY'],
   debug=True,
)

itinerary, list_of_places, validation = travel_agent.suggest_travel(query)
print(validation)
print(itinerary)
print(list_of_places)


        otherData = {}
        for key, value in data.items():
            if key not in keys:
                otherData[key] = value

        # Only replace data after everything is loaded.
        self.flags = flags
        self.shapes = shapes
        self.imagePath = imagePath
        self.imageData = imageData
        self.filename = filename
        self.otherData = otherData

    @staticmethod
    def _check_image_height_and_width(imageData, imageHeight, imageWidth):
        img_arr = utils.img_b64_to_arr(imageData)
        if imageHeight is not None and img_arr.shape[0] != imageHeight:
            logger.error(
                "imageHeight does not match with imageData or imagePath, "
                "so getting imageHeight from actual image."
            )
            imageHeight = img_arr.shape[0]
        if imageWidth is not None and img_arr.shape[1] != imageWidth:
            logger.error(
                "imageWidth does not match with imageData or imagePath, "
                "so getting imageWidth from actual image."
            )
            imageWidth = img_arr.shape[1]
        return imageHeight, imageWidth

    def save(
        self,
        filename,
        shapes,
        imagePath,
        imageHeight,
        imageWidth,
        imageData=None,
        otherData=None,


        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)


from torch.nn import Module, Linear, ReLU, Embedding, LSTM, Dropout
from torchvision.models import inception_v3, Inception_V3_Weights
from torch import Tensor
import torch


class EncoderCNN(Module):
    def __init__(self, embed_size: int, train_cnn: bool = False, dropout: float = 0.5) -> None:
        super().__init__()
        self.train_cnn: bool = train_cnn
        self.inception = inception_v3(weights=Inception_V3_Weights.DEFAULT, aux_logits=True)
        self.inception.fc = Linear(in_features=self.inception.fc.in_features, out_features=embed_size)
        self.relu = ReLU()
        self.dropout = Dropout(p=dropout)
        
    def forward(self, images: Tensor) -> Tensor:
        features = self.inception(images)
        for name, param in self.inception.named_parameters():
            if "fc.name" in name or "fc.bias" in name:
                param.requires_grad = True
            else:
                param.requires_grad = self.train_cnn
                
        return self.dropout(self.relu(features.logits))
                

class DecoderRNN(Module):
    def __init__(self, embed_size: int, hidden_size: int, vocab_size: int, num_layers: int, dropout: float = 0.5) -> None:
        super().__init__()
        self.embed = Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)
        self.lstm = LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers)
        self.linear = Linear(in_features=embed_size, out_features=vocab_size)
        self.dropout = Dropout(p=dropout)
        
    def forward(self, features, captions):
        embeddings = self.dropout(self.embed(captions))
        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)
        hiddens, _ = self.lstm(embeddings)
        outputs = self.linear(hiddens)
        return outputs


"""

sentiment_template = PromptTemplate(template=sentiment_msg, input_variables=["comment"])


class PositiveComment(BaseModel):
    doc_id: int = Field(description="The doc_id from the input")
    topics: list[str] = Field(
        description="List of the relevant topics for the customer review. Include only topics from the list provided.",
        default_factory=list,
    )
    sentiment: str = Field(
        description="Sentiment of the topic", enum=["positive", "neutral", "negative"]
    )


class NegativeComment(BaseModel):
    doc_id: int = Field(description="The doc_id from the input")
    topics: list[str] = Field(
        description="List of the relevant topics for the customer review. Include only topics from the list provided.",
        default_factory=list,
    )
    sentiment: str = Field(
        description="Sentiment of the topic", enum=["positive", "neutral", "negative"]
    )


positive_parser = PydanticOutputParser(pydantic_object=PositiveComment)
negative_parser = PydanticOutputParser(pydantic_object=NegativeComment)

topic_assg_msg: str = """
Below is a customer comment in JSON format with the following keys:
1. doc_id - identifier of the comment
2. comment - the user comment

Please analyze the provided comments and identify the main topics and sentiment. Include only the 
topics provided below:
Topics with a short description: {topics}

Comment:


"""This module contains routes for the app."""
from flask import Blueprint, render_template, jsonify, request, url_for
from http import HTTPStatus
from ..database.schemas.activity import (
    ActivityCreated, CreateActivity, RepeatableActivityCreated,
    CreateComment, CommentCreated)
from ..database.models import (
    User, Post, Comment, Bookmark, Like, View
)
from ..database.schemas.user import GetUser
from ..database.schemas.post import GetPost
from ..database.database import get_db
from ..database.crud.user import get_user, get_random_user
from ..database.crud.post import get_post
from ..database.crud.bookmark import (
    create_bookmark, delete_bookmark, list_user_bookmarks, has_bookmarked
)
from ..database.crud.view import (
    create_view, list_user_views, has_viewed
)
from ..database.crud.like import (
    create_like, delete_like, list_user_likes, has_liked, list_post_likes, 
    get_key_like
)
from ..database.crud.comment import (
    create_comment, list_user_comments, list_post_comments
)
from pydantic import ValidationError
from sqlalchemy.exc import OperationalError, IntegrityError
from ..database.schemas.post import (
    CreatePost, CreatedPost, GetPost, GetPosts, UpdatePost, PostSchema, PostAuthor, 
    PostLike, KeyComment
)
from ..database.crud.post import (
    create_post, get_post, get_posts, delete_post, update_post
)
from ..database.crud.comment import get_key_comment

          nn.ReLU(),
          nn.BatchNorm2d(128),
          nn.MaxPool2d(2),
          # Convolution 4
          nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(256),
          nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(256),
          nn.MaxPool2d(2),
      )

      self.dense_layers = nn.Sequential(
          # Dropout layer
          nn.Dropout(0.5),
          # first fully connected layer
          nn.Linear(224*224, 1024),
          # Relu activation function
          nn.ReLU(),
          nn.Dropout(0.4),
          # Final output layer
          nn.Linear(1024, K),
      )

  def forward(self, output):
    # Convolution Layers
    out = self.conv_layers(output)

    # Flatten the layers
    out = out.view(-1, 224*224)

    # Fully connected Dense Layers
    out = self.dense_layers(out)

    return out


def load_model(model_path: str = os.environ['MODEL_PATH']):
    """Load the pytorch model."""


            self.fileListWidget.currentRow() != self.imageList.index(filename)
        ):
            self.fileListWidget.setCurrentRow(self.imageList.index(filename))
            self.fileListWidget.repaint()
            return

        self.resetState()
        self.canvas.setEnabled(False)
        if filename is None:
            filename = self.settings.value("filename", "")
        filename = str(filename)
        if not QtCore.QFile.exists(filename):
            self.errorMessage(
                self.tr("Error opening file"),
                self.tr("No such file: <b>%s</b>") % filename,
            )
            return False
        # assumes same name, but json extension
        self.status(str(self.tr("Loading %s...")) % osp.basename(str(filename)))
        label_file = osp.splitext(filename)[0] + ".json"
        if self.output_dir:
            label_file_without_path = osp.basename(label_file)
            label_file = osp.join(self.output_dir, label_file_without_path)
        if QtCore.QFile.exists(label_file) and LabelFile.is_label_file(label_file):
            try:
                self.labelFile = LabelFile(label_file)
            except LabelFileError as e:
                self.errorMessage(
                    self.tr("Error opening file"),
                    self.tr(
                        "<p><b>%s</b></p>"
                        "<p>Make sure <i>%s</i> is a valid label file."
                    )
                    % (e, label_file),
                )
                self.status(self.tr("Error reading %s") % label_file)
                return False
            self.imageData = self.labelFile.imageData
            self.imagePath = osp.join(
                osp.dirname(label_file),


    "A15 Bionic chip",
    "ProMotion display",
    "Ceramic Shield front cover",
    "Triple-camera system",
    "LiDAR scanner",
    "Night mode",
    "Cinematic mode",
    "Dolby Vision HDR recording",
    "MagSafe charging",
    "Face ID",
    "Water and dust resistance",
    "iOS 15",
    "Improved battery life",
    "Siri voice recognition",
    "Apple Pay",
    "Apple Fitness+ integration",
    "Apple Arcade subscription",
    "Apple Music",
    "iMessage",
    "App Store",
    "iCloud storage",
    "Privacy features",
]
# start: int = 0
# step: int = 3
# while start < len(features):
#     f: list[str] = features[start: start + step]
#     chain = features_description_template | llm
#     res = chain.invoke({"product": "iphone 13 pro", "features": f})
#     print(res)
#     start += step
with open(save_transcript_dir, "r") as f:
    video_transcript = f.read()
    
text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=0)
splits = text_splitter.split_text(video_transcript)
features_covered = []
for doc in splits:
    chain = features_covered_template | llm
    res = chain.invoke({"transcript": doc, "product": "iphone 13 pro", "features": features})


def generate_class_docstring(class_code: str, config: Config) -> str:
    prompt_formatted_str: str = get_class_prompt_template(
        class_code=class_code, config=config
    )
    class_and_docstring = llm.invoke(prompt_formatted_str)
    return class_and_docstring


def get_class_docstring(class_and_docstring: str) -> str:
    """Get the class docstring."""
    class_tree = ast.parse(class_and_docstring)
    for node in class_tree.body:
        if isinstance(node, ClassDef):
            cls_docstring: str = ast.get_docstring(node)
            return cls_docstring


from datetime import timedelta
from redis import Redis

def request_is_rate_limited(r: Redis, key: str, limit: int, period: timedelta):
    if r.setnx(key, limit):
        r.expire(key, int(period.total_seconds()))
    bucket_val = r.get(key)
    if bucket_val and int(bucket_val) > 0:
        r.decrby(key, 1)
        return False
    return True


import argparse
import base64
import json
import os
import os.path as osp

import imgviz
import PIL.Image

from labelme import utils
from labelme.logger import logger


def main():
    logger.warning(
        "DEPRECATED: This script will be removed in the near future. "
        "Please use `labelme_export_json` instead."
    )
    logger.warning(
        "NOTE: This script is aimed to demonstrate how to convert a JSON file "
        "to a single image dataset. so it won't handle multiple JSON files to "
        "generate a real-use dataset."
    )

    parser = argparse.ArgumentParser()
    parser.add_argument("json_file")
    parser.add_argument("-o", "--out", default=None)
    args = parser.parse_args()

    json_file = args.json_file

    if args.out is None:
        out_dir = osp.basename(json_file).replace(".", "_")
        out_dir = osp.join(osp.dirname(json_file), out_dir)
    else:
        out_dir = args.out
    if not osp.exists(out_dir):
        os.mkdir(out_dir)

    data = json.load(open(json_file))


from create_dataset import dataloader, FlickrDataset, Dataset
from torchvision.transforms import Compose, ToTensor, Resize
from model import EncoderCNN, DecoderRNN, CNNToRNN
from torch.nn import Module
import torch
from PIL import Image

embed_size: int = 256
hidden_size: int = 256
num_layers: int = 3
vocab_size: int = 2994

encoder: Module = EncoderCNN(embed_size=embed_size)
decoder: Module = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, 
                             num_layers=num_layers)
model: Module = CNNToRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, 
                         num_layers=num_layers)


for i, (images, captions) in enumerate(dataloader):
    print(images.shape)
    print(captions.shape)
    break
# print(captions)

dataset: Dataset = FlickrDataset(
    images_dir="raw-data/Images", 
    captions_file="raw-data/captions.txt", 
    transforms=Compose([
        Resize((299, 299)),
        ToTensor()
    ]))
    
# features = encoder(images)
# print(features.shape)
# print(features.unsqueeze(0).shape)
# embedings = decoder.embed(captions)
# print(embedings.size())
# x = torch.cat((features.unsqueeze(0), embedings), dim=0)
# print(x.shape)


from typing import Optional

from pydantic import BaseModel, Field


class Config(BaseModel):
    root_directory: list[str] = Field(
        description="The path to the source code directory"
    )
    directories_ignore: set[str] = Field(
        description="Directories to ignore",
        default=set(["venv", ".venv", "__pycache__", ".git", "build", "dist", "docs"]),
    )
    files_ignore: set[str] = Field(
        description="Files to ignore",
        default_factory=set,
    )
    overwrite_function_docstring: Optional[bool] = Field(
        description="Whether or not to overwrite the existing function docstring",
        default=False,
    )
    documentation_style: Optional[str] = Field(
        description="The format of documentation to use",
        default="Numpy-Style",
        enum=["Numpy-Style", "Google-Style", "Sphinx-Style"],
    )


agent = get_agent_executor()
# query = "What are the main features of the iphone 15?"
# query = "What are some of the complaints about iphone 15?"
# query = "What features of the iphone 15 do its users love most?"
query = "What are the pros and cons of the iphone 15?"
res = agent.invoke({"input": query})
print(res)


    def mousePressEvent(self, ev):
        if QT5:
            pos = self.transformPos(ev.localPos())
        else:
            pos = self.transformPos(ev.posF())

        is_shift_pressed = ev.modifiers() & QtCore.Qt.ShiftModifier

        if ev.button() == QtCore.Qt.LeftButton:
            if self.drawing():
                if self.current:
                    # Add point to existing shape.
                    if self.createMode == "polygon":
                        self.current.addPoint(self.line[1])
                        self.line[0] = self.current[-1]
                        if self.current.isClosed():
                            self.finalise()
                    elif self.createMode in ["rectangle", "circle", "line"]:
                        assert len(self.current.points) == 1
                        self.current.points = self.line.points
                        self.finalise()
                    elif self.createMode == "linestrip":
                        self.current.addPoint(self.line[1])
                        self.line[0] = self.current[-1]
                        if int(ev.modifiers()) == QtCore.Qt.ControlModifier:
                            self.finalise()
                    elif self.createMode in ["ai_polygon", "ai_mask"]:
                        self.current.addPoint(
                            self.line.points[1],
                            label=self.line.point_labels[1],
                        )
                        self.line.points[0] = self.current.points[-1]
                        self.line.point_labels[0] = self.current.point_labels[-1]
                        if ev.modifiers() & QtCore.Qt.ControlModifier:
                            self.finalise()
                elif not self.outOfPixmap(pos):
                    # Create new shape.
                    self.current = Shape(
                        shape_type="points"
                        if self.createMode in ["ai_polygon", "ai_mask"]


import json
import os.path as osp

from labelme.utils import image as image_module
from labelme.utils import shape as shape_module

here = osp.dirname(osp.abspath(__file__))
data_dir = osp.join(here, "../data")


def get_img_and_data():
    json_file = osp.join(data_dir, "annotated_with_data/apc2016_obj3.json")
    with open(json_file) as f:
        data = json.load(f)
    img_b64 = data["imageData"]
    img = image_module.img_b64_to_arr(img_b64)
    return img, data


def get_img_and_lbl():
    img, data = get_img_and_data()

    label_name_to_value = {"__background__": 0}
    for shape in data["shapes"]:
        label_name = shape["label"]
        label_value = len(label_name_to_value)
        label_name_to_value[label_name] = label_value

    n_labels = max(label_name_to_value.values()) + 1
    label_names = [None] * n_labels
    for label_name, label_value in label_name_to_value.items():
        label_names[label_value] = label_name

    lbl, _ = shape_module.shapes_to_label(
        img.shape, data["shapes"], label_name_to_value
    )
    return img, lbl, label_names


from .user import User
from .post import Post
from .like import Like
from .bookmark import Bookmark
from .view import View
from .view import View
from .comment import Comment

from qtpy import QtCore
from qtpy import QtGui
from qtpy import QtWidgets
from qtpy.QtCore import Qt
from qtpy.QtGui import QPalette
from qtpy.QtWidgets import QStyle


# https://stackoverflow.com/a/2039745/4158863
class HTMLDelegate(QtWidgets.QStyledItemDelegate):
    def __init__(self, parent=None):
        super(HTMLDelegate, self).__init__()
        self.doc = QtGui.QTextDocument(self)

    def paint(self, painter, option, index):
        painter.save()

        options = QtWidgets.QStyleOptionViewItem(option)

        self.initStyleOption(options, index)
        self.doc.setHtml(options.text)
        options.text = ""

        style = (
            QtWidgets.QApplication.style()
            if options.widget is None
            else options.widget.style()
        )
        style.drawControl(QStyle.CE_ItemViewItem, options, painter)

        ctx = QtGui.QAbstractTextDocumentLayout.PaintContext()

        if option.state & QStyle.State_Selected:
            ctx.palette.setColor(
                QPalette.Text,
                option.palette.color(QPalette.Active, QPalette.HighlightedText),
            )
        else:
            ctx.palette.setColor(
                QPalette.Text,


# score = bleu(test_data[1:100], model, german, english, device)
# print(f"Bleu score {score*100:.2f}")


            outputs = model(imgs, captions[:-1])
            loss = criterion(
                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)
            )

            writer.add_scalar("Training loss", loss.item(), global_step=step)
            step += 1

            optimizer.zero_grad()
            loss.backward(loss)
            optimizer.step()


if __name__ == "__main__":
    train()


    if orientation == 1:
        # do nothing
        return image
    elif orientation == 2:
        # left-to-right mirror
        return PIL.ImageOps.mirror(image)
    elif orientation == 3:
        # rotate 180
        return image.transpose(PIL.Image.ROTATE_180)
    elif orientation == 4:
        # top-to-bottom mirror
        return PIL.ImageOps.flip(image)
    elif orientation == 5:
        # top-to-left mirror
        return PIL.ImageOps.mirror(image.transpose(PIL.Image.ROTATE_270))
    elif orientation == 6:
        # rotate 270
        return image.transpose(PIL.Image.ROTATE_270)
    elif orientation == 7:
        # top-to-right mirror
        return PIL.ImageOps.mirror(image.transpose(PIL.Image.ROTATE_90))
    elif orientation == 8:
        # rotate 90
        return image.transpose(PIL.Image.ROTATE_90)
    else:
        return image


        for comment_thread in comment_threads:
            comment: Comment = comment_thread.snippet.top_level_comment
            video_comments.append(comment)
            comment_count += 1
            if comment_count > 30:
                done = True
                break
    return video_comments


    unique_label_values = np.unique(label)

    logger.info("Label image shape: {}".format(label.shape))
    logger.info("Label values: {}".format(unique_label_values.tolist()))
    if label_names is not None:
        logger.info(
            "Label names: {}".format(
                [
                    "{}:{}".format(label_value, label_names[label_value])
                    for label_value in unique_label_values
                ]
            )
        )

    if args.image:
        num_cols = 2
    else:
        num_cols = 1

    plt.figure(figsize=(num_cols * 6, 5))

    plt.subplot(1, num_cols, 1)
    plt.title(args.label_png)
    label_viz = imgviz.label2rgb(
        label=label, label_names=label_names, font_size=label.shape[1] // 30
    )
    plt.imshow(label_viz)

    if image is not None:
        plt.subplot(1, num_cols, 2)
        label_viz_with_overlay = imgviz.label2rgb(
            label=label,
            image=image,
            label_names=label_names,
            font_size=label.shape[1] // 30,
        )
        plt.title("{}\n{}".format(args.label_png, args.image))
        plt.imshow(label_viz_with_overlay)



        Image.open("test_examples/boat.png").convert("RGB")
    ).unsqueeze(0)
    print("Example 4 CORRECT: A small boat in the ocean")
    print(
        "Example 4 OUTPUT: "
        + " ".join(model.caption_image(test_img4.to(device), dataset.vocabulary))
    )
    test_img5 = transform(
        Image.open("test_examples/horse.png").convert("RGB")
    ).unsqueeze(0)
    print("Example 5 CORRECT: A cowboy riding a horse in the desert")
    print(
        "Example 5 OUTPUT: "
        + " ".join(model.caption_image(test_img5.to(device), dataset.vocabulary))
    )
    model.train()


def save_checkpoint(state, filename="my_checkpoint.pth.tar"):
    print("=> Saving checkpoint")
    torch.save(state, filename)


def load_checkpoint(checkpoint, model, optimizer):
    print("=> Loading checkpoint")
    model.load_state_dict(checkpoint["state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer"])
    step = checkpoint["step"]
    return step

def generate_posts(authors: list[User], count: int = 100) -> list[Post]:
    """Generate posts."""
    cities = [fake.city() for _ in range(10)]
    posts_text = [fake.text() for _ in range(count)]
    dates_published = (datetime.now() + timedelta(minutes=random.randint(1,60)) for _ in range(count))
    post_images = [f'feed-{i}.jpg' for i in range(1,8)]
    return [
        Post(
            id='Post_' + str(uuid4()),
            author_id=random.choice(authors).id,
            location=random.choice(cities),
            text=text,
            image_url=random.choice(post_images),
            date_published=d
        )
        for text, d in zip(posts_text, dates_published)
    ]
    
def generate_likes(users: list[User], posts: list[Post], likes_count: int = 100) -> list[Like]:
    """Generate likes."""
    likes: list[Like] = []
    ids = set()
    for _ in range(likes_count):
        author_id: str = random.choice(users).id
        post_id: str = random.choice(posts).id
        like: Like = Like(author_id=author_id, post_id=post_id)
        if (author_id, post_id) not in ids:
            likes.append(like)
        ids.add((author_id, post_id))
    return likes

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .routers import register_routers


origins = [
    "http://localhost",
    "http://localhost:8080",
]

def create_app():
    app = FastAPI()
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    register_routers(app=app)
    
    @app.get('/health', tags=['Health'])
    async def get():
        return {'Success': 'Up!'}
    
    return app

            self, title, "<p><b>%s</b></p>%s" % (title, message)
        )

    def currentPath(self):
        return osp.dirname(str(self.filename)) if self.filename else "."

    def toggleKeepPrevMode(self):
        self._config["keep_prev"] = not self._config["keep_prev"]

    def removeSelectedPoint(self):
        self.canvas.removeSelectedPoint()
        self.canvas.update()
        if not self.canvas.hShape.points:
            self.canvas.deleteShape(self.canvas.hShape)
            self.remLabels([self.canvas.hShape])
            if self.noShapes():
                for action in self.actions.onShapesPresent:
                    action.setEnabled(False)
        self.setDirty()

    def deleteSelectedShape(self):
        yes, no = QtWidgets.QMessageBox.Yes, QtWidgets.QMessageBox.No
        msg = self.tr(
            "You are about to permanently delete {} polygons, " "proceed anyway?"
        ).format(len(self.canvas.selectedShapes))
        if yes == QtWidgets.QMessageBox.warning(
            self, self.tr("Attention"), msg, yes | no, yes
        ):
            self.remLabels(self.canvas.deleteSelected())
            self.setDirty()
            if self.noShapes():
                for action in self.actions.onShapesPresent:
                    action.setEnabled(False)

    def copyShape(self):
        self.canvas.endMove(copy=True)
        for shape in self.canvas.selectedShapes:
            self.addLabel(shape)
        self.labelList.clearSelection()
        self.setDirty()


from os import path
import json
from random import choices, choice
from langchain.docstore.document import Document
import re
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI
from langchain.pydantic_v1 import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnableBranch
from langchain_core.output_parsers import StrOutputParser


api_key: str = "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"
llm = OpenAI(temperature=0, api_key=api_key)
file_path: str = "comments.json"


def lower(text: str) -> str:
    return text.lower().strip()


def remove_urls(text: str) -> str:
    url_pattern = r"https?://\S+|www\.\S+"
    text = re.sub(url_pattern, "", text)
    return text


def remove_punctuations(text: str) -> str:
    punctuation_pattern = r"[^\w\s]"
    cleaned = re.sub(punctuation_pattern, "", text)
    return cleaned


def clean_text(text: str) -> str:
    text = lower(text)
    text = remove_urls(text)
    text = remove_punctuations(text)
    return text



@post.route("/delete", methods=["DELETE"])
def delete_one_post():
    """Delete a post."""
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        post = delete_post(get_db, GetPost(post_id=request.args.get('post_id')))
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    resp = CreatedPost(
            id=post.id,
            location=post.location,
            text=post.text,
            image_url=post.image_url,
            author_id=post.author_id,
            date_published=post.date_published
        )
    return resp.model_dump_json(indent=4), HTTPStatus.OK

from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import Field, BaseModel
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAI
import re
import json
from langchain.docstore.document import Document
from random import choices
from os import path


file_path: str = "comments.json"
api_key: str = "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"


def lower(text: str) -> str:
    return text.lower().strip()


def remove_urls(text: str) -> str:
    url_pattern = r"https?://\S+|www\.\S+"
    text = re.sub(url_pattern, "", text)
    return text


def remove_punctuations(text: str) -> str:
    punctuation_pattern = r"[^\w\s]"
    cleaned = re.sub(punctuation_pattern, "", text)
    return cleaned


def clean_text(text: str) -> str:
    text = lower(text)
    text = remove_urls(text)
    text = remove_punctuations(text)
    return text


def is_acceptable_len(text: str, l=15) -> bool:
    return len(text.split()) >= l


        inverted_dict: dict = {value: key for key, value in dct.items()}
        return inverted_dict
    
    def build_vocab(self, sentence_list: list[list[str]]) -> None:
        idx: int = max(self.itos.keys())
        word_frequencies: dict[str, int] = defaultdict(int)

        for sentence in sentence_list:
            for word in self.tokenize(sentence):
                word_frequencies[word] += 1
                if word not in self.stoi and word_frequencies[word] == self.freq_threshold:
                    idx += 1
                    self.stoi[word] = idx
                    self.itos[idx] = word
    
    def numericalize(self, text: str) -> list[int]:
        tokenized_txt: list[str] = self.tokenize(text)
        
        return [
            self.stoi[token] if token in self.stoi else self.stoi["<UNK>"]
            for token in tokenized_txt
        ]
        
    def textualize(self, vector: list[int]) -> list[str]:
        return [
            self.itos[i.item()] if i.item() in self.itos else self.itos[3]
            for i in vector
        ]
    


class FlickrDataset(Dataset):
    def __init__(self, images_dir: str, captions_file: str, freq_threshold: int = 5, transforms = None) -> None:
        super().__init__()
        self.images_dir: str = images_dir
        self.df: pd.DataFrame = pd.read_csv(captions_file)
        self.transforms = transforms
        
        # Get the image, captions
        self.images: pd.Series = self.df["image"]


        assert self.current
        if self.createMode == "ai_polygon":
            # convert points to polygon by an AI model
            assert self.current.shape_type == "points"
            points = self._ai_model.predict_polygon_from_points(
                points=[[point.x(), point.y()] for point in self.current.points],
                point_labels=self.current.point_labels,
            )
            self.current.setShapeRefined(
                points=[QtCore.QPointF(point[0], point[1]) for point in points],
                point_labels=[1] * len(points),
                shape_type="polygon",
            )
        elif self.createMode == "ai_mask":
            # convert points to mask by an AI model
            assert self.current.shape_type == "points"
            mask = self._ai_model.predict_mask_from_points(
                points=[[point.x(), point.y()] for point in self.current.points],
                point_labels=self.current.point_labels,
            )
            y1, x1, y2, x2 = imgviz.instances.masks_to_bboxes([mask])[0].astype(int)
            self.current.setShapeRefined(
                shape_type="mask",
                points=[QtCore.QPointF(x1, y1), QtCore.QPointF(x2, y2)],
                point_labels=[1, 1],
                mask=mask[y1 : y2 + 1, x1 : x2 + 1],
            )
        self.current.close()

        self.shapes.append(self.current)
        self.storeShapes()
        self.current = None
        self.setHiding(False)
        self.newShape.emit()
        self.update()

    def closeEnough(self, p1, p2):
        # d = distance(p1 - p2)
        # m = (p1-p2).manhattanLength()
        # print "d %.2f, m %d, %.2f" % (d, m, d - m)


    num_epochs = 100

    # for tensorboard
    writer = SummaryWriter("runs/flickr")
    step = 0

    # initialize model, loss etc
    model = CNNToRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)
    criterion = CrossEntropyLoss(ignore_index=dataset.vocabulary.stoi["<PAD>"])
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Only finetune the CNN
    for name, param in model.encoder_cnn.inception.named_parameters():
        if "fc.weight" in name or "fc.bias" in name:
            param.requires_grad = True
        else:
            param.requires_grad = False

    if load_model:
        step = load_checkpoint(torch.load("my_checkpoint.pth.tar"), model, optimizer)

    model.train()

    for epoch in range(num_epochs):
        # Uncomment the line below to see a couple of test cases
        print_examples(model, device, dataset)

        if save_model:
            checkpoint = {
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "step": step,
            }
            save_checkpoint(checkpoint)

        for idx, (imgs, captions) in tqdm(
            enumerate(train_loader), total=len(train_loader), leave=False
        ):
            imgs = imgs.to(device)
            captions = captions.to(device)


from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.vectorstores.chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate
from os import path
import json
from random import choices

data_dir = "data"
video_data_dir = "data"
transcribed_data = "transcriptions"
video_title = "iphone_15_marques_review"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")
persist_directory = path.join(data_dir, "vectore_store")

api_key: str = "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"
file_path: str = "comments.json"

with open(file_path, "r", encoding="utf-8") as f:
    all_comments: list[str] = json.load(fp=f)
    comments: list[str] = choices(population=all_comments, k=50)
    comments: list[Document] = [Document(page_content=comment) for comment in all_comments]

text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)
split_docs = text_splitter.split_documents(comments)

embeddings = OpenAIEmbeddings(api_key=api_key)
# vectordb = FAISS.from_texts(splits, embeddings)
# vectordb = FAISS.from_documents(documents=comments, embedding=embeddings)
# vectordb = Chroma.from_documents(
#     documents=split_docs,
#     embedding=embeddings,
#     persist_directory=persist_directory
# )

template_str: str = """


def send_email_local(user_email_address: str, message: str) -> None:
    pass

def send_email_aws_ses(user_email_address: str, message: str) -> None:
    pass

def send_account_activation_email(user_email_address: str, message: str) -> None:
    pass

def send_password_reset_email(user_email_address: str, message: str) -> None:
    pass

def generate_account_activation_email(message: str) -> None:
    pass

def generate_password_reset_email(message: str) -> None:
    pass



channel_file_path: str = "channels.json"
product: str = "iphone 15 pro"
channel: str = "Marques Brownlee"
video_file_path: str = "videos.json"
comments_file_path: str = "comments_1.json"
# save_data(file_path=channel_file_path, data=list(get_channels(product=product)))
# channels: list[dict] = load_data(file_path=channel_file_path)

# videos: list[dict] = get_videos(product=product, channel=channel)
# save_data(file_path=video_file_path, data=list(get_videos(product=product, channel=channel)))
# videos: list[dict] = load_data(file_path=video_file_path)

# save_data(file_path=comments_file_path, data=list(get_video_comments(video_id='cBpGq-vDr2Y', max_results=100)))
comments: list[dict] = load_data(file_path=comments_file_path)
console = Console()
batch: int = 10
from time import sleep
# with Live(create_comments_table(comments[:batch])) as live:
#     index: int = 0
#     for i in range(batch, len(comments)):
#         live.update(create_comments_table(comments[i: i+batch]))
#         sleep(0.1)
        
# from collections import deque
# queue = deque(maxlen=10, iterable=comments[:batch])
# with Live(create_comments_table(table_data=queue)) as live:
#     for data in comments[batch:]:
#         queue.append(data)
#         live.update(create_comments_table(table_data=queue))
#         sleep(0.5)
# from collections import deque
# queue = deque(maxlen=10)
# iterator = get_video_comments(video_id='cBpGq-vDr2Y', max_results=100)
# for _ in range(batch):
#     queue.append(next(iterator))
# with Live(create_comments_table(table_data=queue)) as live:
#     for data in iterator:
#         queue.append(data)


from dotenv import load_dotenv
load_dotenv()
import chainlit as cl
from farm_agent.agents import agent
from farm_agent.utils import load_model, evaluate_image
from PIL import Image
import io


user_location: str = None
user_name: str = None
welcome_text: str = """
Hello there. This is an application that helps farmers monitor the health level of their crops. 
Start by giving me your name and location, then upload an image of your crops. I will analyze it to 
determine the diasease or pest that affects it and then tell you how to deal with the pest or 
disease and where to purchase pesticides or fungicides.
"""

@cl.on_chat_start
async def start():
    cl.user_session.set("agent", agent)
    await cl.Message(content=welcome_text).send()
    user_name = await cl.AskUserMessage(content="What is your name?", timeout=120).send()
    user_location = await cl.AskUserMessage(content="Where are you from?", timeout=120).send()
    res = await cl.AskActionMessage(
        content="Would you like to determine if your crops are infected by a disease or by pests?",
        actions=[
            cl.Action(name="Check for diseases", value="diseases", label="‚úÖ Check for diseases"),
            cl.Action(name="Check for Pests", value="pests", label="‚ùå Check for Pests")
        ]
    ).send()
    if res and res.get("value") == "diseases":
        files = None
        # Wait for the user to upload a file
        while files == None:
            files = await cl.AskFileMessage(
                content=f"{user_name['content']}, start by uploading an image of your crop.", 
                accept=["image/jpeg", "image/png", "image/jpg"]
            ).send()
        # Decode the file


"""This module declares the app configuration.

The classes include:

BaseConfig:
    Has all the configurations shared by all the environments.

"""
import os

from dotenv import load_dotenv

load_dotenv()


class BaseConfig:
    """Base configuration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ["SECRET_KEY"]
    db_conn_string = 'sqlite:///./butterfly.db'
    SQLALCHEMY_DATABASE_URI = db_conn_string
    SQLALCHEMY_TRACK_MODIFICATIONS = False

    def removeItem(self, item):
        index = self.model().indexFromItem(item)
        self.model().removeRows(index.row(), 1)

    def selectItem(self, item):
        index = self.model().indexFromItem(item)
        self.selectionModel().select(index, QtCore.QItemSelectionModel.Select)

    def findItemByShape(self, shape):
        for row in range(self.model().rowCount()):
            item = self.model().item(row, 0)
            if item.shape() == shape:
                return item
        raise ValueError("cannot find shape: {}".format(shape))

    def clear(self):
        self.model().clear()


                "elementProperties": {
                    "pageObjectId": page_id,
                    "size": {"height": emu4M, "width": emu4M},
                    "transform": {
                        "scaleX": 1,
                        "scaleY": 1,
                        "translateX": 100000,
                        "translateY": 100000,
                        "unit": "EMU",
                    },
                },
            }
        }
    )

    # Execute the request.
    body = {"requests": requests}
    response = (
        slide_client.presentations()
        .batchUpdate(presentationId=presentation_id, body=body)
        .execute()
    )
    create_image_response = response.get("replies")[0].get("createImage")
    print(f"Created image with ID: {(create_image_response.get('objectId'))}")

    return response
  except HttpError as error:
    print(f"An error occurred: {error}")
    print("Images not created")
    return error


from sqlalchemy import create_engine, Column, Table, ForeignKey, MetaData
from sqlalchemy.orm import relationship
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import (
    Integer, String, Date, DateTime, Float, Boolean, Text)
from scrapy.utils.project import get_project_settings
from sqlalchemy_utils import ScalarListType


Base = declarative_base()

def db_connect():
    """
    Performs database connection using database settings from settings.py.
    Returns sqlalchemy engine instance
    """
    settings: dict = get_project_settings()
    connection_string: str = settings.get("CONNECTION_STRING")
    return create_engine(connection_string)

def create_table(engine):
    Base.metadata.create_all(engine)
    
    
slide_tag = Table('slide_tag', Base.metadata,
    Column('slide_id', Integer, ForeignKey('slide.id')),
    Column('tag_id', Integer, ForeignKey('tag.id'))
)

class Slide(Base):
    __tablename__ = "slide"

    id = Column(String(), primary_key=True)
    title = Column('title', String())
    description = Column('description', Text())
    category_id = Column(String(), ForeignKey('category.id'))
    tags = relationship('Tag', secondary='slide_tag',
        lazy='dynamic', backref="slide")  # M-to-M for quote and tag
    colors = Column(ScalarListType())


# print(youtube.find_channel_by_name('The Joy Ride'))
# print(youtube.find_channel_playlists('UCCjULCQvh2cQQLzYe4DC2Nw'))
# print(youtube.find_my_playlists())
# snippet: CreatePlaylistSnippet = CreatePlaylistSnippet(
#     title='Another Test Playlist 5'
# )
# playlist_schema: CreatePlaylistSchema = CreatePlaylistSchema(
#     snippet=snippet
# )
# print(youtube.insert_playlist(playlist_schema))
# snippet: CreatePlaylistSnippet = CreatePlaylistSnippet(
#     title='Another Test Playlist 6',
#     description='New description.',
#     defaultLanguage='en'
# )
# playlist_schema: CreatePlaylistSchema = CreatePlaylistSchema(
#     snippet=snippet,
#     status=CreateStatus(privacyStatus='public')
# )
# print(youtube.update_playlist('PL_26vmg8W_AfJWy6SVtoSmtYWimhexwF7', playlist_schema))
# print(youtube.delete_playlist('PL_26vmg8W_AfeubZM4lQJiBU8UbCl-R3L'))
# print(youtube.find_channel_by_name('Isaac Author'))
# print(youtube.find_playlist_items('PLgCR4dyaQRlq0SM6y7cQqspDnfqiNHfWf'))
# print(youtube.find_playlist_items_by_ids(['UExnQ1I0ZHlhUVJscTBTTTZ5N2NRcXNwRG5mcWlOSGZXZi5EMEEwRUY5M0RDRTU3NDJC', 
#                         'UExnQ1I0ZHlhUVJscTBTTTZ5N2NRcXNwRG5mcWlOSGZXZi45NDk1REZENzhEMzU5MDQz', 
#                         'UExnQ1I0ZHlhUVJscTBTTTZ5N2NRcXNwRG5mcWlOSGZXZi41MzJCQjBCNDIyRkJDN0VD']))

# video_resource: VideoResourceId = VideoResourceId(videoId='j0OvCL-6ic4')
# snippet: CreatePlaylistItemSnippet = CreatePlaylistItemSnippet(
#     playlistId='PL_26vmg8W_AfJWy6SVtoSmtYWimhexwF7',
#     resourceId=video_resource
# )
# create = CreatePlaylistItem(snippet=snippet)
# print(youtube.insert_playlist_item(create))
# playlist_id='PL_26vmg8W_AfJWy6SVtoSmtYWimhexwF7', 
# playlist_item_id='UExfMjZ2bWc4V19BZkpXeTZTVnRvU210WVdpbWhleHdGNy41MzJCQjBCNDIyRkJDN0VD'
# video_id='j0OvCL-6ic4' 
# position=6
# print(youtube.update_playlist_item(playlist_id, playlist_item_id, video_id, position))
# print(youtube.delete_playlist_item('UExnQ1I0ZHlhUVJsclpDZEtoN2ZQb1FwejVGYXI3Xy1HSS4yODlGNEE0NkRGMEEzMEQy'))


#         AttendeeSchema(email='sbrin@example.com')
#     ],
#     reminders=RemindersSchema(
#         useDefault=False,
#         overrides=[
#             ReminderSchema(method='email', minutes=10),
#             ReminderSchema(method='popup', minutes=20)
#         ]
#     )
# )
# created_event = google_calendar.create_event(create_event)
# print(created_event)
today = d.today()
year = today.year
month = today.month
day = today.day
time_min: datetime = datetime(year=year, month=month, day=25, hour=0, minute=1)
time_min: datetime = datetime.astimezone(time_min, pytz.timezone('Africa/Nairobi'))
time_min: str = str(time_min.strftime("%Y-%m-%dT%H:%M:%S.%fZ"))
time_max: datetime = datetime(year=year, month=month, day=day, hour=23, minute=59)
time_max: datetime = datetime.astimezone(time_max, pytz.timezone('Africa/Nairobi'))
time_max: str = str(time_max.strftime("%Y-%m-%dT%H:%M:%S.%fZ"))
req = ListCalendarEvents(
    timeMin=time_min
)
events = google_calendar.list_calendar_events(req)
print(events.items)
def parse_events(events: list[Event]) -> str:
    event_str: str = ""
    for i, event in enumerate(events, start=1):
        event_str: str = f"\n{i}. "
        start_time: str = event.start.date_time.time()
        end_time: str = event.end.date_time.time()
        title: str = event.summary
        location: str = event.location
        event_str += f'From {start_time} to {end_time} you will {title} at {location}.'
    return event_str
# print(time_min)
# print(time_max)
# print(datetime.astimezone(datetime.now(), pytz.timezone('Africa/Nairobi')))


    transforms,
    batch_size: int = 8,
    num_workers: int = 2,
    shuffle: bool = True,
    pin_memory: bool = True,
    ) -> DataLoader:
    dataset: Dataset = FlickrDataset(images_dir=images_dir, captions_file=captions_file, transforms=transforms)
    pad_idx: int = dataset.vocabulary.stoi["<PAD>"]
    loader: DataLoader = DataLoader(
        dataset=dataset, 
        batch_size=batch_size, 
        shuffle=shuffle, 
        num_workers=num_workers, 
        pin_memory=pin_memory, 
        collate_fn=MyCollate(pad_idx=pad_idx)
    )
    return loader, dataset


dataloader: DataLoader = get_loader(
    images_dir="raw-data/Images",
    captions_file="raw-data/captions.txt",
    transforms=Compose([
        Resize((299, 299)),
        ToTensor()
    ])
)

# for i, (images, captions) in enumerate(dataloader):
#     print(images.shape)
#     print(captions.shape)
#     break
# print(captions[0])
# dataset: Dataset = FlickrDataset(
#     images_dir="raw-data/Images", 
#     captions_file="raw-data/captions.txt", 
#     transforms=Compose([
#         Resize((224, 224)),
#         ToTensor()
#     ]))


def save_post_photo_aws_s3(post_image: dict) -> None:
    """Save the uploadeded post image."""
    file: FileStorage = post_image['post_image']
    if file and allowed_file(file.filename):
        filename = f'{secrets.token_hex(8)}.{get_file_extension(file.filename)}'
        # Use celery task
        return filename
    return ''

def no_save_post_photo(post_image: dict) -> None:
    """Save the uploadeded post image."""
    file: FileStorage = post_image['post_image']
    if file and allowed_file(file.filename):
        filename = f'{secrets.token_hex(8)}.{get_file_extension(file.filename)}'
        return filename
    return ''


def save_post_photo(post_image: dict, save_location: str = '') -> str:
    """Save the uploadeded post image."""
    save_photo_funcs: dict[str, Callable[[dict], str]] = {
        'locally': save_post_photo_locally,
        'aws_s3': save_post_photo_aws_s3,
        'default': no_save_post_photo
    }
    if save_photo_funcs.get(save_location):
        filename: str = save_photo_funcs[save_location](post_image)
    else:
        filename: str = save_photo_funcs['default'](post_image)
    return filename

from youtube import YouTube
from youtube.models import Channel
from youtube.resources.schemas import YouTubeResponse


client_secret_file: str = 'client_secret.json'
youtube: YouTube = YouTube(client_secret_file=client_secret_file)
youtube.authenticate()

channel_name: str = 'Ticker Symbol You'
search_response: YouTubeResponse = youtube.find_channel_by_name(channel_name)
print(search_response.items[0])

        print("Generating dataset from:", filename)

        label_file = labelme.LabelFile(filename=filename)

        base = osp.splitext(osp.basename(filename))[0]
        out_img_file = osp.join(args.output_dir, "JPEGImages", base + ".jpg")
        out_clsp_file = osp.join(args.output_dir, "SegmentationClass", base + ".png")
        if not args.nonpy:
            out_cls_file = osp.join(
                args.output_dir, "SegmentationClassNpy", base + ".npy"
            )
        if not args.noviz:
            out_clsv_file = osp.join(
                args.output_dir,
                "SegmentationClassVisualization",
                base + ".jpg",
            )
        if not args.noobject:
            out_insp_file = osp.join(
                args.output_dir, "SegmentationObject", base + ".png"
            )
            if not args.nonpy:
                out_ins_file = osp.join(
                    args.output_dir, "SegmentationObjectNpy", base + ".npy"
                )
            if not args.noviz:
                out_insv_file = osp.join(
                    args.output_dir,
                    "SegmentationObjectVisualization",
                    base + ".jpg",
                )

        img = labelme.utils.img_data_to_arr(label_file.imageData)
        imgviz.io.imsave(out_img_file, img)

        cls, ins = labelme.utils.shapes_to_label(
            img_shape=img.shape,
            shapes=label_file.shapes,
            label_name_to_value=class_name_to_id,
        )


    GOOGLE_OAUTH_CLIENT_SECRET = os.environ.get("GOOGLE_OAUTH_CLIENT_SECRET")
    OAUTHLIB_INSECURE_TRANSPORT = os.environ.get("OAUTHLIB_INSECURE_TRANSPORT")
    OAUTHLIB_RELAX_TOKEN_SCOPE = os.environ.get("OAUTHLIB_RELAX_TOKEN_SCOPE " )


class DevelopmentConfig(BaseConfig):
    """Development confuguration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ.get(
        "SECRET_KEY", "df0331cefc6c2b9a5d0208a726a5d1c0fd37324feba25506"
    )


class TestingConfig(BaseConfig):
    """Testing configuration."""

    TESTING = True
    SECRET_KEY = os.environ.get("SECRET_KEY", "secret-key")


class ProductionConfig(BaseConfig):
    """Production configuration."""

    TESTING = False
    SECRET_KEY = os.environ.get("SECRET_KEY", "secret-key")


Config = {
    "development": DevelopmentConfig,
    "test": TestingConfig,
    "production": ProductionConfig,
    "staging": ProductionConfig,
}



    def labelSelectionChanged(self):
        if self._noSelectionSlot:
            return
        if self.canvas.editing():
            selected_shapes = []
            for item in self.labelList.selectedItems():
                selected_shapes.append(item.shape())
            if selected_shapes:
                self.canvas.selectShapes(selected_shapes)
            else:
                self.canvas.deSelectShape()

    def labelItemChanged(self, item):
        shape = item.shape()
        self.canvas.setShapeVisible(shape, item.checkState() == Qt.Checked)

    def labelOrderChanged(self):
        self.setDirty()
        self.canvas.loadShapes([item.shape() for item in self.labelList])

    # Callback functions:

    def newShape(self):
        """Pop-up and give focus to the label editor.

        position MUST be in global coordinates.
        """
        items = self.uniqLabelList.selectedItems()
        text = None
        if items:
            text = items[0].data(Qt.UserRole)
        flags = {}
        group_id = None
        description = ""
        if self._config["display_label_popup"] or not text:
            previous_text = self.labelDialog.edit.text()
            text, flags, group_id, description = self.labelDialog.popUp(text)
            if not text:
                self.labelDialog.edit.setText(previous_text)


that can be used during the creation of a youtube video that reviews the product for users. Only 
return a JSON object with the key ``features``.
Product: {product}
"""

product_features_description_template: str = """
You will be provided by a list of product features as well as a product name. Your task is to 
provide a detailed description of each feature. Only return a JSON object with each feature as 
a key.
Product Name: {product}
Product features: {features}
"""

features_covered_template_str: str = """
You will be provided with the transcript for a youtube video that reviews a product. The product name 
will also be provided as well as the features of the given product. Your task will be to find all the 
features of the product that are found in the feature list and are also covered in the transcript. For 
each feature covered in the transcript, provide a short summary of what is covered. Only consider the 
features in the feature list. Only return a valid JSON object with each feature as a key and a short 
summary.
Product: {product}
Features: {features}
Transcript: {transcript}
"""


features_template = PromptTemplate(
    template=product_features_template_str, input_variables=["product"]
)
features_description_template = PromptTemplate(
    template=product_features_description_template,
    input_variables=["product", "features"],
)
features_covered_template = PromptTemplate(
    template=features_covered_template_str, input_variables=["product", "transcript", "features"]
)

product: str = "iphone 13 pro"
features: list[str] = [
    "5G connectivity",


    )

    label_name_to_value = {"_background_": 0}
    for shape in shapes:
        label_name = shape["label"]
        if label_name in label_name_to_value:
            label_value = label_name_to_value[label_name]
        else:
            label_value = len(label_name_to_value)
            label_name_to_value[label_name] = label_value

    lbl, _ = shapes_to_label(img_shape, shapes, label_name_to_value)
    return lbl, label_name_to_value


def masks_to_bboxes(masks):
    if masks.ndim != 3:
        raise ValueError("masks.ndim must be 3, but it is {}".format(masks.ndim))
    if masks.dtype != bool:
        raise ValueError(
            "masks.dtype must be bool type, but it is {}".format(masks.dtype)
        )
    bboxes = []
    for mask in masks:
        where = np.argwhere(mask)
        (y1, x1), (y2, x2) = where.min(0), where.max(0) + 1
        bboxes.append((y1, x1, y2, x2))
    bboxes = np.asarray(bboxes, dtype=np.float32)
    return bboxes


from youtube import YouTube


client_secrets_file = '/home/lyle/Downloads/search.json'
youtube_client = YouTube(client_secret_file=client_secrets_file)
youtube_client_object = youtube_client.authenticate()
youtube_client.youtube_client = youtube_client_object

# Scrapy settings for slidesgo project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = "slidesgo"

SPIDER_MODULES = ["slidesgo.spiders"]
NEWSPIDER_MODULE = "slidesgo.spiders"


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = "slidesgo (+http://www.yourdomain.com)"

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {


from ..database.crud.user import get_random_user
from ..database.crud.post import get_posts
from ..database.crud.like import list_post_likes, get_key_like, has_liked
from ..database.crud.comment import get_key_comment, list_post_comments
from ..database.crud.bookmark import has_bookmarked
from ..database.database import get_db
from ..database.models import (
    User, Post, Like, Bookmark, Comment
)
from ..database.schemas.post import (
    GetPosts, PostAuthor, GetPost, PostLike, KeyComment, PostSchema
)
from ..database.schemas.activity import CreateActivity
from flask import url_for
from pydantic import ValidationError
from sqlalchemy.exc import OperationalError, IntegrityError
from datetime import datetime

from typing import Generator, Optional, Iterator
from pydantic import BaseModel, Field
from youtube import YouTube
from youtube.models import Channel, Comment, Search, Video
from youtube.schemas import (SearchOptionalParameters, SearchPart,
                             YouTubeListResponse, YouTubeRequest,
                             YouTubeResponse)
import json
from rich.table import Table
from rich.console import Console
from rich import box
from rich.live import Live


client_secrets_file = "/home/lyle/Downloads/search.json"
youtube_client = YouTube(client_secret_file=client_secrets_file)
youtube_client_object = youtube_client.authenticate()
youtube_client.youtube_client = youtube_client_object


def get_channel_id(channel_name: str) -> str:
    """Get the channel id."""
    response: YouTubeResponse = youtube_client.find_channel_by_name(channel_name)
    search_result: Search = response.items[0]
    return search_result.resource_id


def search_youtube_channels(product: str, max_results: int = 5) -> list[Search]:
    search_part: SearchPart = SearchPart()
    query: str = f"latest {product} review"
    optional_params: SearchOptionalParameters = SearchOptionalParameters(
        q=query,
        maxResults=max_results,
        type=["channel"],
    )
    search_schema: YouTubeRequest = YouTubeRequest(
        part=search_part, optional_parameters=optional_params
    )
    response: YouTubeResponse = youtube_client.search(search_schema)
    items: list[Search] = response.items


from typing import Optional

from pydantic import BaseModel, Field


class AgentState(BaseModel):
    product: Optional[str] = Field(
        description="The product being reviewed", default=None
    )
    channel: Optional[str] = Field(description="The youtube channel used", default=None)
    video: Optional[str] = Field(description="The video being reviewed", default=None)
    transcript: Optional[str] = Field(description="The video transcript", default=None)
    comments: Optional[list[str]] = Field(
        description="The video comments", default_factory=list
    )
    summary: Optional[str] = Field(description="The video summary", default=None)
    analysis_summary: Optional[str] = Field(
        description="The video analysis_summary", default=None
    )
    features: Optional[str] = Field(
        description="The product features reviewed", default=None
    )
    questions: Optional[list[str]] = Field(
        description="The questions that the analyst had", default_factory=list
    )


    logging.info('Request authenticated.')
    return youtube


def create_playlist(title: str, 
                    description: str, 
                    youtube: YouTube,
                    default_language: str = 'en', 
                    privacy_status: str = 'public') -> Playlist:
    snippet: CreatePlaylistSnippet = CreatePlaylistSnippet(
        title=title,
        description=description,
        defaultLanguage=default_language
    )
    create_schema: CreatePlaylistSchema = CreatePlaylistSchema(
        snippet=snippet,
        status=CreateStatus(privacyStatus=privacy_status)
    )
    playlist: Playlist = youtube.insert_playlist(create_schema)
    return playlist


def get_playlists() -> dict[str, dict]:
    playlists: dict[str, str] = {
        'Daily Videos': {
            'id': 'PL_26vmg8W_AcEEl_Bo2AhziS-93r6b8bu',
            'url': 'https://www.youtube.com/playlist?list=PL_26vmg8W_AcEEl_Bo2AhziS-93r6b8bu'
        }
    }
    return playlists


def get_playlist_id(name: str) -> str:
    playlists: dict[str, dict] = get_playlists()
    return playlists[name]['id']
    
    
def get_playlist_url(name: str) -> str:
    playlists: dict[str, dict] = get_playlists()
    return playlists[name]['url']



        # Application state.
        self.image = QtGui.QImage()
        self.imagePath = None
        self.recentFiles = []
        self.maxRecent = 7
        self.otherData = None
        self.zoom_level = 100
        self.fit_window = False
        self.zoom_values = {}  # key=filename, value=(zoom_mode, zoom_value)
        self.brightnessContrast_values = {}
        self.scroll_values = {
            Qt.Horizontal: {},
            Qt.Vertical: {},
        }  # key=filename, value=scroll_value

        if filename is not None and osp.isdir(filename):
            self.importDirImages(filename, load=False)
        else:
            self.filename = filename

        if config["file_search"]:
            self.fileSearch.setText(config["file_search"])
            self.fileSearchChanged()

        # XXX: Could be completely declarative.
        # Restore application settings.
        self.settings = QtCore.QSettings("labelme", "labelme")
        self.recentFiles = self.settings.value("recentFiles", []) or []
        size = self.settings.value("window/size", QtCore.QSize(600, 500))
        position = self.settings.value("window/position", QtCore.QPoint(0, 0))
        state = self.settings.value("window/state", QtCore.QByteArray())
        self.resize(size)
        self.move(position)
        # or simply:
        # self.restoreGeometry(settings['window/geometry']
        self.restoreState(state)

        # Populate the File menu dynamically.
        self.updateFileMenu()


from .google_drive import GoogleDrive


@post.route("/create", methods=["POST", "GET"])
def create_new_post():
    """Create a new post."""
    if request.method == 'GET':
        return {'success': 'post creation form'}, HTTPStatus.OK
    elif request.method == 'POST':
        try:
            post_data = CreatePost(**request.form) 
        except ValidationError:
            return {'Error': 'The data provided is invalid or incomplete!'}, HTTPStatus.BAD_REQUEST
        try:
            user_data = GetUser(user_id=post_data.author_id)
            user = get_user(session=get_db, user_data=user_data)
            if not user:
                return {'Error': f'User with id {user_data.user_id} does not exists'}, HTTPStatus.NOT_FOUND 
            post: Post = create_post(post_data=post_data, post_image=request.files, session=get_db) 
        except (OperationalError, IntegrityError) as e:
            print(e)
            # Send email to
            return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
        resp = CreatedPost(
            id=post.id,
            location=post.location,
            text=post.text,
            image_url=post.image_url,
            author_id=post.author_id,
            date_published=post.date_published
        )
        return resp.model_dump_json(indent=4), HTTPStatus.CREATED

            "insertText": {
                "objectId": element_id,
                "insertionIndex": 0,
                "text": "New Box Text Inserted!",
            }
        },
    ]

    # Execute the request.
    body = {"requests": requests}
    response = (
        slide_client.presentations()
        .batchUpdate(presentationId=presentation_id, body=body)
        .execute()
    )
    create_shape_response = response.get("replies")[0].get("createShape")
    print(f"Created textbox with ID:{(create_shape_response.get('objectId'))}")
  except HttpError as error:
    print(f"An error occurred: {error}")

    return error

  return response


def create_image(presentation_id: str, page_id: str, slide_client: Any):
  try:
    IMAGE_URL = (
        "https://www.google.com/images/branding/"
        "googlelogo/2x/googlelogo_color_272x92dp.png"
    )
    # pylint: disable=invalid-name
    requests = []
    image_id = "MyImage_11"
    emu4M = {"magnitude": 4000000, "unit": "EMU"}
    requests.append(
        {
            "createImage": {
                "objectId": image_id,
                "url": IMAGE_URL,


    def changeOutputDirDialog(self, _value=False):
        default_output_dir = self.output_dir
        if default_output_dir is None and self.filename:
            default_output_dir = osp.dirname(self.filename)
        if default_output_dir is None:
            default_output_dir = self.currentPath()

        output_dir = QtWidgets.QFileDialog.getExistingDirectory(
            self,
            self.tr("%s - Save/Load Annotations in Directory") % __appname__,
            default_output_dir,
            QtWidgets.QFileDialog.ShowDirsOnly
            | QtWidgets.QFileDialog.DontResolveSymlinks,
        )
        output_dir = str(output_dir)

        if not output_dir:
            return

        self.output_dir = output_dir

        self.statusBar().showMessage(
            self.tr("%s . Annotations will be saved/loaded in %s")
            % ("Change Annotations Dir", self.output_dir)
        )
        self.statusBar().show()

        current_filename = self.filename
        self.importDirImages(self.lastOpenDir, load=False)

        if current_filename in self.imageList:
            # retain currently selected file
            self.fileListWidget.setCurrentRow(self.imageList.index(current_filename))
            self.fileListWidget.repaint()

    def saveFile(self, _value=False):
        assert not self.image.isNull(), "cannot save empty image"
        if self.labelFile:
            # DL20180323 - overwrite when in directory
            self._saveFile(self.labelFile.filename)


        ins[cls == -1] = 0  # ignore it.

        # class label
        labelme.utils.lblsave(out_clsp_file, cls)
        if not args.nonpy:
            np.save(out_cls_file, cls)
        if not args.noviz:
            clsv = imgviz.label2rgb(
                cls,
                imgviz.rgb2gray(img),
                label_names=class_names,
                font_size=15,
                loc="rb",
            )
            imgviz.io.imsave(out_clsv_file, clsv)

        if not args.noobject:
            # instance label
            labelme.utils.lblsave(out_insp_file, ins)
            if not args.nonpy:
                np.save(out_ins_file, ins)
            if not args.noviz:
                instance_ids = np.unique(ins)
                instance_names = [str(i) for i in range(max(instance_ids) + 1)]
                insv = imgviz.label2rgb(
                    ins,
                    imgviz.rgb2gray(img),
                    label_names=instance_names,
                    font_size=15,
                    loc="rb",
                )
                imgviz.io.imsave(out_insv_file, insv)


if __name__ == "__main__":
    main()


@staticmethod
    def hash_password(password: str) -> str:
        return bcrypt.generate_password_hash(password).decode("utf-8")

    def check_password(self, password: str) -> bool:
        return bcrypt.check_password_hash(self.password, password)
    
    @staticmethod
    def encode_auth_token(user_id: int):
        try:
            payload = {
                "exp": datetime.utcnow() + timedelta(days=0, hours=2),
                "iat": datetime.utcnow(),
                "sub": user_id,
            }
            return jwt.encode(payload, current_app.config.get("SECRET_KEY"), algorithm="HS256")
        except Exception as e:
            return e
        
    @staticmethod
    def decode_auth_token(auth_token: str):
        try:
            payload = jwt.decode(auth_token, current_app.config.get("SECRET_KEY"), algorithms="HS256")
            return payload["sub"]
        except (ExpiredSignatureError, InvalidTokenError) as e:
            raise e

            (self._image_embedding,) = self._encoder_session.run(
                output_names=None,
                input_feed={"batched_images": batched_images},
            )
            if len(self._image_embedding_cache) > 10:
                self._image_embedding_cache.popitem(last=False)
            self._image_embedding_cache[self._image.tobytes()] = self._image_embedding
            logger.debug("Done computing image embedding.")

    def _get_image_embedding(self):
        if self._thread is not None:
            self._thread.join()
            self._thread = None
        with self._lock:
            return self._image_embedding

    def predict_mask_from_points(self, points, point_labels):
        return _compute_mask_from_points(
            decoder_session=self._decoder_session,
            image=self._image,
            image_embedding=self._get_image_embedding(),
            points=points,
            point_labels=point_labels,
        )

    def predict_polygon_from_points(self, points, point_labels):
        mask = self.predict_mask_from_points(points=points, point_labels=point_labels)
        return _utils.compute_polygon_from_mask(mask=mask)


def _compute_mask_from_points(
    decoder_session, image, image_embedding, points, point_labels
):
    input_point = np.array(points, dtype=np.float32)
    input_label = np.array(point_labels, dtype=np.float32)

    # batch_size, num_queries, num_points, 2
    batched_point_coords = input_point[None, None, :, :]
    # batch_size, num_queries, num_points
    batched_point_labels = input_label[None, None, :]


from dotenv import load_dotenv
load_dotenv()
import chainlit as cl
from farm_agent.agents import agent
from farm_agent.utils import load_model, evaluate_image
from PIL import Image
import io


user_location: str = None
user_name: str = None
welcome_text: str = """
Hello there. This is an application that helps farmers monitor the health level of their crops. 
Start by giving me your name and location, then upload an image of your crops. I will analyze it to 
determine the diasease or pest that affects it and then tell you how to deal with the pest or 
disease and where to purchase pesticides or fungicides.
"""

@cl.on_chat_start
async def start():
    cl.user_session.set("agent", agent)
    await cl.Message(content=welcome_text).send()
    user_name = await cl.AskUserMessage(content="What is your name?", timeout=120).send()
    user_location = await cl.AskUserMessage(content="Where are you from?", timeout=120).send()
    res = await cl.AskActionMessage(
        content="Would you like to determine if your crops are infected by a disease or by pests?",
        actions=[
            cl.Action(name="Check for diseases", value="diseases", label="‚úÖ Check for diseases"),
            cl.Action(name="Check for Pests", value="pests", label="‚ùå Check for Pests")
        ]
    ).send()
    if res and res.get("value") == "diseases":
        files = None
        # Wait for the user to upload a file
        while files == None:
            files = await cl.AskFileMessage(
                content=f"{user_name['content']}, start by uploading an image of your crop.", 
                accept=["image/jpeg", "image/png", "image/jpg"]
            ).send()
        # Decode the file


def handle_unsupported_media_type(exeption: Exception) -> Response:
    """Handle all unsupported media type errors.

    This method is called when a a request does not supply the data or the data supplied is
    invalid.

    Parameters
    ----------
    exception: Exception
        The exception that was raised. This is a subclass of Exception.

    Returns
    -------
    Response:
        A string consiting of json data and response code.
    """
    return make_response(jsonify({"error": str(exeption)}), HTTPStatus.UNSUPPORTED_MEDIA_TYPE)


def register_error_handlers(app: Flask) -> None:
    """Register the error handlers.

    Parameters
    ----------
    app: flask.Flask
        The Flask app instance.
    """
    app.register_error_handler(HTTPStatus.NOT_FOUND, handle_resource_not_found)
    app.register_error_handler(HTTPStatus.METHOD_NOT_ALLOWED, handle_method_not_allowed)
    app.register_error_handler(HTTPStatus.INTERNAL_SERVER_ERROR, handle_internal_server_error)
    app.register_error_handler(HTTPStatus.UNSUPPORTED_MEDIA_TYPE, handle_unsupported_media_type)

from .view import code


# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = "httpcache"
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"

# Set settings whose default value is deprecated to a future-proof value
REQUEST_FINGERPRINTER_IMPLEMENTATION = "2.7"
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
FEED_EXPORT_ENCODING = "utf-8"


@post.route("/likes", methods=["GET"])
def get_post_likes():
    """Get a posts likes."""
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post: Post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        likes: list[Bookmark] = list_post_likes(session=get_db, post_data=post_data)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    resp = [
        ActivityCreated(
            user_id=like.author_id,
            post_id=like.post_id,
            date_created=like.like_date
        ).model_dump()
        for like in likes
    ]
    return resp, HTTPStatus.OK

from qtpy import QtWidgets
from qtpy.QtCore import Qt


class EscapableQListWidget(QtWidgets.QListWidget):
    def keyPressEvent(self, event):
        super(EscapableQListWidget, self).keyPressEvent(event)
        if event.key() == Qt.Key_Escape:
            self.clearSelection()


from os import path

from langchain.chains import StuffDocumentsChain
from langchain.chains.llm import LLMChain
from langchain.llms.base import BaseLLM
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.text import TextLoader
from langchain_openai import ChatOpenAI, OpenAI
from langchain.docstore.document import Document

with open('analysis.json', 'r') as f:
    import json
    analy = json.load(f)

api_key: str = "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"

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)


template_str: str = """
You are provided with the coments of various users to the review of the {product}

Please provide a detailed summary of the users comments. Make sure to identify the features 
that the users loved as well as those that they hated.
comments: {comments}
detailed summary: 
"""

product: str = "Apple vision pro"
template = PromptTemplate.from_template(template_str)
template = template.partial(product=product)

api_key: str = "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"
chat: BaseLLM = ChatOpenAI(temperature=0, api_key=api_key)
llm: BaseLLM = OpenAI(temperature=0, api_key=api_key)

llm_chain = LLMChain(llm=chat, prompt=template)
stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="comments")



        return train_config


def get_video_comments(video_id: str, max_results: Optional[int] = 10) -> Generator:
    """List a given videos comments"""
    comment_iterator: Iterator = youtube_client.get_comments_iterator(video_id=video_id)
    done: bool = False
    comment_count: int = 0
    for comment_threads in comment_iterator:
        if done:
            break
        for comment_thread in comment_threads:
            comment: Comment = comment_thread.snippet.top_level_comment
            comment = parse_comment(comment=comment)
            # comment = Data(
            #     id=comment["comment_id"],
            #     comment=comment["comment"],
            #     likes=comment["likes"],
            #     date=comment["date_published"],
            # )
            yield comment
            comment_count += 1
            if comment_count > max_results:
                done = True
                break
            
            
def create_comments_table(table_data: list[dict]) -> Table:
    table: Table = Table(row_styles=["dim", ""],leading=1, box=box.MINIMAL_DOUBLE_HEAD)
    table.add_column(header="[b]Comment Id", justify="left", style="dark_orange")
    table.add_column(header="Comment", justify="left", style="light_coral")
    table.add_column(header="[b]Likes", justify="left", style="yellow2")
    table.add_column(header="Date", justify="center", style="violet")
    table.columns[0].header_style = "bold chartreuse1"
    table.columns[1].header_style = "bold dark_goldenrod"
    table.columns[2].header_style = "bold chartreuse1"
    table.columns[3].header_style = "bold dark_goldenrod"
    table.border_style = "bright_yellow"
    table.pad_edge = True
    for row in table_data:
        table.add_row(row["comment_id"], row["comment"], str(row["likes"]), row["date_published"])
    return table


        packages=find_packages(),
        description="Image Polygonal Annotation with Python",
        long_description=get_long_description(),
        long_description_content_type="text/markdown",
        author="Kentaro Wada",
        author_email="www.kentaro.wada@gmail.com",
        url="https://github.com/wkentaro/labelme",
        install_requires=get_install_requires(),
        license="GPLv3",
        keywords="Image Annotation, Machine Learning",
        classifiers=[
            "Development Status :: 5 - Production/Stable",
            "Intended Audience :: Developers",
            "Intended Audience :: Science/Research",
            "Natural Language :: English",
            "Operating System :: OS Independent",
            "Programming Language :: Python",
            "Programming Language :: Python :: 3.5",
            "Programming Language :: Python :: 3.6",
            "Programming Language :: Python :: 3.7",
            "Programming Language :: Python :: 3.8",
            "Programming Language :: Python :: 3.9",
            "Programming Language :: Python :: 3 :: Only",
        ],
        package_data={"labelme": ["icons/*", "config/*.yaml", "translate/*"]},
        entry_points={
            "console_scripts": [
                "labelme=labelme.__main__:main",
                "labelme_draw_json=labelme.cli.draw_json:main",
                "labelme_draw_label_png=labelme.cli.draw_label_png:main",
                "labelme_json_to_dataset=labelme.cli.json_to_dataset:main",
                "labelme_export_json=labelme.cli.export_json:main",
                "labelme_on_docker=labelme.cli.on_docker:main",
            ],
        },
    )


if __name__ == "__main__":
    main()


        search_resp: Search = search_responses[0]
        video: Video = Video(**search_resp.model_dump(exclude={'thumbnails'}))
        video_str: str = dumps(video.dict(), default=str)
        logging.info('caching the retrieved video.')
        redis.set(name=f'latest:{channel_id}', value=video_str)
        logging.info('Successfully cached the retrieved video.')
        logging.info('Setting an expiration time of %d for the cached video.', config.expiration_time)
        redis.setex(f'latest:{channel_id}', value=video_str, time=timedelta(seconds=config.expiration_time))
    return video

def add_video_to_playlist(video: Video, 
                          playlist_id: str, 
                          youtube: YouTube, 
                          position: int = 0) -> PlaylistItem | None:
    playlist_item: PlaylistItem = None
    if redis.setnx(name=f'{playlist_id}:{video.resource_id}', value=video.resource_id):
        logging.info('The video "%s" odes not exist in playlist, adding it.', video.title)
        resource_id: VideoResourceId = VideoResourceId(videoId=video.resource_id)
        snippet: CreatePlaylistItemSnippet = CreatePlaylistItemSnippet(
            playlistId=playlist_id,
            resourceId=resource_id,
            position=position
        )
        create: CreatePlaylistItem = CreatePlaylistItem(snippet=snippet)
        logging.info('Inserting "%s" into playlist.', video.title)
        playlist_item: PlaylistItem = youtube.insert_playlist_item(create)
        logging.info('Inserted "%s" to playlist.', video.title)
        logging.info('Adding "%s" to "unwatched list".', video.title)
        video.save()
        logging.info('Added "%s" to "unwatched list".', video.title)
        logging.info('Setting the video expiration time of %d for video.', config.expiration_time)
        video.expire(num_seconds=3600)
    return playlist_item


def delete_video_playlist(playlist_item_id: str, youtube: YouTube) -> None:
    youtube.delete_playlist_item(playlist_item_id)
    
    
def workflow(youtube: YouTube, channel_names: list[str], playlist_name: str = 'Daily Videos'):




class ProductFeatures(BaseModel):
    display: str
    design: str
    perfomance: str
    camera: str


@tool(args_schema=UserQuery)
def google_search_tool(query: str, result_count: int) -> str:
    """Search for the latest information from the web using Google."""
    google_search = GoogleSearchAPIWrapper(
        google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID, k=3
    )
    google_search.k = result_count
    return google_search.run(query=query)


chat_model.bind(functions=[convert_to_openai_function(google_search_tool)])
prompt = ChatPromptTemplate.from_messages(
    messages=[
        (
            "system",
            "You are a very good product analyst for apple products. Your task is to find out the most accurate detailed information on various apple products.",
        ),
        ("human", "{request}"),
    ]
)

# analyst_chain = prompt | chat_model

# res = analyst_chain.invoke(
#     {"request": "What are all the features of the iphone 13 pro max?"}
# )

# print(res)

product_features_template_str: str = """
You will be provided with a product name. Your task will be to list all all the product,s features 



def is_acceptable_len(text: str, l: int = 20) -> bool:
    return len(text.split()) >= l


with open(file_path, "r", encoding="utf-8") as f:
    all_comments: list[str] = json.load(fp=f)
    cleaned_comments: list[str] = list(map(clean_text, all_comments))
    comments: list[str] = choices(population=cleaned_comments, k=10)
    docs: list[Document] = [
        Document(page_content=comment)
        for comment in comments
        if is_acceptable_len(comment)
    ]
    comments: list[dict[str, str | int]] = [
        {"doc_id": i + 1, "comment": docs[i].page_content} for i in range(len(docs))
    ]

data_dir = "./agent_nelly/data_analysis/data"
features_dir = "features"
save_features_dir = path.join(data_dir, features_dir, "features.json")

with open(save_features_dir, 'r') as f:
    topics: list[str] = json.load(f)

comment: dict = choice(comments)


sentiment_msg: str = """
Below is a customer comment in JSON format with the following keys:
1. doc_id - identifier of the comment
2. comment - the user comment

Please analyze the comment and identify the sentiment. The sentiment can be negative, neutral or 
positive. Only return a single string, the sentiment.

Comment:
```
{comment}
```



def add_module_to_queue(module_path: str, module_path_queue: Queue):
    module_path_queue.put(module_path)


def get_node_source(node, module_src: str) -> str:
    return ast.get_source_segment(source=module_src, node=node)


def get_functions_source(module_path: str) -> list[str]:
    functions_src: list[str] = []
    module_src = get_module_source_code(module_path)
    module_tree = ast.parse(module_src)
    for node in module_tree.body:
        if isinstance(node, FunctionDef):
            function_src: str = get_node_source(node=node, module_src=module_src)
            functions_src.append((node.name, function_src))
    return functions_src


def get_class_source(module_path: str) -> None:
    class_src: list[str] = []
    module_src = get_module_source_code(module_path)
    module_tree = ast.parse(module_src)
    for node in module_tree.body:
        if isinstance(node, ClassDef):
            classsrc: str = get_node_source(node=node, module_src=module_src)
            class_src.append((node.name, classsrc))
    return class_src


class DirectoryIterator:
    def __init__(self, config: Config):
        self.config: Config = config
        self.queue: deque[str] = deque(self.config.path)

    def __iter__(self) -> Iterator:
        return self

    def __next__(self) -> list[str]:


    agent = cl.user_session.get("agent")
    msg = cl.Message(content="")
    await msg.send()
    await cl.sleep(1)
    msg.content = agent.invoke({"input": message.content})["output"]
    await msg.update()

                function_code=function_code, config=self.config
            )
            try:
                function_docstring: str = get_function_docstring(function_and_docstring)
            except Exception:
                new_docstring_node = make_docstring_node(function_and_docstring)
            else:
                new_docstring_node = make_docstring_node(function_docstring)
            node.body.insert(0, new_docstring_node)
        return node


class ClassDocStringWriter(NodeTransformer, BaseModel):
    module_path: str = Field(description='The path to this module')
    class_name: str = Field(description='The name of the class to generate docstrings')
    class_code: str = Field(description='The source code for this class')
    config: Config = Field(description='The application configurations.')

    @property
    def module_code(self) -> str:
        return get_module_source_code(self.module_path)

    def visit_ClassDef(self, node: ClassDef) -> Any:
        docstring: str = ast.get_docstring(node=node)
        if node.name == self.class_name and (
            self.config.overwrite_class_docstring or not docstring
        ):
            class_code: str = ast.get_source_segment(
                source=self.module_code, node=node, padded=True
            )
            class_and_docstring: str = generate_class_docstring(
                class_code=class_code, config=self.config
            )
            try:
                class_docstring: str = get_class_docstring(class_and_docstring)
            except Exception:
                class_docstring = class_and_docstring
                new_docstring_node = make_docstring_node(class_and_docstring)
                class_docstring = class_and_docstring
            else:


        ins[cls == -1] = 0  # ignore it.

        # class label
        labelme.utils.lblsave(out_clsp_file, cls)
        if not args.nonpy:
            np.save(out_cls_file, cls)
        if not args.noviz:
            clsv = imgviz.label2rgb(
                cls,
                imgviz.rgb2gray(img),
                label_names=class_names,
                font_size=15,
                loc="rb",
            )
            imgviz.io.imsave(out_clsv_file, clsv)

        if not args.noobject:
            # instance label
            labelme.utils.lblsave(out_insp_file, ins)
            if not args.nonpy:
                np.save(out_ins_file, ins)
            if not args.noviz:
                instance_ids = np.unique(ins)
                instance_names = [str(i) for i in range(max(instance_ids) + 1)]
                insv = imgviz.label2rgb(
                    ins,
                    imgviz.rgb2gray(img),
                    label_names=instance_names,
                    font_size=15,
                    loc="rb",
                )
                imgviz.io.imsave(out_insv_file, insv)


if __name__ == "__main__":
    main()



@celery.task(name="train_model_task")
def train_model_task(train_config: TrainConfig):
    train_results: dict = fit_pipeline(train_config=train_config)
    trained_model: TrainedModel = TrainedModel(
        classifier_name=train_results['model_name'],
        train_date=datetime.now(),
        save_path=path.join(app_config.models_dir, 'trained', train_results['model_name']),
        owner='Lyle Okoth',
        metrics=train_results['metrics'],
        train_time=train_results['train_time']
    )
    trained_model.post_model_metrics(app_config, redis)
    return {
        'Model Name': train_results['model_name'],
        'Train Time': train_results['train_time'],
        'Metrics': train_results['metrics']
    }
    

@celery.task(name='tune_model')
def tune_model(train_config: TrainConfig):
    train_features = train_config.preprocessor.fit_transform(train_config.train_features)
    test_features = train_config.preprocessor.fit_transform(train_config.test_features)
    clf = GridSearchCV(train_config.model, hyperparameters[train_config.classifier_name], cv=10)
    best_model = clf.fit(train_features, train_config.train_labels.values.ravel())
    best_params = best_model.best_estimator_.get_params()
    preds = best_model.predict(test_features)
    accuracy = accuracy_score(preds, train_config.test_labels)
    return {
        'params': best_params,
        'acuracy': accuracy,
        'name': train_config.classifier_name
    }
    
def fit_pipeline(train_config: TrainConfig, model_params: dict = {}, train: bool = True) -> dict:
    logging.info('Training and saving the tuned model')
    untrained_model: BaseEstimator = train_config.model
    pipeline: Pipeline = Pipeline(steps=[
            ('preprocessor', train_config.preprocessor),


@staticmethod
    def hash_password(password: str) -> str:
        return bcrypt.generate_password_hash(password).decode("utf-8")

    def check_password(self, password: str) -> bool:
        return bcrypt.check_password_hash(self.password, password)
    
    @staticmethod
    def encode_auth_token(user_id: int):
        try:
            payload = {
                "exp": datetime.utcnow() + timedelta(days=0, hours=2),
                "iat": datetime.utcnow(),
                "sub": user_id,
            }
            return jwt.encode(payload, current_app.config.get("SECRET_KEY"), algorithm="HS256")
        except Exception as e:
            return e
        
    @staticmethod
    def decode_auth_token(auth_token: str):
        try:
            payload = jwt.decode(auth_token, current_app.config.get("SECRET_KEY"), algorithms="HS256")
            return payload["sub"]
        except (ExpiredSignatureError, InvalidTokenError) as e:
            raise e

# channel_list = ['Asianometry', '']
# watched_list = []
# playlist_id = 'PL_26vmg8W_AfJWy6SVtoSmtYWimhexwF7'
# playlist_title = ''
# channel_name = 'Asianometry'
# channel_id = ''
# video_id = ''
# channels = youtube.find_channel_by_name(channel_name)
# for channel in channels.items:
#     if channel.title == channel_name:
#         channel_id = channel.channel_id
# part = Part()
# optional_parameters: OptionalParameters = OptionalParameters(
#     q='',
#     maxResults=1,
#     type=['video'],
#     channelId=channel_id,
#     order='date'
# )
# search = SearchSchema(part=part, optional_parameters=optional_parameters)
# search_result = youtube.search(search)
# videos = search_result.items
# latest_video = videos[0]
# video_id = latest_video.resource_id
# video_resource: VideoResourceId = VideoResourceId(videoId=video_id)
# snippet: CreatePlaylistItemSnippet = CreatePlaylistItemSnippet(
#     playlistId=playlist_id,
#     resourceId=video_resource
# )
# create = CreatePlaylistItem(snippet=snippet)
# playlist_item = youtube.insert_playlist_item(create)
# print(f'Added {playlist_item.snippet.title} to the playlist.')
# print(youtube.get_video_categories())

# def search_save():
#     from json import dump
#     part: SearchPart = SearchPart()
#     optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
#         q='israel palestine conflict',
#         maxResults=5,


# Scrapy settings for slidesmodel project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = "slidesmodel"

SPIDER_MODULES = ["slidesmodel.spiders"]
NEWSPIDER_MODULE = "slidesmodel.spiders"


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = "slidesmodel (+http://www.yourdomain.com)"

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {


from setuptools import find_packages, setup
from pip._vendor import tomli

# For consistent encoding
from codecs import open
from os import path

# The directory containing this file
HERE = path.abspath(path.dirname(__file__))

# Get the long description from the README file
with open(path.join(HERE, 'README.md'), encoding='utf-8') as f:
    LONG_DESCRIPTION = f.read()

with open('pyproject.toml', 'r') as f:
    VERSION = tomli.load(f)['tool']['commitizen']['version']

DESCRIPTION = 'A chatbot that enables the user interact with youtube over chat.'

key_words = [
    'youtube', 'youtube-api', 'youtube comments', 'youtube videos', 'chat with youtube',
    'youtube channels', 'youtube comment thread', 'create youtube playlist'
]

install_requires = [
    'oryks-youtube',
    'pydantic',
    'pydantic-settings'
]

setup(
    name='youtube-assistant',
    packages=find_packages(
        include=[
            'assistant',
        ]
    ),
    version=VERSION,
    description=DESCRIPTION,
    long_description_content_type='text/markdown',


from scrapy import Item, Field
from itemloaders.processors import TakeFirst, MapCompose, Join
import re


def remove_html_tags(description: str) -> str:
    html_pattern = "<(?:\"[^\"]*\"['\"]*|'[^']*'['\"]*|[^'\">])+>" 
    return re.sub(html_pattern, '', description)

def remove_unicode_chars(text: str) -> str:
    return text.replace(u"\xa0", "")

def num_of_slides(text: str) -> int:
    vals = [val for val in list(text) if val.isdigit()]
    return "".join(vals)


class SlidesModelItem(Item):
    title = Field(output_processor=TakeFirst())
    category = Field(output_processor=TakeFirst())
    description = Field(
        input_processor=MapCompose(remove_html_tags, remove_unicode_chars),
        output_processor=Join()
    )
    tags = Field()
    slides_count = Field(
        input_processor=MapCompose(num_of_slides),
        output_processor=TakeFirst()
    )
    colors = Field()
    image_urls = Field()
    images = Field()


        )
        generate_functions_docstring_thread.start()

    for _ in range(1):
        generate_class_docstring_thread: Thread = Thread(
            target=generate_class_docstrings,
            args=(class_source_queue, config),
            daemon=True,
        )
        generate_class_docstring_thread.start()

    queue_modules.join()
    module_path_queue.join()
    functions_source_queue.join()
    class_source_queue.join()


    )
    parser.add_argument(
        "--labels",
        help="comma separated list of labels OR file containing labels",
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--validatelabel",
        dest="validate_label",
        choices=["exact"],
        help="label validation types",
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--keep-prev",
        action="store_true",
        help="keep annotation of previous frame",
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--epsilon",
        type=float,
        help="epsilon to find nearest vertex on canvas",
        default=argparse.SUPPRESS,
    )
    args = parser.parse_args()

    if args.version:
        print("{0} {1}".format(__appname__, __version__))
        sys.exit(0)

    logger.setLevel(getattr(logging, args.logger_level.upper()))

    if hasattr(args, "flags"):
        if os.path.isfile(args.flags):
            with codecs.open(args.flags, "r", encoding="utf-8") as f:
                args.flags = [line.strip() for line in f if line.strip()]
        else:
            args.flags = [line for line in args.flags.split(",") if line]



    def undoLastPoint(self):
        if not self.current or self.current.isClosed():
            return
        self.current.popPoint()
        if len(self.current) > 0:
            self.line[0] = self.current[-1]
        else:
            self.current = None
            self.drawingPolygon.emit(False)
        self.update()

    def loadPixmap(self, pixmap, clear_shapes=True):
        self.pixmap = pixmap
        if self._ai_model:
            self._ai_model.set_image(
                image=labelme.utils.img_qt_to_arr(self.pixmap.toImage())
            )
        if clear_shapes:
            self.shapes = []
        self.update()

    def loadShapes(self, shapes, replace=True):
        if replace:
            self.shapes = list(shapes)
        else:
            self.shapes.extend(shapes)
        self.storeShapes()
        self.current = None
        self.hShape = None
        self.hVertex = None
        self.hEdge = None
        self.update()

    def setShapeVisible(self, shape, value):
        self.visible[shape] = value
        self.update()

    def overrideCursor(self, cursor):
        self.restoreCursor()
        self._cursor = cursor


def create_like(session: Session, activity: CreateActivity) -> Like:
    with session() as db:
        like: Like = Like(
            author_id=activity.user_id,
            post_id=activity.post_id
        )
        db.add(like)
        db.commit()
        db.refresh(like)
    return like

    )
    documents: list[Document] = loader.load()
    full_transcript: str = ""
    for document in documents:
        full_transcript += document.page_content
    return full_transcript

def analayze_video(video_transcript: str):
    """Break a video into the slide components.
    
    The video is to be broken down into:
    1. The Cover - summary details about this video
    2. Index - a shortcut to other slides
    3. The slides
    
    The cover
    ---------
    Contains the video name and channel presenting the video
    
    Index
    -----
    Contains links to the other slides and their titles
    
    The slides
    -----------
    Consist of the slide title and a bullet of points
    """
    pass


from queue import Queue

from langchain_openai import OpenAI

modules_path_queue: Queue = Queue()
functions_source_code_queue: Queue = Queue()
class_source_code_queue: Queue = Queue()
failed_modules_queue: Queue = Queue()
llm = OpenAI(temperature=0)


from youtube import YouTube
from youtube.models import Search
from youtube.schemas import (
        YouTubeRequest, YouTubeListResponse, YouTubeResponse,
        SearchFilter, SearchOptionalParameters, SearchPart
)
from typing import Iterator


client_secrets_file = "/home/lyle/oryks/backend/api/libraries/youtube.json"
def get_youtube_client(client_secrets_file: str = client_secrets_file) -> YouTube:
    youtube: YouTube = YouTube(client_secret_file=client_secrets_file)
    client = youtube.authenticate()
    youtube.youtube_client = client
    return youtube

youtube: YouTube = get_youtube_client(client_secrets_file="/home/lyle/Downloads/test.json")


# query: str = ''
# part: SearchPart = SearchPart()
# optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
#     q=query,
#     type=['video'],
#     channelId="UCtAcpQcYerN8xxZJYTfWBMw"
# )
# search_request: YouTubeRequest = YouTubeRequest(
#     part=part, 
#     optional_parameters=optional_parameters
# )
# search_results: YouTubeResponse = youtube.search(search_request)
# search_iterator: Iterator = youtube.get_search_iterator(search_request)
# # res: YouTubeResponse = youtube.find_channel_by_name(display_name="Umar Jamil")
# # print(res.items[0])
# res = next(search_iterator)
# final = []
# for x in search_iterator:
#         for search in x:
#                 final.append(
#                         dict(



#     print(f"Translated example sentence: \n {translated_sentence}")

#     model.train()

#     for batch_idx, batch in enumerate(train_iterator):
#         # Get input and targets and get to cuda
#         inp_data = batch.src.to(device)
#         target = batch.trg.to(device)

#         # Forward prop
#         output = model(inp_data, target)

#         # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss
#         # doesn't take input in that form. For example if we have MNIST we want to have
#         # output to be: (N, 10) and targets just (N). Here we can view it in a similar
#         # way that we have output_words * batch_size that we want to send in into
#         # our cost function, so we need to do some reshapin. While we're at it
#         # Let's also remove the start token while we're at it
#         output = output[1:].reshape(-1, output.shape[2])
#         target = target[1:].reshape(-1)

#         optimizer.zero_grad()
#         loss = criterion(output, target)

#         # Back prop
#         loss.backward()

#         # Clip to avoid exploding gradient issues, makes sure grads are
#         # within a healthy range
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)

#         # Gradient descent step
#         optimizer.step()

#         # Plot to tensorboard
#         writer.add_scalar("Training loss", loss, global_step=step)
#         step += 1




from typing import Any
import json
from googleapiclient.errors import HttpError


def get_presentation(presentation_id: str, slide_client: Any) -> Any:
    presentation = (
        slide_client.presentations().get(presentationId=presentation_id).execute()
    )
    return presentation


def get_slides(presentation: Any) -> Any:
    slides = presentation.get("slides")
    return slides


def create_blank_presentation(title: str, slide_client: Any) -> dict:
    body: dict = {"title": title}
    presentation = slide_client.presentations().create(body=body).execute()
    print(f"Created blank presentation with the id: {presentation.get('presentationId')}")
    presentation_name: str = f"{title.casefold().strip().replace(' ', '_')}.json"
    with open(presentation_name, "w") as f:
        json.dump(presentation, f)
    return presentation
    

def create_slide(presentation_id: str, page_id: str, slide_client: Any) -> dict:
    create_slides_request = [
        {
            "createSlide": {
                "objectId": page_id,
                "insertionIndex": "1",
                "slideLayoutReference": {
                    "predefinedLayout": "TITLE_AND_TWO_COLUMNS"
                }
            }
        }
    ]
    request_body = {


from sklearn.gaussian_process.kernels import RBF, DotProduct, Matern, RationalQuadratic, WhiteKernel


names = [
    "Nearest Neighbors",
    "Linear SVM",
    "RBF SVM",
    "Gaussian Process",
    "Decision Tree",
    "Random Forest",
    "Neural Net",
    "AdaBoost",
    "Naive Bayes",
    "QDA",
]
knn = dict(
    leaf_size=list(range(1, 15)),
    n_neighbors=list(range(1, 10)),
    p=[1, 2]
)

gaussian_process = dict(
    kernel=[1*RBF(), 1*DotProduct(), 1*Matern(),  1*RationalQuadratic(), 1*WhiteKernel()]
)

decision_tree = dict(
    criterion=['gini', 'entropy'],
    max_depth=list(range(1, 10)),
    min_samples_split=list(range(1, 10)),
    min_samples_leaf=list(range(1, 10))
)

hyperparameters: dict[str, dict] = {
    "Nearest Neighbors": knn,
    "Gaussian Process": gaussian_process,
    "Decision Tree": decision_tree
}

        )
        removePoint = action(
            text="Remove Selected Point",
            slot=self.removeSelectedPoint,
            shortcut=shortcuts["remove_selected_point"],
            icon="edit",
            tip="Remove selected point from polygon",
            enabled=False,
        )

        undo = action(
            self.tr("Undo\n"),
            self.undoShapeEdit,
            shortcuts["undo"],
            "undo",
            self.tr("Undo last add and edit of shape"),
            enabled=False,
        )

        hideAll = action(
            self.tr("&Hide\nPolygons"),
            functools.partial(self.togglePolygons, False),
            shortcuts["hide_all_polygons"],
            icon="eye",
            tip=self.tr("Hide all polygons"),
            enabled=False,
        )
        showAll = action(
            self.tr("&Show\nPolygons"),
            functools.partial(self.togglePolygons, True),
            shortcuts["show_all_polygons"],
            icon="eye",
            tip=self.tr("Show all polygons"),
            enabled=False,
        )
        toggleAll = action(
            self.tr("&Toggle\nPolygons"),
            functools.partial(self.togglePolygons, None),
            shortcuts["toggle_all_polygons"],
            icon="eye",


        a.setIcon(newIcon(icon))
    if shortcut is not None:
        if isinstance(shortcut, (list, tuple)):
            a.setShortcuts(shortcut)
        else:
            a.setShortcut(shortcut)
    if tip is not None:
        a.setToolTip(tip)
        a.setStatusTip(tip)
    if slot is not None:
        a.triggered.connect(slot)
    if checkable:
        a.setCheckable(True)
    a.setEnabled(enabled)
    a.setChecked(checked)
    return a


def addActions(widget, actions):
    for action in actions:
        if action is None:
            widget.addSeparator()
        elif isinstance(action, QtWidgets.QMenu):
            widget.addMenu(action)
        else:
            widget.addAction(action)


def labelValidator():
    return QtGui.QRegExpValidator(QtCore.QRegExp(r"^[^ \t].+"), None)


class struct(object):
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)


def distance(p):
    return sqrt(p.x() * p.x() + p.y() * p.y())



        self.actions.createPointMode.setEnabled(True)
        self.actions.createLineStripMode.setEnabled(True)
        self.actions.createAiPolygonMode.setEnabled(True)
        self.actions.createAiMaskMode.setEnabled(True)
        title = __appname__
        if self.filename is not None:
            title = "{} - {}".format(title, self.filename)
        self.setWindowTitle(title)

        if self.hasLabelFile():
            self.actions.deleteFile.setEnabled(True)
        else:
            self.actions.deleteFile.setEnabled(False)

    def toggleActions(self, value=True):
        """Enable/Disable widgets which depend on an opened image."""
        for z in self.actions.zoomActions:
            z.setEnabled(value)
        for action in self.actions.onLoadActive:
            action.setEnabled(value)

    def queueEvent(self, function):
        QtCore.QTimer.singleShot(0, function)

    def status(self, message, delay=5000):
        self.statusBar().showMessage(message, delay)

    def resetState(self):
        self.labelList.clear()
        self.filename = None
        self.imagePath = None
        self.imageData = None
        self.labelFile = None
        self.otherData = None
        self.canvas.resetState()

    def currentItem(self):
        items = self.labelList.selectedItems()
        if items:
            return items[0]


#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = "httpcache"
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"

# Set settings whose default value is deprecated to a future-proof value
REQUEST_FINGERPRINTER_IMPLEMENTATION = "2.7"
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
FEED_EXPORT_ENCODING = "utf-8"


        
    def get_default_credentials_path(self) -> str:
        """Generate the default api token file location."""
        credentials_dir: str = self.create_default_credentials_path()
        credentials_file_path = path.join(credentials_dir, self.credentials_file_name)
        return credentials_file_path

    def get_credentials(self) -> Credentials:
        """Get the credentials."""
        credentials: Credentials = None
        credentials_path: str = self.get_default_credentials_path()
        try:
            with open(credentials_path, 'r', encoding='utf-8') as creds:
                credentials = Credentials(**load(creds))
        except FileNotFoundError:
            pass
        return credentials
    
    def generate_credentials(self) -> Credentials:
        flow = InstalledAppFlow.from_client_secrets_file(self.secrets_file, self.scopes)
        credentials = flow.run_local_server(port=0)
        return credentials
    
    def save_credentials(self, credentials: Credentials) -> None:
        credentials_dict = self.credentials_to_dict(credentials)
        credentials_path: str = self.get_default_credentials_path()
        with open(credentials_path, 'w', encoding='utf-8') as f:
            dump(credentials_dict, f)
            
    def credentials_expired(self, credentials: Credentials) -> bool:
        # youtube_client = self.get_youtube_client(credentials=credentials)
        # youtube_find_request = youtube_client.search().list(q='', part='id')
        # try:
        #     youtube_find_request.execute()
        # except RefreshError:
        #     return True
        # return False
        return False
            
    def get_gmail_client(self, credentials: Credentials) -> Any:


from oryks_google_oauth import GoogleOAuth, YouTubeScopes


client_secret_file: str = '/home/lyle/Professional Projects/youtube/client_secret.json'
api_service_name: str = 'youtube'
api_version: str = 'v3'
credentials_dir: str = '.youtube'
scopes: list[str] = [YouTubeScopes.youtube.value]
oauth: GoogleOAuth = GoogleOAuth(
    secrets_file=client_secret_file,
    scopes=scopes,
    api_service_name=api_service_name,
    api_version=api_version,
    credentials_dir=credentials_dir
)
youtube_client = oauth.authenticate_google_server()
youtube_find_request = youtube_client.search().list(q='python programming videos', part='id, snippet')
print(youtube_find_request.execute())


        )

    def _update_shape_color(self, shape):
        r, g, b = self._get_rgb_by_label(shape.label)
        shape.line_color = QtGui.QColor(r, g, b)
        shape.vertex_fill_color = QtGui.QColor(r, g, b)
        shape.hvertex_fill_color = QtGui.QColor(255, 255, 255)
        shape.fill_color = QtGui.QColor(r, g, b, 128)
        shape.select_line_color = QtGui.QColor(255, 255, 255)
        shape.select_fill_color = QtGui.QColor(r, g, b, 155)

    def _get_rgb_by_label(self, label):
        if self._config["shape_color"] == "auto":
            item = self.uniqLabelList.findItemByLabel(label)
            if item is None:
                item = self.uniqLabelList.createItemFromLabel(label)
                self.uniqLabelList.addItem(item)
                rgb = self._get_rgb_by_label(label)
                self.uniqLabelList.setItemLabel(item, label, rgb)
            label_id = self.uniqLabelList.indexFromItem(item).row() + 1
            label_id += self._config["shift_auto_shape_color"]
            return LABEL_COLORMAP[label_id % len(LABEL_COLORMAP)]
        elif (
            self._config["shape_color"] == "manual"
            and self._config["label_colors"]
            and label in self._config["label_colors"]
        ):
            return self._config["label_colors"][label]
        elif self._config["default_shape_color"]:
            return self._config["default_shape_color"]
        return (0, 255, 0)

    def remLabels(self, shapes):
        for shape in shapes:
            item = self.labelList.findItemByShape(shape)
            self.labelList.removeItem(item)

    def loadShapes(self, shapes, replace=True):
        self._noSelectionSlot = True
        for shape in shapes:



@pytest.mark.gui
def test_MainWindow_open_json(qtbot):
    json_files = [
        osp.join(data_dir, "annotated_with_data/apc2016_obj3.json"),
        osp.join(data_dir, "annotated/2011_000003.json"),
    ]
    for json_file in json_files:
        labelme.testing.assert_labelfile_sanity(json_file)

        win = labelme.app.MainWindow(filename=json_file)
        qtbot.addWidget(win)
        _win_show_and_wait_imageData(qtbot, win)
        win.close()


def create_MainWindow_with_directory(qtbot):
    directory = osp.join(data_dir, "raw")
    win = labelme.app.MainWindow(filename=directory)
    qtbot.addWidget(win)
    _win_show_and_wait_imageData(qtbot, win)
    return win


@pytest.mark.gui
def test_MainWindow_openNextImg(qtbot):
    win = create_MainWindow_with_directory(qtbot)
    win.openNextImg()


@pytest.mark.gui
def test_MainWindow_openPrevImg(qtbot):
    win = create_MainWindow_with_directory(qtbot)
    win.openNextImg()


@pytest.mark.gui
def test_MainWindow_annotate_jpg(qtbot):
    tmp_dir = tempfile.mkdtemp()
    input_file = osp.join(data_dir, "raw/2011_000003.jpg")


# hiddens, _ = decoder.lstm(x)
# print(hiddens.shape)
# output = decoder.linear(hiddens)
# print(output.shape)
# outputs = model(images, captions)
# print(outputs.shape)

# transforms=Compose([
#     Resize((299, 299)),
#     ToTensor()
# ])
# img_path = "/home/lyle/oryks/finetune-image-captioning-model/raw-data/Images/667626_18933d713e.jpg"
# img = Image.open(img_path)
# trans = transforms(img)
# print(trans.unsqueeze(0).shape)
# res = model.caption_image(image=trans.unsqueeze(0), vocabulary=dataset.vocabulary)


# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.


def generate_data(
    user_count: int = 200, 
    posts_count: int = 500, 
    likes_count: int = 2000, 
    comments_count: int = 1000,
    bookmarks_count: int = 1500
    ):
    users = generate_users(user_count)
    users = [
        add_user(user) for user in users
    ]
    posts: list[Post] = generate_posts(users, count=posts_count)
    posts: list[Post] = [
        add_post(post) for post in posts
    ]
    likes: list[Like] = generate_likes(users, posts, likes_count=likes_count)
    add_likes(likes)
    
    comments: list[Comment] = generate_comments(users, posts, comments_count=comments_count)
    add_comments(comments)
    
    bookmarks: list[Bookmark] = generate_bookmarks(users, posts, bookmarks_count=bookmarks_count)
    add_bookmarks(bookmarks)

                        self.drawVertex(negative_vrtx_path, i)
            else:
                line_path.moveTo(self.points[0])
                # Uncommenting the following line will draw 2 paths
                # for the 1st vertex, and make it non-filled, which
                # may be desirable.
                # self.drawVertex(vrtx_path, 0)

                for i, p in enumerate(self.points):
                    line_path.lineTo(p)
                    self.drawVertex(vrtx_path, i)
                if self.isClosed():
                    line_path.lineTo(self.points[0])

            painter.drawPath(line_path)
            if vrtx_path.length() > 0:
                painter.drawPath(vrtx_path)
                painter.fillPath(vrtx_path, self._vertex_fill_color)
            if self.fill and self.mask is None:
                color = self.select_fill_color if self.selected else self.fill_color
                painter.fillPath(line_path, color)

            pen.setColor(QtGui.QColor(255, 0, 0, 255))
            painter.setPen(pen)
            painter.drawPath(negative_vrtx_path)
            painter.fillPath(negative_vrtx_path, QtGui.QColor(255, 0, 0, 255))

    def drawVertex(self, path, i):
        d = self.point_size / self.scale
        shape = self.point_type
        point = self.points[i]
        if i == self._highlightIndex:
            size, shape = self._highlightSettings[self._highlightMode]
            d *= size
        if self._highlightIndex is not None:
            self._vertex_fill_color = self.hvertex_fill_color
        else:
            self._vertex_fill_color = self.vertex_fill_color
        if shape == self.P_SQUARE:
            path.addRect(point.x() - d / 2, point.y() - d / 2, d, d)


    out_file = osp.join(tmp_dir, "2011_000003.json")

    config = labelme.config.get_default_config()
    win = labelme.app.MainWindow(
        config=config,
        filename=input_file,
        output_file=out_file,
    )
    qtbot.addWidget(win)
    _win_show_and_wait_imageData(qtbot, win)

    label = "whole"
    points = [
        (100, 100),
        (100, 238),
        (400, 238),
        (400, 100),
    ]
    shapes = [
        dict(
            label=label,
            group_id=None,
            points=points,
            shape_type="polygon",
            mask=None,
            flags={},
            other_data={},
        )
    ]
    win.loadLabels(shapes)
    win.saveFile()

    labelme.testing.assert_labelfile_sanity(out_file)
    shutil.rmtree(tmp_dir)


from setuptools import find_packages, setup
from pip._vendor import tomli
from codecs import open
from os import path

HERE = path.abspath(path.dirname(__file__))
with open(path.join(HERE, 'README.md'), encoding='utf-8') as f:
    LONG_DESCRIPTION = f.read()
with open('pyproject.toml', 'r') as f:
    VERSION = tomli.load(f)['tool']['commitizen']['version']
DESCRIPTION = 'A python library for generating documentation for python projects.'
key_words = ['dosctrings', 'documentation']
install_requires = [
    'langchain',
    'langchain-openai',
    'black',
    'pydantic',
    'pydantic-settings',
]
setup(
    name='oryks-docstring-generator',
    packages=find_packages(include=['docstring_generator']),
    version=VERSION,
    description=DESCRIPTION,
    long_description_content_type='text/markdown',
    long_description=LONG_DESCRIPTION,
    url='https://youtube-wrapper.readthedocs.io/en/latest/index.html',
    author='Lyle Okoth',
    author_email='lyceokoth@gmail.com',
    license='MIT',
    install_requires=install_requires,
    keywords=key_words,
    classifiers=[
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.11',
        'Programming Language :: Python :: 3.12',
        'Operating System :: OS Independent',




def get_class_methods_docstrings(class_and_docstring: str) -> dict[str, str]:
    """Get a class methods docstrings."""
    class_methods: dict[str, str] = {}
    class_tree = ast.parse(class_and_docstring)
    for node in class_tree.body:
        if isinstance(node, ClassDef):
            for class_node in node.body:
                if isinstance(class_node, FunctionDef):
                    class_methods[class_node.name] = ast.get_docstring(class_node)
    return class_methods


def make_docstring_node(docstr: str):
    constant_str: Constant = Constant(docstr)
    return Expr(value=constant_str)


def get_function_docstring(function_and_docstring: str) -> str:
    """Get the function docstring."""
    function_tree = ast.parse(function_and_docstring)
    for node in function_tree.body:
        if isinstance(node, (FunctionDef, AsyncFunctionDef)):
            function_docstring: str = ast.get_docstring(node)
            return function_docstring


def get_module_source_code(module_path: str) -> str:
    """Get the source code for a given module."""
    with open(module_path, 'r') as f:
        return f.read()


def add_module_code_to_queue(module_path: str, module_source_queue: Queue):
    module_src: str = ''
    if module_path:
        module_src = get_module_source_code(module_path)
    module_source_queue.put((module_path, module_src))



class GetPosts(BaseModel):
    offset: Optional[int] = 0
    limit: Optional[int] = 10
    
class PostAuthor(BaseModel):
    id: str
    profile_picture: str
    name: str
    
class PostLike(BaseModel):
    liked: bool
    liked_by: Optional[list[PostAuthor]] = Field(default_factory=list)
    key_like: Optional[PostAuthor] = None
    likes_count: Optional[int] = Field(default=0)
    
class KeyComment(BaseModel):
    author: PostAuthor
    text: str
    comments_count: int
    
class PostSchema(BaseModel):
    id: str
    text: str
    image: str
    author: PostAuthor
    date_published: str
    location: str
    like: PostLike
    bookmarked: bool
    key_comment: Optional[KeyComment] = None

full_chain = {
    "sentiment": sentiment_chain,
    "comment": lambda input: input['comment'],
    "topics": lambda input: input['topics']
} | branch

res = full_chain.invoke({'comment': comment, "topics": topics})
print(comment)
print(res)


        self,
        label=None,
        line_color=None,
        shape_type=None,
        flags=None,
        group_id=None,
        description=None,
        mask=None,
    ):
        self.label = label
        self.group_id = group_id
        self.points = []
        self.point_labels = []
        self.shape_type = shape_type
        self._shape_raw = None
        self._points_raw = []
        self._shape_type_raw = None
        self.fill = False
        self.selected = False
        self.flags = flags
        self.description = description
        self.other_data = {}
        self.mask = mask

        self._highlightIndex = None
        self._highlightMode = self.NEAR_VERTEX
        self._highlightSettings = {
            self.NEAR_VERTEX: (4, self.P_ROUND),
            self.MOVE_VERTEX: (1.5, self.P_SQUARE),
        }

        self._closed = False

        if line_color is not None:
            # Override the class line_color attribute
            # with an object attribute. Currently this
            # is used for drawing the pending line a different color.
            self.line_color = line_color

    def setShapeRefined(self, shape_type, points, point_labels, mask=None):


import logging.config
import logstash

from dotenv import load_dotenv

load_dotenv()


def create_dev_logger():
    """Create the application logger."""
    config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "standard": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
            },
            "json": {
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
                "datefmt": "%Y-%m-%dT%H:%M:%S%z",
                "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
            },
        },
        "handlers": {
            "standard": {
                "class": "logging.StreamHandler",
                "formatter": "json",
            },
        },
        "loggers": {"": {"handlers": ["standard"], "level": logging.INFO}},
    }

    logging.config.dictConfig(config)

    logger = logging.getLogger(__name__)

    return logger


    return app

from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from pathlib import Path
import os
from models import Trip


def load_secets():
    load_dotenv()
    env_path = Path(".") / ".env"
    load_dotenv(dotenv_path=env_path)

    open_ai_key = os.getenv("OPENAI_API_KEY")
    google_palm_key = os.getenv("GOOGLE_PALM_API_KEY")

    return {
        "OPENAI_API_KEY": open_ai_key,
        "GOOGLE_PALM_API_KEY": google_palm_key,
    }


class Validation(BaseModel):
    plan_is_valid: str = Field(
        description="This field is 'yes' if the plan is feasible, 'no' otherwise"
    )
    updated_request: str = Field(description="Your update to the plan")


class ValidationTemplate(object):
    def __init__(self):
        self.system_template = """
      You are a travel agent who helps users make exciting travel plans.

      The user's request will be denoted by four hashtags. Determine if the user's


            label_file = osp.splitext(filename)[0] + ".json"
            if self.output_dir:
                label_file_without_path = osp.basename(label_file)
                label_file = osp.join(self.output_dir, label_file_without_path)
            item = QtWidgets.QListWidgetItem(filename)
            item.setFlags(Qt.ItemIsEnabled | Qt.ItemIsSelectable)
            if QtCore.QFile.exists(label_file) and LabelFile.is_label_file(label_file):
                item.setCheckState(Qt.Checked)
            else:
                item.setCheckState(Qt.Unchecked)
            self.fileListWidget.addItem(item)
        self.openNextImg(load=load)

    def scanAllImages(self, folderPath):
        extensions = [
            ".%s" % fmt.data().decode().lower()
            for fmt in QtGui.QImageReader.supportedImageFormats()
        ]

        images = []
        for root, dirs, files in os.walk(folderPath):
            for file in files:
                if file.lower().endswith(tuple(extensions)):
                    relativePath = os.path.normpath(osp.join(root, file))
                    images.append(relativePath)
        images = natsort.os_sorted(images)
        return images


    table.add_column(header="[b]Comments", justify="left", style="yellow2")
    table.add_column(header="Likes", justify="left", style="magenta3")
    table.add_column(header="[b]Date", justify="center", style="violet")
    table.columns[0].header_style = "bold chartreuse1"
    table.columns[1].header_style = "bold dark_goldenrod"
    table.columns[2].header_style = "bold chartreuse1"
    table.columns[3].header_style = "bold dark_goldenrod"
    table.columns[4].header_style = "bold chartreuse1"
    table.border_style = "bright_yellow"
    table.pad_edge = True
    for row in table_data:
        table.add_row(row["title"], str(row["views"]), str(row["comments"]), str(row["likes"]), row["date"])
    return table


def parse_comment(comment: Comment) -> dict:
    return {
        "comment_id": comment.id,
        "comment": comment.snippet.text_display,
        "likes": comment.snippet.like_count,
        "date_published": str(comment.snippet.published_at),
    }
    
    
class Data(BaseModel):
    id: str
    comment: str
    sentiment: Optional[str] = Field(
        description="The comment sentiment",
        enum=["neutral", "positive", "negative"],
        default=None,
    )
    features: Optional[list[str]] = Field(
        description="The features mentioned in the comment", default_factory=list
    )
    likes: Optional[int] = Field(description="The number of likes", default=None)
    date: Optional[str] = Field(
        description="The date when the comment was posted", default=None
    )



"""This module declares the app configuration.

The classes include:

BaseConfig:
    Has all the configurations shared by all the environments.

"""
import os

from dotenv import load_dotenv

load_dotenv()


class BaseConfig:
    """Base configuration."""

    DEBUG = True
    TESTING = False
    SECRET_KEY = os.environ.get(
        "SECRET_KEY", "df0331cefc6c2b9a5d0208a726a5d1c0fd37324feba25506"
    )
    POSTGRES_HOST = os.environ["POSTGRES_HOST"]
    POSTGRES_DB = os.environ["POSTGRES_DB"]
    POSTGRES_PORT = os.environ["POSTGRES_PORT"]
    POSTGRES_USER = os.environ["POSTGRES_USER"]
    POSTGRES_PASSWORD = os.environ["POSTGRES_PASSWORD"]
    # db_conn_string = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
    db_conn_string = os.environ.get("SQLALCHEMY_DATABASE_URI", 'sqlite:///./oryks.db')
    SQLALCHEMY_DATABASE_URI = db_conn_string
    SQLALCHEMY_TRACK_MODIFICATIONS = False
    MAIL_USERNAME = os.environ["MAIL_USERNAME"]
    MAIL_PASSWORD = os.environ["MAIL_PASSWORD"]
    MAIL_SERVER = os.environ["MAIL_SERVER"]
    MAIL_PORT = os.environ["MAIL_PORT"]
    MAIL_USE_SSL = os.environ["MAIL_USE_SSL"]
    MAIL_DEFAULT_SENDER = os.environ["MAIL_DEFAULT_SENDER"]
    PASSWORD_RESET_SALT = os.environ.get("PASSWORD_RESET_SALT", "salt")
    GOOGLE_OAUTH_CLIENT_ID = os.environ.get("GOOGLE_OAUTH_CLIENT_ID")



data_dir: str = "/home/lyle/oryks/backend/api/libraries/data"
descriptions_dir: str = path.join(data_dir, "descriptions")
segments_dir: str = path.join(data_dir, "segments")

class TimeStamp(BaseModel):
    start_time: Optional[str] = Field(description="Start time")
    end_time: Optional[str] = Field(description="End time")
    title: Optional[str] = Field(description="The time stamp title")
    
class TimeStamps(BaseModel):
    time_stamps: list[TimeStamp]

def save_description(description: str, video_id: str) -> None:
    video_path: str = path.join(descriptions_dir, f"{video_id}.txt")
    with open(video_path, "w", encoding="utf-8") as f:
        f.write(description)
        
        
def save_timestamps(timestamps: TimeStamps, video_id: str) -> None:
    video_path: str = path.join(segments_dir, f"{video_id}.json")
    with open(video_path, "w", encoding="utf-8") as f:
        json.dump(timestamps.dict(), f, indent=4)
        
        
def load_timestamps(video_id: str) -> TimeStamps:
    video_path: str = path.join(segments_dir, f"{video_id}.json")
    with open(video_path, "r", encoding="utf-8") as f:
        timestamps: TimeStamps = TimeStamps(**json.load(f))
    return timestamps


def get_timestamps(video_id: str) -> TimeStamps:
    video_path: str = path.join(segments_dir, f"{video_id}.json")
    if not path.exists(video_path):
        description: str = get_description(video_id=video_id)
        timestamps: TimeStamps = get_video_segments(video_description=description)
        save_timestamps(timestamps=timestamps, video_id=video_id)
    else:
        timestamps: TimeStamps = load_timestamps(video_id=video_id)


"""

sentiment_template = PromptTemplate(template=sentiment_msg, input_variables=["comment"])


class PositiveComment(BaseModel):
    doc_id: int = Field(description="The doc_id from the input")
    topics: list[str] = Field(
        description="List of the relevant topics for the customer review. Include only topics from the list provided.",
        default_factory=list,
    )
    sentiment: str = Field(
        description="Sentiment of the topic", enum=["positive", "neutral", "negative"]
    )


class NegativeComment(BaseModel):
    doc_id: int = Field(description="The doc_id from the input")
    topics: list[str] = Field(
        description="List of the relevant topics for the customer review. Include only topics from the list provided.",
        default_factory=list,
    )
    sentiment: str = Field(
        description="Sentiment of the topic", enum=["positive", "neutral", "negative"]
    )


positive_parser = PydanticOutputParser(pydantic_object=PositiveComment)
negative_parser = PydanticOutputParser(pydantic_object=NegativeComment)

topic_assg_msg: str = """
Below is a customer comment in JSON format with the following keys:
1. doc_id - identifier of the comment
2. comment - the user comment

Please analyze the provided comments and identify the main topics and sentiment. Include only the 
topics provided below:
Topics with a short description: {topics}

Comment:


    agent = cl.user_session.get("agent")
    msg = cl.Message(content="")
    await msg.send()
    await cl.sleep(1)
    msg.content = agent.invoke({"input": message.content})["output"]
    await msg.update()

from celery import Celery
from pydantic_settings import BaseSettings
from dotenv import load_dotenv
from redis import Redis
from json import dumps

load_dotenv()


class BaseConfig(BaseSettings):
    celery_broker_url: str
    celery_result_backend: str
    redis_host: str
    
config = BaseConfig()

celery = Celery(__name__)
celery.conf.broker_url = config.celery_broker_url
celery.conf.result_backend = config.celery_result_backend
redis: Redis = Redis(host=config.redis_host, port=6379, db=0, decode_responses=True)

@celery.task(name="analyze_quote")
def analyze_quote(quote: dict) -> dict:
    analyzed_quote: dict = quote
    analyzed_quote.update({'result': 'Some result'})
    redis.publish('analyzed_quotes', dumps(analyzed_quote))
    return analyzed_quote


@celery.task(name="send_email")
def send_email(quote: dict) -> dict:
    
    return analyzed_quote

from celery_app import celery_app


if __name__ == '__main__':
    args = ['worker', '--loglevel=INFO']
    # celery_app.autodiscover_tasks(['tasks'])
    celery_app.worker_main(argv=args)

@post.route("/posts", methods=["GET"])
def get_all_posts():
    """Get many post post."""
    offset: str = request.args.get('offset')
    limit: str = request.args.get('limit')
    try:
        posts = get_posts(get_db, GetPosts(offset=offset, limit=limit))
    except (OperationalError, IntegrityError) as e:
            print(e)
            # Send email to
            return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    created_posts = []
    for post in posts:
        post_author: PostAuthor = PostAuthor(
            id=post.author.id,
            profile_picture=url_for('static', filename='img/default.jpeg'),
            name=post.author.first_name
        )
        post_schema: PostSchema = PostSchema(
            id=post.id,
            text=post.text,
            image=url_for('static', filename=f'img/{post.image_url}'),
            location=post.location,
            date_published='10',
            author=post_author
        ).model_dump()
        created_posts.append(post_schema)

            new_tree = transformer.visit(src_tree)
            ast.fix_missing_locations(new_tree)
            new_module_code = ast.unparse(new_tree)
            print(new_module_code)
            save_src(file_path=file_path, new_src=new_module_code)
            format_file(file_path=file_path)
            function_code_queue.task_done()
        except Empty:
            print("Terminating the function processing..")
            break


from pydantic import BaseModel
from typing import Optional
from datetime import datetime


class CreateActivity(BaseModel):
    user_id: str
    post_id: str
    

class ActivityCreated(CreateActivity):
    date_created: datetime
    
class RepeatableActivityCreated(ActivityCreated):
    id: str
    
class GetRepeatableActivity(BaseModel):
    id: str
    
class CreateComment(CreateActivity):
    comment: str
    
class CommentCreated(CreateComment):
    comment_id: str
    date_created: datetime

    version=VERSION,
    description=DESCRIPTION,
    long_description_content_type='text/markdown',
    long_description=LONG_DESCRIPTION,
    url='https://youtube-wrapper.readthedocs.io/en/latest/index.html',
    author='Lyle Okoth',
    author_email='lyceokoth@gmail.com',
    license='MIT',
    install_requires=install_requires,
    keywords=key_words,
    classifiers=[
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.11',
        'Programming Language :: Python :: 3.12',
        'Operating System :: OS Independent'
    ],
)


# import openai
import logging
import time
# for Palm
from langchain.llms import GooglePalm
# for OpenAI
from langchain.chat_models import ChatGooglePalm, ChatOpenAI
from langchain.chains import LLMChain, SequentialChain
from prompt_templates import (
    ValidationTemplate, load_secets, MappingTemplate, ItineraryTemplate)


logging.basicConfig(level=logging.INFO)

class Agent(object):
    def __init__(
        self,
        open_ai_api_key,
        model="gpt-3.5-turbo",
        temperature=0,
        debug=True,
    ):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        self._openai_key = open_ai_api_key

        self.chat_model = ChatOpenAI(model=model, temperature=temperature, openai_api_key=self._openai_key)
        self.validation_prompt = ValidationTemplate()
        self.itinerary_prompt = ItineraryTemplate()
        self.mapping_prompt = MappingTemplate()
        self.validation_chain = self._set_up_validation_chain(debug)
        self.agent_chain = self._set_up_agent_chain(debug)

    def _set_up_validation_chain(self, debug=True):
        """

        Parameters
        ----------
        debug



          nn.ReLU(),
          nn.BatchNorm2d(128),
          nn.MaxPool2d(2),
          # Convolution 4
          nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(256),
          nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
          nn.ReLU(),
          nn.BatchNorm2d(256),
          nn.MaxPool2d(2),
      )

      self.dense_layers = nn.Sequential(
          # Dropout layer
          nn.Dropout(0.5),
          # first fully connected layer
          nn.Linear(224*224, 1024),
          # Relu activation function
          nn.ReLU(),
          nn.Dropout(0.4),
          # Final output layer
          nn.Linear(1024, K),
      )

  def forward(self, output):
    # Convolution Layers
    out = self.conv_layers(output)

    # Flatten the layers
    out = out.view(-1, 224*224)

    # Fully connected Dense Layers
    out = self.dense_layers(out)

    return out


def load_model(model_path: str = os.environ['MODEL_PATH']):
    """Load the pytorch model."""



    def setEditing(self, value=True):
        self.mode = self.EDIT if value else self.CREATE
        if self.mode == self.EDIT:
            # CREATE -> EDIT
            self.repaint()  # clear crosshair
        else:
            # EDIT -> CREATE
            self.unHighlight()
            self.deSelectShape()

    def unHighlight(self):
        if self.hShape:
            self.hShape.highlightClear()
            self.update()
        self.prevhShape = self.hShape
        self.prevhVertex = self.hVertex
        self.prevhEdge = self.hEdge
        self.hShape = self.hVertex = self.hEdge = None

    def selectedVertex(self):
        return self.hVertex is not None

    def selectedEdge(self):
        return self.hEdge is not None

    def mouseMoveEvent(self, ev):
        """Update line with last point and current coordinates."""
        try:
            if QT5:
                pos = self.transformPos(ev.localPos())
            else:
                pos = self.transformPos(ev.posF())
        except AttributeError:
            return

        self.prevMovePoint = pos
        self.restoreCursor()

        is_shift_pressed = ev.modifiers() & QtCore.Qt.ShiftModifier


                        else self.createMode
                    )
                    self.current.addPoint(pos, label=0 if is_shift_pressed else 1)
                    if self.createMode == "point":
                        self.finalise()
                    elif (
                        self.createMode in ["ai_polygon", "ai_mask"]
                        and ev.modifiers() & QtCore.Qt.ControlModifier
                    ):
                        self.finalise()
                    else:
                        if self.createMode == "circle":
                            self.current.shape_type = "circle"
                        self.line.points = [pos, pos]
                        if (
                            self.createMode in ["ai_polygon", "ai_mask"]
                            and is_shift_pressed
                        ):
                            self.line.point_labels = [0, 0]
                        else:
                            self.line.point_labels = [1, 1]
                        self.setHiding()
                        self.drawingPolygon.emit(True)
                        self.update()
            elif self.editing():
                if self.selectedEdge():
                    self.addPointToEdge()
                elif (
                    self.selectedVertex()
                    and int(ev.modifiers()) == QtCore.Qt.ShiftModifier
                ):
                    # Delete point if: left-click + SHIFT on a point
                    self.removeSelectedPoint()

                group_mode = int(ev.modifiers()) == QtCore.Qt.ControlModifier
                self.selectShapePoint(pos, multiple_selection_mode=group_mode)
                self.prevPoint = pos
                self.repaint()
        elif ev.button() == QtCore.Qt.RightButton and self.editing():
            group_mode = int(ev.modifiers()) == QtCore.Qt.ControlModifier


ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}

def get_file_extension(filename: str) -> str:
    if '.' in filename and filename.rsplit('.', 1)[1].lower():
        return filename.rsplit('.', 1)[1].lower()
    return ''

def allowed_file(filename: str) -> bool:
    file_extension: str = get_file_extension(filename)
    if file_extension and file_extension in ALLOWED_EXTENSIONS:
        return True
    return False


def save_post_photo_locally(post_image: dict) -> None:
    """Save the uploadeded post image."""
    file: FileStorage = post_image['post_image']
    upload_folder = os.path.join(current_app.root_path, 'static', 'img')
    if file and allowed_file(file.filename):
        filename = f'{secrets.token_hex(8)}.{get_file_extension(file.filename)}' 
        # Use celery task
        file.save(os.path.join(upload_folder, filename))
        return filename
    return ''

            return

        # Just hovering over the canvas, 2 possibilities:
        # - Highlight shapes
        # - Highlight vertex
        # Update shape/vertex fill and tooltip value accordingly.
        self.setToolTip(self.tr("Image"))
        for shape in reversed([s for s in self.shapes if self.isVisible(s)]):
            # Look for a nearby vertex to highlight. If that fails,
            # check if we happen to be inside a shape.
            index = shape.nearestVertex(pos, self.epsilon / self.scale)
            index_edge = shape.nearestEdge(pos, self.epsilon / self.scale)
            if index is not None:
                if self.selectedVertex():
                    self.hShape.highlightClear()
                self.prevhVertex = self.hVertex = index
                self.prevhShape = self.hShape = shape
                self.prevhEdge = self.hEdge
                self.hEdge = None
                shape.highlightVertex(index, shape.MOVE_VERTEX)
                self.overrideCursor(CURSOR_POINT)
                self.setToolTip(self.tr("Click & drag to move point"))
                self.setStatusTip(self.toolTip())
                self.update()
                break
            elif index_edge is not None and shape.canAddPoint():
                if self.selectedVertex():
                    self.hShape.highlightClear()
                self.prevhVertex = self.hVertex
                self.hVertex = None
                self.prevhShape = self.hShape = shape
                self.prevhEdge = self.hEdge = index_edge
                self.overrideCursor(CURSOR_POINT)
                self.setToolTip(self.tr("Click to create point"))
                self.setStatusTip(self.toolTip())
                self.update()
                break
            elif shape.containsPoint(pos):
                if self.selectedVertex():
                    self.hShape.highlightClear()


from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import OpenAIWhisperParser
from langchain_community.document_loaders.blob_loaders.youtube_audio import (
    YoutubeAudioLoader,
)
from langchain_core.documents import Document
from os import path

# Two Karpathy lecture videos
urls = ["https://www.youtube.com/watch?v=altvPR7x9IA"]

# Directory to save audio files
data_dir = "data"
video_data_dir = "video"
transcribed_data = "transcriptions"
video_title = "sample"
save_video_dir = path.join(data_dir, video_data_dir, video_title)
save_transcript_dir = path.join(data_dir, transcribed_data, video_title + ".txt")

api_key: str = "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"

loader = GenericLoader(
    YoutubeAudioLoader(urls, save_video_dir), OpenAIWhisperParser(api_key=api_key)
)
docs = loader.load()

full_transcript = ""
for doc in docs:
    full_transcript += doc.page_content

with open(save_transcript_dir, "w", encoding="utf-8") as f:
    f.write(full_transcript)

print(full_transcript)


def transcribe_video(video_id: str, save_video_dir: str, api_key: str) -> str:
    url: str = f"https://www.youtube.com/watch?v={video_id}"
    loader: GenericLoader = GenericLoader(
        YoutubeAudioLoader([url], save_video_dir), OpenAIWhisperParser(api_key=api_key)


# -*- encoding: utf-8 -*-

import pytest

from labelme.widgets import LabelListWidget
from labelme.widgets import LabelListWidgetItem


@pytest.mark.gui
def test_LabelListWidget(qtbot):
    widget = LabelListWidget()

    item = LabelListWidgetItem(text="person <font color='red'>‚óè</fon>")
    widget.addItem(item)
    item = LabelListWidgetItem(text="dog <font color='blue'>‚óè</fon>")
    widget.addItem(item)

    widget.show()
    qtbot.addWidget(widget)
    qtbot.waitExposed(widget)


        image_file = files[0]
        image_data = image_file.content # byte values of the image
        image = Image.open(io.BytesIO(image_data))
        model = load_model()
        predicted_label, predictions = evaluate_image(image, model)
        analysis_text: str = f"""
            After analyzing the image you uploaded, here is what I found:
            Maize Leaf Rust probability: {predictions['Maize Leaf Rust']}%
            Northern Leaf Blight probability: {predictions['Northern Leaf Blight']}%
            Healthy probability: {predictions['Healthy']}%
            Gray Leaf Spot probability: {predictions['Gray Leaf Spot']}%
            Your plant is most likely infected with {predicted_label}.
            """
        elements = [
            cl.Image(
                name="image2", display="inline", content=image_data
                ), 
            cl.Text(name="simple_text", content=analysis_text, display="inline", size='large')
        ]
        await cl.Message(content=f"Maize image with {predicted_label}!", elements=elements).send()
        msg = cl.Message(content="")
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run('Tell me some facts about the maize disease leaf rust especially in relation to kenya.')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Tell me some facts about the maize disease {predicted_label} especially in relation to kenya.')
        await msg.update()
        await msg.send()
        await cl.sleep(1)
        msg.content = agent.run(f'Get me aggrovets in {user_location}, Kenya')
        await msg.update()
        await cl.Message(content='Feel free to ask me more questions about maize plant diseases and how to deal with them.').send()
    else:
        await cl.Message(content='Currently cannot detect pests. Still working on that model.').send()
    

@cl.on_message
async def main(message: cl.Message):


# posts = [
    #     CreatedPost(
    #         id=post.id,
    #         location=post.location,
    #         text=post.text,
    #         image_url=post.image_url,
    #         author_id=post.author_id,
    #         date_published=post.date_published
    #     ).model_dump()
    #     for post in posts
    # ]
    return created_posts, HTTPStatus.OK

@post.route("/load_more_posts", methods=["GET"])
def load_more_posts():
    """Get a single post."""
    offset: str = request.args.get('offset', 0)
    limit: str = request.args.get('limit', 10)
    more_posts = load_posts(limit=int(limit), offset=int(offset))
    return more_posts

        Args:
            i (int): The vertex index
            action (int): The action
            (see Shape.NEAR_VERTEX and Shape.MOVE_VERTEX)
        """
        self._highlightIndex = i
        self._highlightMode = action

    def highlightClear(self):
        """Clear the highlighted point"""
        self._highlightIndex = None

    def copy(self):
        return copy.deepcopy(self)

    def __len__(self):
        return len(self.points)

    def __getitem__(self, key):
        return self.points[key]

    def __setitem__(self, key, value):
        self.points[key] = value


    os.makedirs(osp.join(args.output_dir, "JPEGImages"))
    os.makedirs(osp.join(args.output_dir, "SegmentationClass"))
    if not args.nonpy:
        os.makedirs(osp.join(args.output_dir, "SegmentationClassNpy"))
    if not args.noviz:
        os.makedirs(osp.join(args.output_dir, "SegmentationClassVisualization"))
    if not args.noobject:
        os.makedirs(osp.join(args.output_dir, "SegmentationObject"))
        if not args.nonpy:
            os.makedirs(osp.join(args.output_dir, "SegmentationObjectNpy"))
        if not args.noviz:
            os.makedirs(osp.join(args.output_dir, "SegmentationObjectVisualization"))
    print("Creating dataset:", args.output_dir)

    if osp.exists(args.labels):
        with open(args.labels) as f:
            labels = [label.strip() for label in f if label]
    else:
        labels = [label.strip() for label in args.labels.split(",")]

    class_names = []
    class_name_to_id = {}
    for i, label in enumerate(labels):
        class_id = i - 1  # starts with -1
        class_name = label.strip()
        class_name_to_id[class_name] = class_id
        if class_id == -1:
            assert class_name == "__ignore__"
            continue
        elif class_id == 0:
            assert class_name == "_background_"
        class_names.append(class_name)
    class_names = tuple(class_names)
    print("class_names:", class_names)
    out_class_names_file = osp.join(args.output_dir, "class_names.txt")
    with open(out_class_names_file, "w") as f:
        f.writelines("\n".join(class_names))
    print("Saved class_names:", out_class_names_file)

    for filename in sorted(glob.glob(osp.join(args.input_dir, "*.json"))):


from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
i

options = Options()
options.add_argument("--headless=new")
driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)
driver.get("https://leetcode.com/problems/remove-linked-list-elements")
paragraphs = driver.find_elements(By.TAG_NAME, "p")
print(paragraphs)
driver.quit()

        cx, cy = xy[0]
        r = point_size
        draw.ellipse([cx - r, cy - r, cx + r, cy + r], outline=1, fill=1)
    else:
        assert len(xy) > 2, "Polygon must have points more than 2"
        draw.polygon(xy=xy, outline=1, fill=1)
    mask = np.array(mask, dtype=bool)
    return mask


def shapes_to_label(img_shape, shapes, label_name_to_value):
    cls = np.zeros(img_shape[:2], dtype=np.int32)
    ins = np.zeros_like(cls)
    instances = []
    for shape in shapes:
        points = shape["points"]
        label = shape["label"]
        group_id = shape.get("group_id")
        if group_id is None:
            group_id = uuid.uuid1()
        shape_type = shape.get("shape_type", None)

        cls_name = label
        instance = (cls_name, group_id)

        if instance not in instances:
            instances.append(instance)
        ins_id = instances.index(instance) + 1
        cls_id = label_name_to_value[cls_name]

        mask = shape_to_mask(img_shape[:2], points, shape_type)
        cls[mask] = cls_id
        ins[mask] = ins_id

    return cls, ins


def labelme_shapes_to_label(img_shape, shapes):
    logger.warn(
        "labelme_shapes_to_label is deprecated, so please use " "shapes_to_label."


from .set_config import set_configuration

@post.route("/bookmarks", methods=["GET"])
def get_post_bookmarks():
    """Bookmark a single post."""
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post: Post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        bookmarks: list[Bookmark] = list_post_bookmarks(session=get_db, post_data=post_data)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    resp = [
        ActivityCreated(
            user_id=bookmark.author_id,
            post_id=bookmark.post_id,
            date_created=bookmark.bookmark_date
        ).model_dump()
        for bookmark in bookmarks
    ]
    return resp, HTTPStatus.OK



def get_all_modules(config: Config, source_code_queue: Queue) -> None:
    """Iterate throug all the directories from the root directory."""
    for entry in config.root_directory:
        if path.isfile(entry):
            source_code_queue.put(entry)
        else:
            directory_iterator: DirectoryIterator = DirectoryIterator(config=config)
            for modules in directory_iterator:
                for module in modules:
                    source_code_queue.put(module)


            enabled=False,
        )
        zoomOut = action(
            self.tr("&Zoom Out"),
            functools.partial(self.addZoom, 0.9),
            shortcuts["zoom_out"],
            "zoom-out",
            self.tr("Decrease zoom level"),
            enabled=False,
        )
        zoomOrg = action(
            self.tr("&Original size"),
            functools.partial(self.setZoom, 100),
            shortcuts["zoom_to_original"],
            "zoom",
            self.tr("Zoom to original size"),
            enabled=False,
        )
        keepPrevScale = action(
            self.tr("&Keep Previous Scale"),
            self.enableKeepPrevScale,
            tip=self.tr("Keep previous zoom scale"),
            checkable=True,
            checked=self._config["keep_prev_scale"],
            enabled=True,
        )
        fitWindow = action(
            self.tr("&Fit Window"),
            self.setFitWindow,
            shortcuts["fit_window"],
            "fit-window",
            self.tr("Zoom follows window size"),
            checkable=True,
            enabled=False,
        )
        fitWidth = action(
            self.tr("Fit &Width"),
            self.setFitWidth,
            shortcuts["fit_width"],
            "fit-width",


def get_posts(session: Session, post_data: GetPosts):
    with session() as db:
        posts: list[Post] = db.query(Post).offset(post_data.offset).limit(post_data.limit).all()
        for post in posts:
            post.author
        return posts

def delete_post(session: Session, post_data: GetPost):
    with session() as db:
        post = db.query(Post).filter(Post.id == post_data.post_id).first()
        db.delete(post)
        db.commit()
        
    return post

    """Get a video thumbnail."""
    thumbnail_key: dict[str, Any] = {}
    if data.get('high'):
        thumbnail_key = data.get('high')
    elif data.get('medium'):
        thumbnail_key = data.get('high')
    elif data.get('standard'):
        thumbnail_key = data.get('high')
    else:
        thumbnail_key = data.get('high')
    return thumbnail_key.get('url')

def get_caption(data: dict[str, Any]) -> str:
    return data.get('title', 'Caption')

def get_description(data: dict[str, Any]) -> str:
    return data.get('description', 'Description')

st.title("Live News Bot.")

# React to user input
if prompt := st.chat_input("What is up?"):
    # Display user message in chat message container
    with st.chat_message('user'):
        st.markdown(f'Finding latest news videos mentioning "{prompt}"')
    # Display assistant response in chat message container
    with st.spinner('Searching for the latest live news coverage...'):
        search_results: list[dict[str, Any]] = search_news(prompt)
    # search_results: list[dict[str, Any]] = load_data()
    for search_result in search_results: 
        video_id: str = search_result.get('resource_id')
        with st.spinner('Loading the news piece...'):
            video: dict[str, Any] = search_video(video_id)
        with st.chat_message("assistant"):
            url: str = f'https://www.youtube.com/watch?v={video_id}'
            st.video(url)
            st.markdown(get_description(video['snippet']))

# videos: list[dict[str, Any]] = load_data()
# video: dict[str, Any] = search_video(videos[0].get('resource_id'))



            s.append(shape)
        self.loadShapes(s)

    def loadFlags(self, flags):
        self.flag_widget.clear()
        for key, flag in flags.items():
            item = QtWidgets.QListWidgetItem(key)
            item.setFlags(item.flags() | Qt.ItemIsUserCheckable)
            item.setCheckState(Qt.Checked if flag else Qt.Unchecked)
            self.flag_widget.addItem(item)

    def saveLabels(self, filename):
        lf = LabelFile()

        def format_shape(s):
            data = s.other_data.copy()
            data.update(
                dict(
                    label=s.label.encode("utf-8") if PY2 else s.label,
                    points=[(p.x(), p.y()) for p in s.points],
                    group_id=s.group_id,
                    description=s.description,
                    shape_type=s.shape_type,
                    flags=s.flags,
                    mask=None if s.mask is None else utils.img_arr_to_b64(s.mask),
                )
            )
            return data

        shapes = [format_shape(item.shape()) for item in self.labelList]
        flags = {}
        for i in range(self.flag_widget.count()):
            item = self.flag_widget.item(i)
            key = item.text()
            flag = item.checkState() == Qt.Checked
            flags[key] = flag
        try:
            imagePath = osp.relpath(self.imagePath, osp.dirname(filename))
            imageData = self.imageData if self._config["store_data"] else None


                createLineStripMode,
                createAiPolygonMode,
                createAiMaskMode,
                editMode,
                edit,
                duplicate,
                copy,
                paste,
                delete,
                undo,
                undoLastPoint,
                removePoint,
            ),
            onLoadActive=(
                close,
                createMode,
                createRectangleMode,
                createCircleMode,
                createLineMode,
                createPointMode,
                createLineStripMode,
                createAiPolygonMode,
                createAiMaskMode,
                editMode,
                brightnessContrast,
            ),
            onShapesPresent=(saveAs, hideAll, showAll, toggleAll),
        )

        self.canvas.vertexSelected.connect(self.actions.removePoint.setEnabled)

        self.menus = utils.struct(
            file=self.menu(self.tr("&File")),
            edit=self.menu(self.tr("&Edit")),
            view=self.menu(self.tr("&View")),
            help=self.menu(self.tr("&Help")),
            recentFiles=QtWidgets.QMenu(self.tr("Open &Recent")),
            labelList=labelMenu,
        )



from sqlalchemy import create_engine
from sqlalchemy.orm import DeclarativeBase, MappedAsDataclass
from sqlalchemy.orm import sessionmaker
from ...config.config import BaseConfig
from contextlib import contextmanager
from flask import current_app


class Base(MappedAsDataclass, DeclarativeBase):
    pass

SQLALCHEMY_DATABASE_URI = BaseConfig().db_conn_string
engine = create_engine(SQLALCHEMY_DATABASE_URI)
Session = sessionmaker(bind=engine, autocommit=False, autoflush=False)

def create_all():
    Base.metadata.create_all(bind=engine)
    
def drop_all():
    Base.metadata.drop_all(bind=engine)

@contextmanager
def get_db():
    try:
        db = Session()
        yield db
    finally:
        db.close()

import os.path as osp
import shutil
import tempfile

import pytest

import labelme.app
import labelme.config
import labelme.testing

here = osp.dirname(osp.abspath(__file__))
data_dir = osp.join(here, "data")


def _win_show_and_wait_imageData(qtbot, win):
    win.show()

    def check_imageData():
        assert hasattr(win, "imageData")
        assert win.imageData is not None

    qtbot.waitUntil(check_imageData)  # wait for loadFile


@pytest.mark.gui
def test_MainWindow_open(qtbot):
    win = labelme.app.MainWindow()
    qtbot.addWidget(win)
    win.show()
    win.close()


@pytest.mark.gui
def test_MainWindow_open_img(qtbot):
    img_file = osp.join(data_dir, "raw/2011_000003.jpg")
    win = labelme.app.MainWindow(filename=img_file)
    qtbot.addWidget(win)
    _win_show_and_wait_imageData(qtbot, win)
    win.close()



def update_post(post_data: UpdatePost, post_image: dict, session: Session):
    post_image_url: str = save_post_photo(post_image)
    with session() as db:
        post: Post = db.query(Post).filter(Post.id == post_data.post_id).first()
        if post_data.location:
            post.location = post_data.location
        if post_data.text:
            post.text = post_data.text
        if post_image_url:
            post.image_url = post_image_url
        db.commit()
        db.refresh(post)
    return post

def get_post(session: Session, post_data: GetPost):
    with session() as db:
        post = db.query(Post).filter(Post.id == post_data.post_id).first()
    return post

    features_covered.append(res)
    print(res)
print(features_covered)
# print(len(splits))
# for split in splits:
#     print(split)
#     print("###################")

from dotenv import load_dotenv
load_dotenv()
from flask.cli import FlaskGroup
from api import create_app

app = create_app()
cli = FlaskGroup(create_app=create_app)



if __name__ == "__main__":
    cli()

import ast
import subprocess
from ast import AST, Constant, Expr
from os import path
from openai import RateLimitError
import sys

from .config import Config
from .llms import chatgpt
from .templates import function_prompt


def read_src(file_path: str) -> str:
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


def save_src(file_path: str, new_src: str) -> str:
    with open(file_path, "w", encoding="utf-8") as f:
        return f.write(new_src)


def parse_src(file_src: str) -> AST:
    return ast.parse(file_src)


def print_src(src_tree: AST) -> None:
    print(ast.dump(src_tree, indent=4))


function_dcstr: str = '''
def subtract(a: int | float, b: int | float) -> int | float:
    """Subtracts two numbers

    Parameters
    ----------
    a : int or float
        The first number to subtract.
    b : int or float
        The second number to subtract.


from qtpy import QtCore
from qtpy import QtWidgets


class ToolBar(QtWidgets.QToolBar):
    def __init__(self, title):
        super(ToolBar, self).__init__(title)
        layout = self.layout()
        m = (0, 0, 0, 0)
        layout.setSpacing(0)
        layout.setContentsMargins(*m)
        self.setContentsMargins(*m)
        self.setWindowFlags(self.windowFlags() | QtCore.Qt.FramelessWindowHint)

    def addAction(self, action):
        if isinstance(action, QtWidgets.QWidgetAction):
            return super(ToolBar, self).addAction(action)
        btn = QtWidgets.QToolButton()
        btn.setDefaultAction(action)
        btn.setToolButtonStyle(self.toolButtonStyle())
        self.addWidget(btn)

        # center align
        for i in range(self.layout().count()):
            if isinstance(self.layout().itemAt(i).widget(), QtWidgets.QToolButton):
                self.layout().itemAt(i).setAlignment(QtCore.Qt.AlignCenter)


#                                 id=search.resource_id,
#                                 title=search.title
#                         )
#                 )
# print(final)       

def delete_user(session: Session, user_data: GetUser):
    with session() as db:
        user = db.query(User).filter(User.id == user_data.user_id).first()
        db.delete(user)
        db.commit()
        
    return user


def user_account_active(session: Session, user_data: GetUser):
    with session() as db:
        user: User = db.query(User).filter(User.id == user_data.user_id).first()
    return user.activated

# channel_playlists = youtube.find_channel_playlists('UC5WVOSvL9bc6kwCMXXeFLLw')
# print(channel_playlists)
# search_iterator = youtube.find_playlist_items('PLsyeobzWxl7poL9JTVyndKe62ieoN-MZ3', max_results=25)
# print(next(search_iterator))
# print(youtube.search())

import copy
import math

import numpy as np
import skimage.measure
from qtpy import QtCore
from qtpy import QtGui

import labelme.utils
from labelme.logger import logger

# TODO(unknown):
# - [opt] Store paths instead of creating new ones at each paint.


class Shape(object):
    # Render handles as squares
    P_SQUARE = 0

    # Render handles as circles
    P_ROUND = 1

    # Flag for the handles we would move if dragging
    MOVE_VERTEX = 0

    # Flag for all other handles on the current shape
    NEAR_VERTEX = 1

    # The following class variables influence the drawing of all shape objects.
    line_color = None
    fill_color = None
    select_line_color = None
    select_fill_color = None
    vertex_fill_color = None
    hvertex_fill_color = None
    point_type = P_ROUND
    point_size = 8
    scale = 1.0

    def __init__(


import torch
import torchvision.transforms as transforms
from PIL import Image


def print_examples(model, device, dataset):
    transform = transforms.Compose(
        [
            transforms.Resize((299, 299)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ]
    )

    model.eval()
    test_img1 = transform(Image.open("test_examples/dog.jpg").convert("RGB")).unsqueeze(
        0
    )
    print("Example 1 CORRECT: Dog on a beach by the ocean")
    print(
        "Example 1 OUTPUT: "
        + " ".join(model.caption_image(test_img1.to(device), dataset.vocabulary))
    )
    test_img2 = transform(
        Image.open("test_examples/child.jpg").convert("RGB")
    ).unsqueeze(0)
    print("Example 2 CORRECT: Child holding red frisbee outdoors")
    print(
        "Example 2 OUTPUT: "
        + " ".join(model.caption_image(test_img2.to(device), dataset.vocabulary))
    )
    test_img3 = transform(Image.open("test_examples/bus.png").convert("RGB")).unsqueeze(
        0
    )
    print("Example 3 CORRECT: Bus driving by parked cars")
    print(
        "Example 3 OUTPUT: "
        + " ".join(model.caption_image(test_img3.to(device), dataset.vocabulary))
    )
    test_img4 = transform(


def gemma_extraction(video_id: str):
    description: str = get_description(video_id=video_id)
    template: PromptTemplate = PromptTemplate(template=segment_str_gemma, 
                    input_variables=["text"]
                                              )
    chain = template | llama_2b 
    inputs: dict[str, str] = {
        "text": description
    }
    res = chain.invoke(inputs)
    print(res)
    
    
def get_channel_id(channel_name: str, youtube: YouTube = get_youtube_client()) -> str:
    response: YouTubeResponse = youtube.find_channel_by_name(display_name=channel_name)
    search_result: Search = response.items[0]
    channel_id: str = search_result.channel_id
    return channel_id

def get_channel_playlists(channel_name: str, youtube: YouTube = get_youtube_client()) -> list[str]:
    # channel_id: str = get_channel_id(channel_name=channel_name, youtube=youtube)
    channel_id: str = "UC_mYaQAE6-71rjSN6CeCA-g"
    response: YouTubeListResponse = youtube.find_channel_playlists(channel_id=channel_id)
    playlists: list[Playlist] = response.items
    playlist_ids: list[str] = [playlist.id for playlist in playlists]
    return playlist_ids
    

def main(video_id: str = video_id):
    # output: TimeStamps = partition_video_segments(video_id=video_id)
    # print(output)    
    # gemma_extraction(video_id=video_id)
    channel_name: str = "neetcode"
    playlist_ids: list[str] = get_channel_playlists(channel_name=channel_name)
    print(playlist_ids)
    
    


    try:
        import PyQt5  # NOQA

        QT_BINDING = "pyqt5"
    except ImportError:
        pass

    if QT_BINDING is None:
        try:
            import PySide2  # NOQA

            QT_BINDING = "pyside2"
        except ImportError:
            pass

    if QT_BINDING is None:
        # PyQt5 can be installed via pip for Python3
        # 5.15.3, 5.15.4 won't work with PyInstaller
        install_requires.append("PyQt5!=5.15.3,!=5.15.4")
        QT_BINDING = "pyqt5"

    del QT_BINDING

    if os.name == "nt":  # Windows
        install_requires.append("colorama")

    return install_requires


def get_long_description():
    with open("README.md") as f:
        long_description = f.read()
    try:
        # when this package is being released
        import github2pypi

        return github2pypi.replace_url(
            slug="wkentaro/labelme", content=long_description, branch="main"
        )


#!/usr/bin/env python

from __future__ import print_function

import argparse
import glob
import os
import os.path as osp
import sys

import imgviz
import numpy as np

import labelme


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("input_dir", help="Input annotated directory")
    parser.add_argument("output_dir", help="Output dataset directory")
    parser.add_argument(
        "--labels", help="Labels file or comma separated text", required=True
    )
    parser.add_argument(
        "--noobject", help="Flag not to generate object label", action="store_true"
    )
    parser.add_argument(
        "--nonpy", help="Flag not to generate .npy files", action="store_true"
    )
    parser.add_argument(
        "--noviz", help="Flag to disable visualization", action="store_true"
    )
    args = parser.parse_args()

    if osp.exists(args.output_dir):
        print("Output directory already exists:", args.output_dir)
        sys.exit(1)
    os.makedirs(args.output_dir)


                url="https://github.com/labelmeai/efficient-sam/releases/download/onnx-models-20231225/efficient_sam_vits_decoder.onnx",  # NOQA
                md5="d9372f4a7bbb1a01d236b0508300b994",
            ),
        )


MODELS = [
    SegmentAnythingModelVitB,
    SegmentAnythingModelVitL,
    SegmentAnythingModelVitH,
    EfficientSamVitT,
    EfficientSamVitS,
]




class ProductionConfig(BaseConfig):
    """Production configuration."""

    TESTING = False
    SECRET_KEY = os.environ.get("SECRET_KEY", "secret-key")


Config = {
    "development": DevelopmentConfig,
    "test": TestingConfig,
    "production": ProductionConfig,
    "staging": ProductionConfig,
}


# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.


from api import create_app


app = create_app()

@post.route("/comments", methods=["GET"])
def get_post_comments():
    """Get a posts comments."""
    offset: str = request.args.get('offset', 0)
    limit: str = request.args.get('limit', 10)
    try:
        post_data = GetPost(post_id=request.args.get('post_id'))
    except ValidationError:
        return {'error': 'Invalid input: you probably did not include the post id.'}, HTTPStatus.BAD_REQUEST
    try:
        post: Post = get_post(session=get_db, post_data=post_data)
        if not post:
            return {'Error': f'post with id {post_data.post_id} does not exists'}, HTTPStatus.NOT_FOUND
        comments: list[Comment] = list_post_comments(session=get_db, post_data=post_data, offset=offset, limit=limit)
    except (OperationalError, IntegrityError) as e:
        print(e)
        # Send email to
        return {'Error': 'The application is experiencing a tempoary error. Please try again in a few minutes.'}, HTTPStatus.INTERNAL_SERVER_ERROR
    resp = [
        CommentCreated(
            user_id=comment.author_id,
            post_id=comment.post_id,
            date_created=comment.comment_date,
            comment_id=comment.id,
            comment=comment.comment_text
        ).model_dump()
        for comment in comments
    ]
    return resp, HTTPStatus.OK

#    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
#    "Accept-Language": "en",
#}

# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    "leetcode.middlewares.LeetcodeSpiderMiddleware": 543,
#}

# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,
    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,
    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,
}
FAKEUSERAGENT_PROVIDERS = [
    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try
    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us
    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value
]

# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    "scrapy.extensions.telnet.TelnetConsole": None,
#}

# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
#ITEM_PIPELINES = {
#    "leetcode.pipelines.LeetcodePipeline": 300,
#}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay


# posts = [
    #     CreatedPost(
    #         id=post.id,
    #         location=post.location,
    #         text=post.text,
    #         image_url=post.image_url,
    #         author_id=post.author_id,
    #         date_published=post.date_published
    #     ).model_dump()
    #     for post in posts
    # ]
    return created_posts, HTTPStatus.OK

@post.route("/load_more_posts", methods=["GET"])
def load_more_posts():
    """Get a single post."""
    offset: str = request.args.get('offset', 0)
    limit: str = request.args.get('limit', 10)
    more_posts = load_posts(limit=int(limit), offset=int(offset))
    return more_posts

        elif shape == self.P_ROUND:
            path.addEllipse(point, d / 2.0, d / 2.0)
        else:
            assert False, "unsupported vertex shape"

    def nearestVertex(self, point, epsilon):
        min_distance = float("inf")
        min_i = None
        for i, p in enumerate(self.points):
            dist = labelme.utils.distance(p - point)
            if dist <= epsilon and dist < min_distance:
                min_distance = dist
                min_i = i
        return min_i

    def nearestEdge(self, point, epsilon):
        min_distance = float("inf")
        post_i = None
        for i in range(len(self.points)):
            line = [self.points[i - 1], self.points[i]]
            dist = labelme.utils.distancetoline(point, line)
            if dist <= epsilon and dist < min_distance:
                min_distance = dist
                post_i = i
        return post_i

    def containsPoint(self, point):
        if self.mask is not None:
            y = np.clip(
                int(round(point.y() - self.points[0].y())),
                0,
                self.mask.shape[0] - 1,
            )
            x = np.clip(
                int(round(point.x() - self.points[0].x())),
                0,
                self.mask.shape[1] - 1,
            )
            return self.mask[y, x]
        return self.makePath().contains(point)


        mask, min_size=mask.sum() * MIN_SIZE_RATIO, out=mask
    )

    if 0:
        imgviz.io.imsave("mask.jpg", imgviz.label2rgb(mask, imgviz.rgb2gray(image)))
    return mask


    agent = cl.user_session.get("agent")
    msg = cl.Message(content="")
    await msg.send()
    await cl.sleep(1)
    msg.content = agent.invoke({"input": message.content})["output"]
    await msg.update()

    assert widget.labelList.count() == 4
    widget.addLabelHistory("bicycle")
    assert widget.labelList.count() == 4
    item = widget.labelList.item(0)
    assert item.text() == "bicycle"


@pytest.mark.gui
def test_LabelDialog_popUp(qtbot):
    labels = ["cat", "dog", "person"]
    widget = LabelDialog(labels=labels, sort_labels=True)
    qtbot.addWidget(widget)

    # popUp(text='cat')

    def interact():
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_P)  # enter 'p' for 'person'  # NOQA
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_Enter)  # NOQA
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_Enter)  # NOQA

    QtCore.QTimer.singleShot(500, interact)
    label, flags, group_id, description = widget.popUp("cat")
    assert label == "person"
    assert flags == {}
    assert group_id is None
    assert description == ""

    # popUp()

    def interact():
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_Enter)  # NOQA
        qtbot.keyClick(widget.edit, QtCore.Qt.Key_Enter)  # NOQA

    QtCore.QTimer.singleShot(500, interact)
    label, flags, group_id, description = widget.popUp()
    assert label == "person"
    assert flags == {}
    assert group_id is None
    assert description == ""



                    maker.bndbox(
                        maker.xmin(str(xmin)),
                        maker.ymin(str(ymin)),
                        maker.xmax(str(xmax)),
                        maker.ymax(str(ymax)),
                    ),
                )
            )

        if not args.noviz:
            captions = [class_names[label] for label in labels]
            viz = imgviz.instances2rgb(
                image=img,
                labels=labels,
                bboxes=bboxes,
                captions=captions,
                font_size=15,
            )
            imgviz.io.imsave(out_viz_file, viz)

        with open(out_xml_file, "wb") as f:
            f.write(lxml.etree.tostring(xml, pretty_print=True))


if __name__ == "__main__":
    main()



        validation_test = validation_result["validation_output"].dict()
        t2 = time.time()
        self.logger.info("Time to validate request: {}".format(round(t2 - t1, 2)))

        if validation_test["plan_is_valid"].lower() == "no":
            self.logger.warning("User request was not valid!")
            print("\n######\n Travel plan is not valid \n######\n")
            print(validation_test["updated_request"])
            return None, None, validation_result

        else:
            # plan is valid
            self.logger.info("Query is valid")
            self.logger.info("Getting travel suggestions")
            t1 = time.time()

            self.logger.info(
                "User request is valid, calling agent (model is {})".format(
                    self.chat_model.model_name
                )
            )

            agent_result = self.agent_chain(
                {
                    "query": query,
                    "format_instructions": self.mapping_prompt.parser.get_format_instructions(),
                }
            )

            trip_suggestion = agent_result["agent_suggestion"]
            list_of_places = agent_result["mapping_list"].dict()
            t2 = time.time()
            self.logger.info("Time to get suggestions: {}".format(round(t2 - t1, 2)))

            return trip_suggestion, list_of_places, validation_result


secrets = load_secets()    



from setuptools import find_packages, setup

# For consistent encoding
from codecs import open
from os import path

# The directory containing this file
HERE = path.abspath(path.dirname(__file__))

# Get the long description from the README file
with open(path.join(HERE, 'README.md'), encoding='utf-8') as f:
    LONG_DESCRIPTION = f.read()

VERSION = '0.5.1'
DESCRIPTION = 'A python library that wraps around the YouTube V3 API. You can use it find and manage YouTube resources including Videos, Playlists, Channels and Comments.'

key_words = [
    'youtube', 'youtube-api', 'youtube comments', 'youtube videos',
    'youtube channels', 'youtube comment thread', 'create youtube playlist'
]

install_requires = [
    'google-api-python-client',
    'google-auth-oauthlib'
]

setup(
    name='ayv',
    packages=find_packages(
        include=[
            'youtube',
            'youtube.oauth',
            'youtube.models',
            'youtube.resources',
            'youtube.resources.video',
            'youtube.exceptions',
            'youtube.resources.channel',
            'youtube.resources.playlist',
            'youtube.resources.playlist_item',
            'youtube.resources.comment_thread',


            "prev",
            self.tr("Open prev (hold Ctl+Shift to copy labels)"),
            enabled=False,
        )
        save = action(
            self.tr("&Save\n"),
            self.saveFile,
            shortcuts["save"],
            "save",
            self.tr("Save labels to file"),
            enabled=False,
        )
        saveAs = action(
            self.tr("&Save As"),
            self.saveFileAs,
            shortcuts["save_as"],
            "save-as",
            self.tr("Save labels to a different file"),
            enabled=False,
        )

        deleteFile = action(
            self.tr("&Delete File"),
            self.deleteFile,
            shortcuts["delete_file"],
            "delete",
            self.tr("Delete current label file"),
            enabled=False,
        )

        changeOutputDir = action(
            self.tr("&Change Output Dir"),
            slot=self.changeOutputDirDialog,
            shortcut=shortcuts["save_to"],
            icon="open",
            tip=self.tr("Change where annotations are loaded/saved"),
        )

        saveAuto = action(
            text=self.tr("Save &Automatically"),



    Returns
    -------
    int or float
        The result of subtracting b from a.
    """
    return a - b
'''


def generate_doc_string(src_code: str, config: Config) -> str:
    prompt_formatted_str: str = function_prompt.format(
        function_code=src_code, documentation_style=config.documentation_style
    )
    # function_and_docstring: str = chatgpt.invoke(prompt_formatted_str)
    # return function_and_docstring
    return function_dcstr


def make_docstring_node(docstr: str):
    constant_str: Constant = Constant(docstr)
    return Expr(value=constant_str)


def format_file(file_path: str) -> None:
    """Format the file using black."""
    if path.exists(file_path):
        subprocess.run(["black", file_path], capture_output=True)


            self.tr("Fill Drawing Polygon"),
            self.canvas.setFillDrawing,
            None,
            "color",
            self.tr("Fill polygon while drawing"),
            checkable=True,
            enabled=True,
        )
        if self._config["canvas"]["fill_drawing"]:
            fill_drawing.trigger()

        # Lavel list context menu.
        labelMenu = QtWidgets.QMenu()
        utils.addActions(labelMenu, (edit, delete))
        self.labelList.setContextMenuPolicy(Qt.CustomContextMenu)
        self.labelList.customContextMenuRequested.connect(self.popLabelListMenu)

        # Store actions for further handling.
        self.actions = utils.struct(
            saveAuto=saveAuto,
            saveWithImageData=saveWithImageData,
            changeOutputDir=changeOutputDir,
            save=save,
            saveAs=saveAs,
            open=open_,
            close=close,
            deleteFile=deleteFile,
            toggleKeepPrevMode=toggle_keep_prev_mode,
            delete=delete,
            edit=edit,
            duplicate=duplicate,
            copy=copy,
            paste=paste,
            undoLastPoint=undoLastPoint,
            undo=undo,
            removePoint=removePoint,
            createMode=createMode,
            editMode=editMode,
            createRectangleMode=createRectangleMode,
            createCircleMode=createCircleMode,


from youtube import YouTube
from youtube.models import Search
from youtube.schemas import (
        YouTubeRequest, YouTubeListResponse, YouTubeResponse,
        SearchFilter, SearchOptionalParameters, SearchPart
)
from typing import Iterator


client_secrets_file = "/home/lyle/oryks/backend/api/libraries/youtube.json"
def get_youtube_client(client_secrets_file: str = client_secrets_file) -> YouTube:
    youtube: YouTube = YouTube(client_secret_file=client_secrets_file)
    client = youtube.authenticate()
    youtube.youtube_client = client
    return youtube

youtube: YouTube = get_youtube_client(client_secrets_file="/home/lyle/Downloads/test.json")


# query: str = ''
# part: SearchPart = SearchPart()
# optional_parameters: SearchOptionalParameters = SearchOptionalParameters(
#     q=query,
#     type=['video'],
#     channelId="UCtAcpQcYerN8xxZJYTfWBMw"
# )
# search_request: YouTubeRequest = YouTubeRequest(
#     part=part, 
#     optional_parameters=optional_parameters
# )
# search_results: YouTubeResponse = youtube.search(search_request)
# search_iterator: Iterator = youtube.get_search_iterator(search_request)
# # res: YouTubeResponse = youtube.find_channel_by_name(display_name="Umar Jamil")
# # print(res.items[0])
# res = next(search_iterator)
# final = []
# for x in search_iterator:
#         for search in x:
#                 final.append(
#                         dict(


from langchain.llms import OpenAI

from langchain_google_genai import ChatGoogleGenerativeAI

open_ai = OpenAI(temperature=0)
google = ChatGoogleGenerativeAI(model="gemini-pro")


import whisper

model = whisper.load_model("medium.en")
result = model.transcribe("code.wav")
print(result["text"])

import argparse
import codecs
import logging
import os
import os.path as osp
import sys

import yaml
from qtpy import QtCore
from qtpy import QtWidgets

from labelme import __appname__
from labelme import __version__
from labelme.app import MainWindow
from labelme.config import get_config
from labelme.logger import logger
from labelme.utils import newIcon


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--version", "-V", action="store_true", help="show version")
    parser.add_argument("--reset-config", action="store_true", help="reset qt config")
    parser.add_argument(
        "--logger-level",
        default="debug",
        choices=["debug", "info", "warning", "fatal", "error"],
        help="logger level",
    )
    parser.add_argument("filename", nargs="?", help="image or label filename")
    parser.add_argument(
        "--output",
        "-O",
        "-o",
        help="output file or directory (if it ends with .json it is "
        "recognized as file, else as directory)",
    )
    default_config_file = os.path.join(os.path.expanduser("~"), ".labelmerc")
    parser.add_argument(
        "--config",


    "Face ID",
    "Water and dust resistance",
    "iOS 15",
    "Improved battery life",
    "Siri voice recognition",
    "Apple Pay",
    "Apple Fitness+ integration",
    "Apple Arcade subscription",
    "Apple Music",
    "iMessage",
    "App Store",
    "iCloud storage",
    "Privacy features",
]

for feature in features:
    print("#######################################################")
    query = f"What does the video cover in relation to {feature}?"
    print(qa_chain.invoke(query))
    print("#######################################################")


        self.zoomMode = self.FIT_WIDTH if value else self.MANUAL_ZOOM
        self.adjustScale()

    def enableKeepPrevScale(self, enabled):
        self._config["keep_prev_scale"] = enabled
        self.actions.keepPrevScale.setChecked(enabled)

    def onNewBrightnessContrast(self, qimage):
        self.canvas.loadPixmap(QtGui.QPixmap.fromImage(qimage), clear_shapes=False)

    def brightnessContrast(self, value):
        dialog = BrightnessContrastDialog(
            utils.img_data_to_pil(self.imageData),
            self.onNewBrightnessContrast,
            parent=self,
        )
        brightness, contrast = self.brightnessContrast_values.get(
            self.filename, (None, None)
        )
        if brightness is not None:
            dialog.slider_brightness.setValue(brightness)
        if contrast is not None:
            dialog.slider_contrast.setValue(contrast)
        dialog.exec_()

        brightness = dialog.slider_brightness.value()
        contrast = dialog.slider_contrast.value()
        self.brightnessContrast_values[self.filename] = (brightness, contrast)

    def togglePolygons(self, value):
        flag = value
        for item in self.labelList:
            if value is None:
                flag = item.checkState() == Qt.Unchecked
            item.setCheckState(Qt.Checked if flag else Qt.Unchecked)

    def loadFile(self, filename=None):
        """Load the specified file, or the last opened file if None."""
        # changing fileListWidget loads file
        if filename in self.imageList and (


import json

from flask import request, Flask

from ..config.logger_config import app_logger
from .rate_limiter import request_is_rate_limited
from redis import Redis
from datetime import timedelta
from http import HTTPStatus

r = Redis(host='localhost', port=6379, db=0)


def log_post_request():
    request_data = {
        "method": request.method,
        "url root": request.url_root,
        "user agent": request.user_agent,
        "scheme": request.scheme,
        "remote address": request.remote_addr,
        "headers": request.headers,
    }
    if request.args:
        request_data["args"] = request.args
    if request.form:
        request_data["data"] = request.form
    else:
        request_data["data"] = request.json
    if request.cookies:
        request_data["cookies"] = request.cookies
    if request.files:
        request_data["image"] = {
            "filename": request.files["Image"].filename,
            "content type": request.files["Image"].content_type,
            "size": len(request.files["Image"].read()) // 1000,
        }
    app_logger.info(str(request_data))

    img_b64 = base64.b64encode(img_data).decode("utf-8")
    return img_b64


def img_arr_to_data(img_arr):
    img_pil = PIL.Image.fromarray(img_arr)
    img_data = img_pil_to_data(img_pil)
    return img_data


def img_data_to_png_data(img_data):
    with io.BytesIO() as f:
        f.write(img_data)
        img = PIL.Image.open(f)

        with io.BytesIO() as f:
            img.save(f, "PNG")
            f.seek(0)
            return f.read()


def img_qt_to_arr(img_qt):
    w, h, d = img_qt.size().width(), img_qt.size().height(), img_qt.depth()
    bytes_ = img_qt.bits().asstring(w * h * d // 8)
    img_arr = np.frombuffer(bytes_, dtype=np.uint8).reshape((h, w, d // 8))
    return img_arr


def apply_exif_orientation(image):
    try:
        exif = image._getexif()
    except AttributeError:
        exif = None

    if exif is None:
        return image

    exif = {PIL.ExifTags.TAGS[k]: v for k, v in exif.items() if k in PIL.ExifTags.TAGS}

    orientation = exif.get("Orientation", None)


chatgpt: BaseLLM = ChatOpenAI(temperature=0, api_key=api_key)

tools: list[Tool] = [
    GoogleSearchTool(),
]


def get_agent_executor():
    """Get the agent"""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a very useful assistant. Your task will be to asnswer the users question. Be very friendly and professional.",
            ),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )

    functions = [format_tool_to_openai_function(t) for t in tools]

    llm_with_tools = chatgpt.bind(functions=functions)

    agent = (
        {
            "input": lambda x: x["input"],
            "agent_scratchpad": lambda x: format_to_openai_function_messages(
                x["intermediate_steps"]
            ),
        }
        | prompt
        | llm_with_tools
        | OpenAIFunctionsAgentOutputParser()
    )

    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return agent_executor




    def deleteSelected(self):
        deleted_shapes = []
        if self.selectedShapes:
            for shape in self.selectedShapes:
                self.shapes.remove(shape)
                deleted_shapes.append(shape)
            self.storeShapes()
            self.selectedShapes = []
            self.update()
        return deleted_shapes

    def deleteShape(self, shape):
        if shape in self.selectedShapes:
            self.selectedShapes.remove(shape)
        if shape in self.shapes:
            self.shapes.remove(shape)
        self.storeShapes()
        self.update()

    def duplicateSelectedShapes(self):
        if self.selectedShapes:
            self.selectedShapesCopy = [s.copy() for s in self.selectedShapes]
            self.boundedShiftShapes(self.selectedShapesCopy)
            self.endMove(copy=True)
        return self.selectedShapes

    def boundedShiftShapes(self, shapes):
        # Try to move in one direction, and if it fails in another.
        # Give up if both fail.
        point = shapes[0][0]
        offset = QtCore.QPointF(2.0, 2.0)
        self.offsets = QtCore.QPoint(), QtCore.QPoint()
        self.prevPoint = point
        if not self.boundedMoveShapes(shapes, point - offset):
            self.boundedMoveShapes(shapes, point + offset)

    def paintEvent(self, event):
        if not self.pixmap:
            return super(Canvas, self).paintEvent(event)



            self._image_embedding = _compute_image_embedding(
                image_size=self._image_size,
                encoder_session=self._encoder_session,
                image=self._image,
            )
            if len(self._image_embedding_cache) > 10:
                self._image_embedding_cache.popitem(last=False)
            self._image_embedding_cache[self._image.tobytes()] = self._image_embedding
            logger.debug("Done computing image embedding.")

    def _get_image_embedding(self):
        if self._thread is not None:
            self._thread.join()
            self._thread = None
        with self._lock:
            return self._image_embedding

    def predict_mask_from_points(self, points, point_labels):
        return _compute_mask_from_points(
            image_size=self._image_size,
            decoder_session=self._decoder_session,
            image=self._image,
            image_embedding=self._get_image_embedding(),
            points=points,
            point_labels=point_labels,
        )

    def predict_polygon_from_points(self, points, point_labels):
        mask = self.predict_mask_from_points(points=points, point_labels=point_labels)
        return _utils.compute_polygon_from_mask(mask=mask)


def _compute_scale_to_resize_image(image_size, image):
    height, width = image.shape[:2]
    if width > height:
        scale = image_size / width
        new_height = int(round(height * scale))
        new_width = image_size
    else:
        scale = image_size / height


from dotenv import load_dotenv

load_dotenv()
import logging
from logging import Handler

from rich.logging import RichHandler

from .agent_nelly import AgentNelly
from .logger import create_logger
from .states import Introduction
from .ui import RichUI

if __name__ == "__main__":
    handlers: list[Handler] = [RichHandler()]
    create_logger(handlers=handlers)
    logging.getLogger().setLevel(logging.WARN)
    from .states import QA, DataAnalysis

    agent_nelly = AgentNelly(ui=RichUI(), initial_state=Introduction())
    agent_nelly.analyze_product_review()


from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from threading import Lock

from .config import Config
from .directory_iterators import get_all_modules
from .docstring_writer import process_file, process_function


def generate_project_docstrings(
    config: Config, source_code_queue: Queue, function_code_queue: Queue
) -> None:
    with ThreadPoolExecutor(max_workers=4) as executor:
        executor.submit(get_all_modules, config, source_code_queue)
        executor.submit(process_file, source_code_queue, function_code_queue)
        executor.submit(process_function, config, function_code_queue)
    source_code_queue.join()
    function_code_queue.join()


from os import path
import json
from random import choices
from langchain.docstore.document import Document
import re
from langchain.prompts import ChatPromptTemplate


api_key: str = "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"
file_path: str = "comments.json"


def lower(text: str) -> str:
    return text.lower().strip()

# def remove_tags(text: str) -> str:
#     pattern = r'<.*?>'
#     text = re.sub()

def remove_urls(text: str) -> str:
    url_pattern = r'https?://\S+|www\.\S+'
    text = re.sub(url_pattern, "", text)
    return text

def remove_punctuations(text: str) -> str:
    punctuation_pattern = r'[^\w\s]'
    cleaned = re.sub(punctuation_pattern, "", text)
    return cleaned

def clean_text(text: str) -> str:
    text = lower(text)
    text = remove_urls(text)
    text = remove_punctuations(text)
    return text

def is_acceptable_len(text: str, l=6) -> bool:
    return len(text.split()) >= l

with open(file_path, "r", encoding="utf-8") as f:
    all_comments: list[str] = json.load(fp=f)


from zipfile import ZipFile
from os import path, mkdir
import os


def download_data():
    pass


def extract_archive(archive_path: str, archive_name: str, extract_path: str = None) -> None:
    if not extract_path:
        extract_path: str = "raw-data"
        if not path.exists(extract_path):
            mkdir(extract_path)
    archive_path: str = path.join(archive_path, archive_name)
    with ZipFile(file=archive_path, mode="r") as z_object:
        z_object.extractall(path=extract_path)
        
 

def handle_unsupported_media_type(exeption: Exception) -> Response:
    """Handle all unsupported media type errors.

    This method is called when a a request does not supply the data or the data supplied is
    invalid.

    Parameters
    ----------
    exception: Exception
        The exception that was raised. This is a subclass of Exception.

    Returns
    -------
    Response:
        A string consiting of json data and response code.
    """
    return make_response(jsonify({"error": str(exeption)}), HTTPStatus.UNSUPPORTED_MEDIA_TYPE)


def register_error_handlers(app: Flask) -> None:
    """Register the error handlers.

    Parameters
    ----------
    app: flask.Flask
        The Flask app instance.
    """
    app.register_error_handler(HTTPStatus.NOT_FOUND, handle_resource_not_found)
    app.register_error_handler(HTTPStatus.METHOD_NOT_ALLOWED, handle_method_not_allowed)
    app.register_error_handler(HTTPStatus.INTERNAL_SERVER_ERROR, handle_internal_server_error)
    app.register_error_handler(HTTPStatus.UNSUPPORTED_MEDIA_TYPE, handle_unsupported_media_type)

